# Discussion

#### Central Findings

The results of this meta-analysis demonstrate the considerable variability in the predictive performance of machine learning models applied to remote sensing data for SDGs. Some of this variability could be attributed to the proportion of the majority class as well as the inclusion of ancillary data. However, the type of model, whether neural networks and tree-based models or the SDG studied, showed no differences in overall accuracy. Unsurprisingly, the proportion of the majority class significantly affected the overall accuracy of machine learning models. While the use of ancillary data in primary studies has a small but significant negative effect on overall accuracy performance, which is counterintuitive. Perhaps this is explained by other variables not captured in this study, for example researchers addressing more complex classification problems use models with ancillary data.

To the best of available knowledge, this is the only meta-analysis of remote sensing methods that utilized weighted estimates. The use of a three-level random effects model enabled the decomposition of variance into within-study and between-study components, offering insights into the observed heterogeneity. No meaningful difference was found between the weighted and unweighted approaches.

#### Limitations

1.  **Number of reviewers:** From the 200 studies randomly sampled, three reviewers assessed whether full-text screening should be conducted. Only 57 papers were agreed upon by all three reviewers, while each reviewer thought between 77 and 81 studies could have been included. This highlights the subjectivity of the selection process and the importance of having multiple reviewers. The full-text screening was only conducted by one person which means that this subjectively or potential mistakes were missed in the final dataset. This issue is exasperated by the inconsistent reporting on methods in this field. For example, one feature that could not be included in the analysis was whether the results reported were derived from the training or test set because it was very unclear in some of the selected studies.

2.  **Sample size:** This study included a total of 20 studies. While several simulations studies suggest that a three-level meta-analysis can yield accurate results with as few as 20 to 40 studies [@hedges2010], this analysis is at lower bound, and the included studies exhibit considerable variability, making the statistical power a concern. @polanin_lec suggests a minimum of 40 studies is generally recommended to ensure robust results. Furthermore, a relatively high proportion of the studies (6 out of 20) reported only one result, limiting the ability to assess within-study variability. The small sample size inherently increases the potential for bias and may affect the reliability of the findings (@polanin_lec ).

3.  **Choice of effect size:** While overall accuracy is widely used, it does not capture the complexity of model performance, especially in studies with imbalanced classes. To illustrate the problem, if 99% of the data belongs to class A, a model that always predicts class A---without any regard to the predictors---will achieve an overall accuracy of 99%, despite essentially doing nothing and failing to capture meaningful patterns. For more specific details on the issues related to the use of overall accuracy, see @foody2020 and @stehman2019.

4.  **Publication bias:** This study only examined published results, which introduces publication bias---a well-documented effect where studies with positive results are more likely to be published, while negative or neutral findings remain unpublished [@harrer2022; @borenstein_book2009; @bozada2021; @hansen2022]. This bias can lead to an overestimation of effects, as demonstrated in this study, where the average overall accuracy around 90%. Accuracy, is easy to understand and compute, as addressed it does not take into account class imbalance. When models are developed and tuned to maximize accuracy on training data, they often perform poorly on unseen data, inflating the performance metrics, but the reporting of training and test results was inconsistent.

5.  **Study features included:** The analysis would have benefited from the inclusion of more study features. For example, to better understand the effect of ancillary data, a feature representing the complexity of the problem addressed by the primary study could explain the negative effect of additional information on a prediction model. It is also important to note that most of the study features included in this research were between-study covariates and did not differ within studies, which explains why only the between-study heterogeneity was reduced. Furthermore, due to the small sample size, it was necessary to aggregate the study features into broad categories, which limited the granularity of the analysis.

6.  **Apples and organs problem:** The $I^2$ result of effectively 100% may indicate that the included studies are too different to statistically compare. This is often referred to as the "apples and oranges problem" [@harrer2022 chap.1]. The extent to which primary studies can differ while still being meaningfully combined in a meta-analysis is debated. However, when Robert Rosenthal, a pioneer in meta-analysis, was asked whether combining studies with significant differences is valid his response was *"combining apples and oranges makes sense if your goal is to produce a fruit salad"* [@borenstein_introduction_2013, chap.40, pp. 357]. In this case, despite the diverse research aims of the included studies, the objective is to draw general conclusions about machine learning applications in remote sensing for SDG monitoring. This approach can be viewed as a "fruit salad" with potential for broad applicability across different SDG contexts however this circles back to the issue of the sample size. A large sample size is needed to have a high enough power to make confident conclusions.

7.  **Cochran's Q and large sample sizes:** Another limitation of this analysis, is the use of Cochran's Q for testing heterogeneity. The power of the Q-statistic is dependent on the number of included effect sizes ($n$) and the precision of the studies (i.e., the sample size with in that study). In cases with large primary-sample-sizes, the Q statistic becomes highly sensitive to even minor differences between studies. The Q-statistic is "overpowered", which result in the detection of statistically significant heterogeneity even when the actual differences between studies are small. This sensitivity may exaggerate the extent of heterogeneity, potentially lead to misleading conclusions about the variability among the included studies. Little research has been done on the effect of very large primary-sample-sizes since meta-analysis typically compile studies who's unit of analysis in on a patient level. Primary-sample-sizes in the millions is not a common issue.

8.  **Transformation of the effect size:** Although the distribution of the transformed overall accuracy was closer to a normal distribution, it remained significantly non-normal. Furthermore, the use of FT transformation is highly contested in the literature [@schwarzer2019; @lin2020;@doi2021; @röver2022; @doi2021; @röver2022] because of several important limitations. First, the FT is notably unintuitive, notably the calculation of variance relies on the structure of an arcsine function\'s derivative. As seen in @eq-FTT_back, the FT transformation has a complex back-transformation. In this analysis, the pooled variance was used for the back-transformation, addressing main issue debated in the literature---namely, that using harmonic mean of primary-sample-sizes to back-transform the pooled effect size can lead to misleading results [for details see @schwarzer2019; @lin2020; @doi2021; @wang2023; @röver2022]. The choice of back-transformation method significantly influences the outcome, and justifying specific method becomes especially challenging in a multilevel data structure. Lastly, in a random-effects model the true (transformed) proportion is assumed to follow a normal distribution between studies, the FT transformation potentially violates this assumption as the arcsine function has a bounded domain [@röver2022].

#### Implications for Future Research

The limitations identified in this meta-analysis suggest several directions for future research that can enhance the robustness and generalizability of findings related to machine learning applications in remote sensing for SDG monitoring.

1.   **Sample size and model complexity**: One of the primary limitations of this meta-analysis was the small sample size, with only 20 studies included. Future research should aim to expand the pool of included studies. This would mean that interaction effects of between the collected study features could also be included in the analysis. The structure of the random effects can also be explored with the application of more sophisticated variance-covariance structures for random effects. This approach, sometimes referred to as dose-response meta-analysis [@viechtbauer], would provide insights into how specific study characteristics influence effect sizes over time or across varying conditions. ​

2.  **Broader inclusion of performance metrics**: This meta-analysis primarily focused on overall accuracy, a commonly used but potentially misleading performance metric, particularly in imbalanced datasets. Future studies should expand the range of performance metrics used, incorporating precision, recall, F1-score, and AUC to provide a more comprehensive evaluation of model performance. More than one effect size can be modeled using network meta- analysis models. The inclusion of more permortance metrics would offer a more nuanced understanding of how models perform under different conditions.

3.  **Exploring additional study features and moderators**: The present study focused on a limited set of study features, including the proportion of the majority class and the inclusion of ancillary data. Future research should investigate a broader range of potential moderators, such as model complexity, data preprocessing techniques, and environmental or socio-economic factors specific to SDG challenges. By including a more extensive set of features, researchers can better understand the drivers of performance variability and refine model selection for specific applications​.

4.  **Effect of large sample size in primary studies**: Simulation studies could provide insights into the sensitivity of Cochran's Q in the context of large sample sizes. Developing less sensitive methods for assessing heterogeneity would improve the reliability of meta-analytic findings, especially when studies involve substantial sample sizes, which can exaggerate minor differences between studies.

5.  **Data extraction:** In the timeframe of this research, the ChatGPT virtual assistant showed significant improvements in data extraction capabilities. Initially, in January 2024, ChatGPT struggled to extract meaningful features. By May 2024, it was capable of accurately filling in all study features directly from the provided papers (in PDF formate). Although the improvement was not formally assessed in this study, the difference was striking. Some research has allready examined the potential accuracy of large language models (LLMs) in data extraction for meta-analyses, with promising results [@mahuli2023].However, further investigation into the accuracy of LLMs for meta-analysis is required. LLMs can expedite the data extraction process, potentially addressing challenges related to the limited number of included studies. Another unrelated recommendation to improve data extraction would be for journals to require results and specific features to be submitted separately in addition to the manuscript so that the journals themselves can report tends in outcomes.

# Conclusion 

This meta-analysis highlights the considerable variability in the predictive performance of machine learning models applied to remote sensing data for SDGs. While the proportion of the majority class and the inclusion of ancillary data were significant factors influencing overall accuracy, no significant differences were observed between the types of models used (e.g., neural networks or tree-based models) or the specific SDGs studied. Interestingly, the negative impact of ancillary data on model performance, though counterintuitive, suggests the need for further investigation. The use of a three-level random effects model here provided a deminstraction of the potential insights into the sources of heterogeneity, however because of the limited data few conclusions can currently be made. The strongest finding of this paper is the limitation of overall accuracy as a performance metric, as seen from the reduction in the average effect size once the proportion of the majority class was controled for the overall accuaary dropped from 0.90 to 0.70. Future studies and guildlines should expanding the pool of studies, and incorporate a broader range of performance metrics, and exploring additional study features. Furthermore, more research is needed to improve the robustness and applicability of meta-analyses methods to this field. In conclusion, despite the limitations, this study provides a valuable starting point for understanding the variability in machine learning model performance for remote sensing applications in SDG monitoring.
