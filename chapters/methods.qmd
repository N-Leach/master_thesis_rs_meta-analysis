---
execute:
  echo: false
  enabled: true
tbl-cap-location: top
---

# Methods {#sec-methods style="text-align: justify"}

```{r setup_methods}
source("../appendix/packages.R")

```

::: {style="text-align: justify"}
The methods adopted in this study are delineated in sequential steps, following the framework proposed by @debray2017. Additionally, all procedures and reporting were conducted in compliance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines [@page2021]. For the statistical analyses `metafor` [@viechtbauer2010] and `dmetar` [@dmetar] packages the were used.

## Formulating the review question and protocol

The PICOTS (population, intervention, comparison, outcome, timing, and setting) system was used to frame the review aims for this analysis [@debray2017]. Based on this framework, the question was formulated as follows: In studies focused on SDGs, how heterogeneous is the performance of ML applied to various remote sensing applications, and what study features account for any observed differences in model performance?
:::

::: {#tbl-PICOTS}
\renewcommand{\arraystretch}{0.8}

```{r}
read_excel("figures/summary_tables.xlsx", sheet = "PICOTS")|>
  kable(booktabs = TRUE, linesep = "") |>
  kable_styling(font_size = 10, 
                full_width = FALSE)|>
  column_spec(1, width = "2cm")|>
  column_spec(2, width = "12cm")|>
   footnote(c("PICOTS framework items and corresponding role in structuring this review."), 
           threeparttable=TRUE)
```

PICOTS framework
:::

::: {style="text-align: justify"}
```{r}
citations<- read.csv("../appendix/data/citations.csv")
```

The data collected for this report was extracted from peer-reviewed articles published between January 2018 and December 2023. These articles were gathered (on January 15 and 16, 2024) from several academic databases, including ScienceDirect and Taylor & Francis Online, as shown in @fig-prisma. To reduce potential bias from database coverage [@hansen_how_2022; @tawfik_step_2019], several academic databases were used. While Google Scholar can be useful for supplementary searches and grey literature, it is generally considered unsuitable as the primary sourse for systematic reviews [@gusenbauer_which_2020]. Furthermore, Google Scholar searches results are not fully reproducible [@gusenbauer_which_2020] and search result references that cannot be downloaded in batches.The search terms were "remote sensing" AND "machine learning" AND "sustainable development goals." The search results from these databases were downloaded in RIS format and imported into Zotero for further processing.Duplicate articles were handled using Zotero's "merge duplicates" function.
:::

## Specific inclusion and exclusion criteria

After removing review articles and non-research papers, a total of 811 relevant articles remained. Of these potentially relevant papers, `r round(sum(citations$Publication.Year == 2023, na.rm = TRUE)*100/ nrow(citations))`% were published in 2023, highlighting the growth of research in this field. The trend, as illustrated in @fig-pubs, is consistent with other similar research, for example, @ekmen2024, which reported a sharp increase in publications related to ML and RS for SDG monitoring.

::: {#fig-pubs}
```{r no_pubs}
#| fig-width: 4
#| fig-height: 3

ggplot(subset(citations, !is.na(Publication.Year)))+
      geom_bar(mapping = aes(x = as.factor(Publication.Year)), 
                     fill = "#001158", alpha = .8) +
    labs(x = "Publication Year", 
       y = "Number of Reports", 
       title = "Number of Publications in ML for RS.") +
    theme(plot.title = element_text(size=10),
        axis.title.x = element_text(size=8),
        axis.title.y = element_text(size=8), 
        axis.text = element_text(size=8))

```

Publication increase between 2018 and 2022.
:::

Due to the large number of papers remaining, a random sample of 200 articles was drawn for title and abstract screening. These potentially relevant articles were screened independently by three reviewers (the author and two internal supervisors) using the R package `metagear` [@metagear]. The papers were selected according to the following criteria: a) publications utilizing remote sensing and ML techniques, (b) indication of a quality assessment for example overall accuracy. @tbl-keywords shows the words highlighted in the abstract screening phase to aid the reviewers and @fig-metagearGUI shows the user interface highlighting these keywords.

::: {#tbl-keywords}
\renewcommand{\arraystretch}{0.8}

```{r}
read_excel("figures/summary_tables.xlsx", sheet = "keywords")|>
  kable(booktabs = TRUE, linesep = "") |>
  kable_styling(font_size = 10, 
                full_width = FALSE)|>
  column_spec(1, width = "3cm")|>
  column_spec(2, width = "11cm")|>
  
  footnote(c("Keywords highlighted by the `metagear` user interface during abstract screening phase as a visual cue to speed up the screening process."), 
           threeparttable=TRUE, escape = FALSE)
```

Keywords
:::

![Metagear graphical user interface: Example of the metagear abstract screener interface, with key words highlighted. On the bottom left the reviewer can select whether the paper is relevant.](figures/metagear_gui.png){#fig-metagearGUI fig-align="center" width="517"}

::: {style="text-align: justify"}
As shown in @fig-prisma, of the 200 abstracts screened only 57 were deemed potentially relevant by all three reviewers. To have comparable performance metrics it decided to focus on papers related to classification. The titles and abstracts of the 57 articles were screened using `metagear` dividing them to classification (40) and regression (17) papers. In the 40 papers, overall accuracy was the most commonly reported outcome metric and therefore it was decided to include all papers that report overall accuracy.
:::

![PRIMSA flow diagram of manuscript selection. The records were identified from databases including Web of Science (WOS), ScienceDirect, PubMed, Journal Storage (JSTOR), American Geophysical Union Publications (Agupubs), EBSCO, IEEE Xplore, Multidisciplinary Digital Publishing Institute (MDPI), ProQuest, and Taylor & Francis Online (Tandfonline), no papers were gathered from official registers. Note: number of records removed four where not journal articles and 27 were omitted for being reviews. A random sample of 200 of the total 884 was drawn and reviewed by three independent reviewers. A total of 57 records were left, 40 of which were deemed to be classification papers and the full text screened.](figures/fig_PRISMA2020.png){#fig-prisma fig-align="center" width="487" height="700"}

::: {style="text-align: justify"}
## Feature collection

Using the first 10 papers and previous systematic reviews, a list of potential study features was created and structured in a table for data collection. @tbl-exf outlines all the extracted features and study identification information. The features in the table are grouped according to their use in the analysis. These features include methodology and data characteristics, which provide information about the complexity of the classification tasks (e.g., the number of output classes) and the proportion of the majority class, indicating potential class imbalance issues that can affect the performance of classification models. Remote sensing-specific information was also gathered, including the type of devices, spectral bands, and spatial resolution to assess how data collection impacts performance. The reported overall accuracy is the effect size of interest, and the sample size is important for the weighted meta-analysis. The other features are used to help explain some variation in effect sizes. The sample size is also used as a feature, as larger sample sizes might influence overall accuracy. Of the extracted features, the number of spectral bands and spatial resolution were categorized due to high levels of non-reporting. The type of remote sensing device was excluded because only one study did not use satellite data, and the specifics of the spectral bands used were too different to make meaningful groups. Several potentially useful features were not recorded, including temporal resolution (the frequency of data collection) and pre-processing steps, which also impact the performance of the model. These were excluded as the differences between papers were too large to make groups. The number of citations was gathered using the Local Citation Network web app, which collects article metadata from OpenAlex---a bibliographic catalog of scientific papers [@openalex][^methods-1].
:::

[^methods-1]: The idea to add the number of citations was added after the analysis was mostly completed. This suggestion was made during a discussion of the project after the preliminary results were presented to the methodology team at the CBS.

```{=tex}
\blandscape
\renewcommand{\arraystretch}{0.75}
```
::: {#tbl-exf}
```{r}
tbl_ex <- read_excel("../appendix/data/extracted_features.xlsx", sheet = "study_features")

tbl_ex[is.na(tbl_ex)] <- "-"
tbl_ex |> 
  select("Feature", "Definition", "Ranges/Categories Adopted") |> 
  kable(booktabs = TRUE, linesep = ""
        ) |> 
  kable_styling(font_size = 9.8, 
                full_width = FALSE, 
                bootstrap_options = c("condensed")) |> 
  column_spec(1, width = "5cm") |> 
  column_spec(2, width = "8cm") |> 
  column_spec(3, width = "8cm") |>
  row_spec(c(0),  bold = TRUE)|>
   # group titles
  pack_rows("Study Identification and Information", start_row = 1, end_row = 5) |>
  pack_rows("Used in Incercepted Only Model", start_row = 6, end_row = 7) |>
  pack_rows("Features Added to Mixed Effect Model", start_row = 8, end_row = 21) |>
  pack_rows("Features Excluded", start_row = 22, end_row = 23) |>
  
  
   footnote(general = "The Intercept-only Model and Mixed Effect Model are described in the following section.",
           threeparttable = TRUE)
  
```

Extracted features
:::

\elandscape

::: {style="text-align: justify"}
## Statistical analysis

A meta-analysis is a statistical method that aggregates results from several primary studies to assess and interpret the collective evidence on a specific topic or research question. Specifically, the aim is to (a) determine the average (summary) effect, (b) establish the degree of heterogeneity between effect sizes, and (c) access if study characteristics can explain any of the heterogeneity of the effect sizes [@cheung2014]. In this case the effect size (dependent variable) of interest is the overall accuracy. Let $\hat{\theta}_{ij}$ be the $i-$th observed effect size in study $j$ (where $i = 1, ..., k_j$, $j = 1, ..., n$). From @eq-OA, the overall accuracy is the proportion of correctly classified instances, therefore, the effect size is:

$$
\begin{aligned} 
&\ \hat{\theta}_{ij} = \frac{s_{ij}}{m_{ij}}\\
&\ v_{ij} = \frac{\hat{\theta}_{ij}(1-\hat{\theta}_{ij})}{m_{ij}}
\end{aligned} 
$$ {#eq-theta} where $s_{ij}$ is the number of successful predictions and $m_{ij}$ is total number of pixels or objects classified.

#### Weighted Approach

Before conducting the meta-analysis, first the structure of the collected data and assumption of independence of effect sizes need to be addressed. In the context of this research, dependencies are introduced since all reported effect sizes from each study are included. The degree of dependence between effect sizes can be categorized as either known or unknown [@cheung2014]. Multivariate meta-analytic techniques use known dependencies reported in the primary studies, such as reported correlation coefficients [@cheung2014]. However, dependency estimates between outcomes are rarely reported [@assink2016]. Therefore, to model these unknown dependencies a 3-level random-effects meta-analytic model is used. The three-level meta-analysis approach models three different variance components distributed over three levels:

At level 1, the sampling variance of the effect sizes is modeled as: $$
\begin{aligned} 
&\ \text{Level 1:  } \hat{\theta}_{ij} = \theta_{ij} + \epsilon_{ij}, \\
&\  \epsilon_{ij} \sim \mathcal{N}(0, v_{ij}).\\
\end{aligned}
$$ {#eq-lvl1}

The observed overall accuracy $\hat{\theta}_{ij}$ is an estimate of overall accuracy from experiment $i$ in study $j$ and is modelled as the true overall accuracy, $\theta_{ij}$ and error component $\epsilon_{ij}$ which is normally distributed with mean $0$ and known variance $v_{ij}$. A model that only takes into account sampling variance is referred to as a fixed-effects model, where it is assumed that all studies included in the meta-analysis share a single true effect size, and therefore, the only source of variation between effect sizes is the sampling variance. The fixed-effects model assumes homogeneity across studies and allows for conditional inference about the specific set of studies included in the analysis, without accounting for variability that might arise from differences between studies. The inclusion of the random effects (at level 2 and 3) means that as well as sampling variance, the heterogeneity due to differing between and within study features are also taken into account [@schwarzer_meta-analysis_2015, pp. 34; @wang2023; @harrer2022]. Therefore, the addition random effect components allow one to make unconditional inferences about the population from which the included studies are a random sample.

At level 2, within-study heterogeneity ($\sigma^2_{\text{level2}}$) is modelled as: $$
\begin{aligned} 
&\ \text{Level 2:  } \theta_{ij} = \kappa_j + \zeta_{ij}, \\
&\  \zeta_{ij} \sim \mathcal{N}(0, \sigma^2_{\text{level2}}).\\
\end{aligned}
$$ {#eq-lvl2} The true overall accuracy $\theta_{ij}$ is modelled as the average overall accuracy, $\kappa_{j}$ of study $j$ and study-specific heterogeneity $\zeta_{ij}$, which is normally distributed with mean $0$ and variance $\sigma^2_{\text{level2}}$.

Lastly, level 3, the variance between heterogeneity ($\sigma^2_{\text{level3}}$) is modelled as: $$
\begin{aligned} 
&\ \text{Level 3:  } \kappa_j = \mu + \xi_{j}, \\
&\  \xi_{j} \sim \mathcal{N}(0, \sigma^2_{\text{level3}}).\\
\end{aligned}
$$ {#eq-lvl3}

The average overall accuracy $\kappa_{j}$ of study $j$ is modelled as the average population effect $\mu$ and between-study heterogeneity $\xi_{j}$, which is normally distributed with mean $0$ and variance $\sigma^2_{\text{level3}}$. Combined, the three-level meta-analysis models the observed effect size modelled as the sum of the average population effect $\mu$ and these three error components: $$
\hat{\theta}_{ij} = \mu + \xi_j + \zeta_{ij} + \epsilon_{ij}.
$$ {#eq-combined_lvls}

For the expected value of the observed effect size to be the population average, $\mathbb{E}(\hat{\theta}_{ij}) = \mu$, the random effects at the different levels and the sampling variance are assumed independent: $\text{Cov}(\xi_j, \zeta_{ij}) = \text{Cov}(\xi_j, \epsilon_{ij}) = \text{Cov}(\zeta_{ij}, \epsilon_{ij}) = 0$. Therefore, (1) unconditional sampling variance of the effect size is the sum of level 3 and level 2 heterogeneity, and the known sampling variance: $\text{Var}(\hat{\theta}_{ij}) = \sigma^2_{\text{level3}} +\sigma^2_{\text{level2}} + v_{ij}$, (2) the effect sizes within the same study share the same covariance $\text{Cov}(\hat{\theta}_{ij}, \hat{\theta}_{lj}) = \sigma^2_{\text{level3}}$, and (3) the effect sizes in different studies are independent $\text{Cov}(\hat{\theta}_{ij}, \hat{\theta}_{zu}) = 0$ [@cheung2014][^methods-2].

The random-effects model can be extended to a mixed-effects model (also referred to as a meta-regression) by including study characteristics as covariates (predictors). Let $x$ denote the value covariate, where $b'$ refers to the number of covariates included in the model. These covariates can be either $x_{ij}$ for a level-2 covariate or $x_j$ for a level-3 covariate. The mixed-effect model defined as: $$
\hat{\theta}_{ij} = \mu + \beta_1 x_{ij1} + .... + \beta_{b'} x_{jb'} + \xi_j + \zeta_{ij} + \epsilon_{ij}
$$ {#eq-meta_reg} The assumptions here remain the same as @eq-combined_lvls, but the heterogeneity ($\sigma^2_{\text{level3}}, \sigma^2_{\text{level2}}$) is the variability among the true effects which is not explained by the included covariates [@cheung2014; @viechtbauer2010]. The aim of the mixed-effects model is to examine the extent to which the included covariates in the model influence the overall population average $\mu$ and the heterogeneity $\sigma^2_{\text{level3}}$ and $\sigma^2_{\text{level2}}$ [@viechtbauer2010].

In this way, meta-analytic models are essentially, special cases of the general linear (mixed effects) model with heteroscedastic sampling variances which are assumed to be known [@viechtbauer2010]. Therefore, the random- and mixed-effects models are fit by first by estimating the amount of (residual) heterogeneity ($\sigma^2_{\text{level2}}$ and $\sigma^2_{\text{level3}}$), and then, the parameters defined above are estimated via weighted least squares with weights. There are several methods to estimate $\sigma^2_{\text{level2}}$ and $\sigma^2_{\text{level3}}$ heterogeneity --- see @veroniki2015 for different methods and specifics. This study uses the (restricted) maximum likelihood method (ML and REML). The estimated heterogeneity terms are then used to aggregate the primary study results using inverse-variance weighting [@borenstein_book2009]. In inverse-variance weighting, the effect size estimates with the lowest variance (higher sample sizes) are given more weight because they are more precise [@viechtbauer2010]. If the model was only taking into account the sampling variance then the weights are equal to $w_{ij} = 1/v_{ij}$. In this case there are three sources of heterogeneity the sum of which the is the model implied variances of the estimates: $w_{ij} = 1/(\hat{\sigma}^2_{\text{level3}}+\hat{\sigma}^2_{\text{level2}}+v_{ij})$. However, covariance between the effects needs to be taken into account, therefore the marginal variance-covariance matrix of the estimates.

To calculate the weights, let $\mathbf{y}$ be a the vector of observed effects ($\hat{\theta}_{ij}$) of length $n$ ($\mathbf{y} = \hat{\theta}_1, ....., \hat{\theta}_n$). The observations are organized as a series of independent groups, where the marginal variance-covariance matrix ($\mathbf{M}$) of the estimates account for the variance structure of the data. Since the effect sizes from different studies are assumed to be independent, the matrix takes a block-diagonal form. Where each block corresponds to a single study, with the diagonal elements representing the total variance for each outcome, and the off-diagonal elements within each block representing the shared between-study variance. The blocks themselves are independent, reflecting the assumption that there is no covariance between outcomes from different studies.

$$
\mathbf{M} = \begin{pmatrix}
\hat{\sigma}^2_{\text{level3}} + \hat{\sigma}^2_{\text{level2}} + v{_{\text{1}}} &
\hat{\sigma}^2_{\text{level3}} & 0 & 0 &... & 0   \\ 
\hat{\sigma}^2_{\text{level3}} & 
\hat{\sigma}^2_{\text{level3}} + \hat{\sigma}^2_{\text{level2}} + v{_{\text{2}}} &   
 0 & 0 &...&0 \\
\vdots & \vdots  & \vdots & \ddots  & \vdots  & \vdots \\
0 & 0 & 0 & 0 & \hat{\sigma}^2_{\text{level3}} + \hat{\sigma}^2_{\text{level2}} 
+ v{_{\text{n-1}}} &
 \hat{\sigma}^2_{\text{level3}} \\
0 & 0 & 0 & 0 & \hat{\sigma}^2_{\text{level3}} &
\hat{\sigma}^2_{\text{level3}} + \hat{\sigma}^2_{\text{level2}} + v{_{\text{n}}}
\end{pmatrix}
$$ {#eq-varcovar}

Let $\mathbf{W} = \mathbf{M^{-1}}$ be the weight matrix, where, $w_{rc}$ correspond to the $r$-th row and the $c$-th column of $\mathbf{W}$ and let $\hat{\theta_r}$ denote the $r$-th estimate, with $r = 1, ...., k$. Then the estimate of summary effect size $\hat{\mu}$ for the random-effects model, without covariances, i.e., intercept-only model, is given by [@pustejovsky; @viechtbauer_weights]

$$
\begin{aligned}
&\ \hat{\mu} = \frac{ \sum_{r= 1}^{k} (\sum_{c=1}^{k} w_{rc}) \hat{\theta}_{r}}
{\sum_{r=1}^{k}\sum_{c= 1}^{k} w_{rc}}\\
&\ \text{with } \\
&\ \overline{\sigma}^2 = \text{Var}(\hat{\mu}) = \frac{1}{\sum_{r=1}^{k}\sum_{c= 1}^{k} w_{rc}} \\
\end{aligned}
$$ {#eq-mu}

This is equivalent to the generalized least squares estimate for the fixed effects [@viechtbauer_weights]; $$
\mathbf{b}= (\mathbf{X'WX})^{-1}\mathbf{X'W}\mathbf{y}
$$ {#eq-GLS} Where $\mathbf{X}$ is the design matrix corresponding to the fixed effects, in the random-effects model case this is a single column of 1's as there are no predictors, but in the mixed effects model, $\mathbf{X}$ has $b'+1$ columns. In the mixed effects case the estimated parameters are $\mu$ and $\beta_{b'}$'s ($\mathbf{b}$). Following the recommendation of @assink2016, t-distribution was applied to assess the significance of individual regression coefficients in meta-analytic models, as well as to construct confidence intervals.
:::

[^methods-2]: Like $i$, $l$ refers to an effect size within the same study $j$. $z$ and $u$ refer to effect sizes in different clusters, where $u \neq j$ effect sizes are independent.

::: {style="text-align: justify"}
![(Place holder) Illustration of the 3-level random effects meta-analysis model. At level-1: The observed effects $\hat{\theta}_{ij}$ are modelled as random draws from a normal distribution centred around the true effect size $\theta_{ij}$, with known sampling variance $v_{ij}$. Observations from larger sample sizes $m_{ij}$ have smaller sampling variances, which are represented by the narrower distribution around $\hat{\theta}_{1j}$ compared to $\hat{\theta}_{2j}$. At Level 2: The true effects $\theta_{ij}$, from each study are modelled as normally distributed with mean $\kappa_{j}$ and within-study variance $\sigma^2_{\text{level2}}$. Large deviations of $\theta_{ij}$ from $\kappa_{j}$ indicate substantial within-study differences. In the mixed-effects model, the inclusion of Level 2 covariates $x_{ij}$ aiims to reduce within-study heterogeneity $\sigma^2_{\text{level2}}$ by explaining part of this variability. Lastly, at Level 3, study average effects are modelled as normally distributed with mean $\mu$ and between-study variance $\sigma^2_{\text{level3}}$. A large $\sigma^2_{\text{level3}}$ suggests substantial differences across studies, and the inclusion of Level 3 covariates $x_{j}$ aims to explain this heterogeneity.](figures/fig_levels.png){#fig-lvls alt="Illustration of the parameters for a 3 level random effects meta-analysis model" width="371"}
:::

::: {style="text-align: justify"}

#### Heterogeneity tests

To assess the significance of heterogeneity in the true effect sizes, the Cochran's Q statistic is used, with the null hypothesis assuming homogeneity of effect sizes. As defined by @cheung2014: 
$$
\begin{aligned}
&\ H_0: \theta_{r} = \theta \\
&\ Q = \sum^{k}_{r=1}w_{r}(\hat{\theta}_{r} - \hat{\mu}_{\text{fixed}})^2 \\
&\ \text{where  } w_{r} = \frac{1}{v_{r}}, \\
&\ \hat{\mu}_{\text{fixed}} =  \frac{\sum^{k}_{r=1}w_{r} \hat{\theta}_{r}}{\sum^{k}_{r=1}w_{r}} 
\end{aligned} 
$$ {#eq-Qstat} 
Under the null hypothesis Cochran's $Q$ has an approximate chi-squared distribution with $k -1$ degrees of freedom. Note, under the null hypothesis there are no cluster effects (no effect of the dependence) therefore the random effect terms are not considered for $w_{r}$ [@cheung2014]. The magnitude heterogeneity can be assessed using Higgins and Thompson [-@higgins2002] $I^2$, which reflects the proportion of total variation that is not attributable to sampling error (i.e., due to within- and between- study heterogeneity). Therefore $I^2_{\text{level2}}$ and Level 3 $I^2_{\text{level3}}$ are defined as follows [@cheung2014]:

$$
\begin{aligned}
&\ I^2_{\text{ level2}} = \frac{\hat{\sigma}^2_{\text{ level2}}}{\hat{\sigma}^2_{\text{ level2}} + \hat{\sigma}^2_{\text{ level3}} + \tilde{v}} \\
&\ \\
&\ I^2_{\text{ level3}} = \frac{\hat{\sigma}^2_{\text{ level3}}}{\hat{\sigma}^2_{\text{ level2}} + \hat{\sigma}^2_{\text{ level3}} + \tilde{v}}
\end{aligned}
$$ {#eq-Istat}

where $\tilde{v}$ is the typical sampling variance. Since the sampling variance differ across studies the typical variance is needed to estimate the magnitude. There are different ways to define the total variation [@cheung2014]. Here $\tilde{v}$ defined using Higgins and Thompson [-@higgins2002]:

$$
\tilde{v} = \frac{(k - 1) \sum^k_{r = 1} \frac{1}{v_{r}}}
{(\sum^k_{r = 1} \frac{1}{v_{r}})^2 - \sum^k_{r = 1} \frac{1}{v^2_{r}}} \\
$$ {#eq-typical_var}

Lastly, the percentage of variance explained by the mixed-effects can be quantified using $R^2$ [@cheung2014]; $$
\begin{aligned}
&\ R^2_{\text{ level2}} = 1 - \frac{\hat{\sigma}^2_{\text{ level2}(1)}}{\hat{\sigma}^2_{\text{ level2}(0)}} \\
&\ \\
&\ R^2_{\text{ level3}} = 1 - \frac{\hat{\sigma}^2_{\text{ level3}(1)}}{\hat{\sigma}^2_{\text{ level3}(0)}}
\end{aligned}
$$ {#eq-r2}

where, the variance is compaired before$_{(0)}$ and after$_{(1)}$ including predictors.

#### Model Selection

The multi-model inference function from the R package`dmetar` was used to select the best combination of covariates (i.e., the best model). Instead of sequentially adding or removing covariates (stepwise regression methods) this technique models all possible covariate combinations and compares them using an information-theoretic approach such as Akaike's Information Criterion (AIC) [@harrer2022, chap. 8]. Additionally, it assesses the importance of each covariate, calculated by summing the Akaike weights (or probabilities) of the models in which the covariate appears [@viechtbauer_modelselc]. Covariates that frequently appear in high-weight models are assigned higher importance values, indicating their consistent inclusion in the best-performing models[@viechtbauer_modelselc; @harrer2022, chap. 8]. It is important to note that the models will be refit from an REML to ML to make these comparisons [see @harrer2022, chap. 8].

#### Unweighted Approach

The unweighted least squares gives an estimate of the simple (unweighted) average of the population effect, given by [@laird1990] $$
\hat{\mu}_{_\text{UW}} = \frac{\sum \hat{\theta}_{r}}{k}
$$ {#eq-unweighted}

Unlike in the weighted approach methods, the observations from the primary studies, $\hat{\theta}_{ij}$ are not assumed to originate from a distribution. The study results are the unit of analysis rather than the sample components, therefore the Level 1 variance component is ignored. The unweighted effects model, focuses on between-study variance [@hall2018]. It achieves standard meta-analysis goals, such as describing central tendency, variance, and moderator effects, through an unconditional random effects approach[@hall2018]. A practical advantage of the unweighted model is that the effect sizes can be analyzed using standard descriptive and inferential statistics, t-tests, ANCOVA [see @khatami_meta-analysis_2016] and regression[see @hall2023].

#### Assumption of normality

The methods outlined assume that the distribution the effect size; if the number of studies collected is sufficiently large and the observed proportions are centred around 0.5, proportions follow an approximately symmetrical binomial distribution, making the normal distribution a good approximation [@wang2023]. However, in practice observed proportional data is rarely centred around 0.5 [@wang2023]. In this context in particular, the distribution of overall accuracy is likely skewed to the left as models are designed to maximize predictive power. Although the performance is dependent on the complexity and the quality of the data and some models could perform worse than random, their accuracies will not be much lower than 0.5, while well-performing models can achieve significantly higher accuracies, causing the center of accuracies to be pulled toward 1. In @khatami_meta-analysis_2016, the range of collected overall accuracy was between $14.0 \text{ to } 98.7\%$, with a median overall accuracy of $81.1 \% \text{ (IQR = } 68.9, 89.7 )$.

To address skewed observed proportions, transformation methods are applied, most commonly the logit or log-odds transformation. However, this method may not be appropriate in cases where the observed proportions are extremely low (near 0) or extremely high (near 1), as the transformations and their sampling variances can become undefined. In such cases, the Freeman-Tukey (FT) transformation is more appropriate, providing a more robust approach to dealing with skewed distributions of overall accuracy, especially when dealing with extreme values [@wang2023; @borgesmigliavaca2020]. The FT is calculated as follows [@freeman1950; @viechtbauer_web]:

$$
\hat{\theta}^{\text{FT}}_{r}=g(\hat{\theta}_{r}) = \frac{1}{2} \cdot \left( \text{arcsin} \sqrt{\frac{s_{r}}{m_{r}+1}} + \text{arcsin} \sqrt{\frac{s_{r}+1}{m_{r}+1}} \right) 
$$ {#eq-FTT}

where $\hat{\theta}^{\text{FT}}_{r}$ denotes the transformed $\hat{\theta}_{r}$, with variance: $$
\text{Var}(\hat{\theta}^{\text{FT}}_{r}) = v_{r} = \frac{1}{4m_{r} +2}
$$ {#eq-FTT_var}

To return to the pooled effect sizes natural scale, the @barendregt2013 back transformation is used, as instructed by @wang2023:

```{r eval=FALSE, echo=FALSE}
# from https://github.com/wviechtb/metafor/blob/master/R/transf.r#L198
# shorted version of how the back transformation function is defined 
#"
transf.ipft.hm <- function(xi, targs) {            # inverse of Freeman-Tukey transformation for a collection of proportions
   #.....
   nhm <- 1/(mean(1/ni, na.rm=TRUE))               # calculate harmonic mean of the ni's
   zi <- suppressWarnings(1/2 * (1 - sign(cos(2*xi)) * sqrt(1 - (sin(2*xi)+(sin(2*xi)-1/sin(2*xi))/nhm)^2)))
  #.....
}
# as suggested by Wang (2023) in the targ argument assigned: 
targ = list(ni=1/(pes.da.lvl3$se)^2)
#"
# therefore the backtransformation is:
```

$$
\hat{\mu}^{\text{B-FT}} = \frac{1}{2}
\left( 1- \text{sign(cos}(2\hat{\mu}^{\text{FT}})) \cdot
\sqrt{1 - \left(
\text{sin}(2\hat{\mu}^\text{FT}) + 
\frac{\text{sin}(2\hat{\mu}^\text{FT})- 1 / \text{sin}(2\hat{\mu}^\text{FT})}
{1/\overline{\sigma}^2_{\text{FT}}} 
 \right) ^2} \right)
$$ {#eq-FTT_back}

where $\hat{\mu}^{\text{FT}}$ is the (pooled) overall population average and $\overline{\sigma}^2_{\text{FT}}$ is the pooled variance, from @eq-mu but in the transformed scale [@wang2023].
:::
