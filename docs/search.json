[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Evaluating the Performance of Machine Learning Models in Remote Sensing for Sustainable Development Goals: A Meta-Analysis",
    "section": "",
    "text": "Forward\nThe following research is a meta-analysis on performace of machine learning models remote sensing monitoring of Sustainable Development Goals. This was an expository research assessing if study feastures can explain variation in study results. To my knowledge there is currently no published research using weighted meta-analysis techniques in this context. The research tried to follow the PRISMA guidlines where possible, but because of exposporitory nature of this research, both in terms of the research itself but also my own understanding and exporation into meta-analysis methods pre-registration was not done.\nThis thesis was predominatly redered directly from Quarto into PDF format, with only minor stylistic adjustments made at the end. Therefore, the code to rereder the entre project is avaliable on github. A HTML (link) version is also available, where code chunks are displayed in-line with the corresponding sections, offering a seamless integration of the analysis and narrative. The data analysis and appendixes are also available in this format This interactive format allows readers to engage with the code directly in the context of the research findings.\nIn addition, the official Leiden University template has been fully integrated into the Quarto book format used for this thesis. Future students and researchers can easily reuse this template, streamlining the formatting and presentation process for their own work.",
    "crumbs": [
      "Forward"
    ]
  },
  {
    "objectID": "frontmatter/abstract.html",
    "href": "frontmatter/abstract.html",
    "title": "Abstract",
    "section": "",
    "text": "Objective: This meta-analysis aims to evaluate machine learning methods in remote sensing applications for monitoring Sustainable Development Goals (SDGs). Specifically, the aims to (1) estimate the average performance (summary effect size); (2) determine the degree of heterogeneity within and across studies; (3) assess whether specific study features influence model performance, and (4) compare the sample-weighted and unweighted estimate summary effect.\nMethods: The meta-analysis used the PRISMA guidelines. A search was performed across multiple academic databases to identify peer-reviewed studies which applied machine learning models to remote sensing data for SDG monitoring. A random sample of 200 relevant studies was selected for abstract screening, which was reduced to \\(n = 20\\) studies with \\(k = 86\\) effect sizes for the analysis. To estimate the overall accuracy of machine learning models both a three-level random-effects model and an unweighted model were used.\nResults: The average overall accuracy of the unweighted model is 0.90 (95% CI [0.85; 0.94]), which is not substantially different from the weighted model at 0.89 (CI 95% [0.85, 0.94]. The weighted models found substantial heterogeneity between results. Unsirpirsingly, the proportion of the majority class was identified as the most important factor affecting the overall accuracy, followed by the inclusion of ancillary data. However, machine learning model group (i.e., neural networks, tree-based models) or SDG goal did not have a significant effect on the reported overall accuracy.\nConclusion: This study demonstrates the high variability model performance in remote sensing applications. As well as the impact class imbalance has on the reported overall accuracy. These findings suggest the need for precise metrics to assess model performance, particularly in imbalanced datasets. Future research should examine a broader range of performance metrics and explore additional study features to explore further what features affect the outcomes. In addition the robustness of the random-effects meta-analysis methods application to this field should be further examined.",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "frontmatter/abbreviations_notation.html",
    "href": "frontmatter/abbreviations_notation.html",
    "title": "Table of Notation",
    "section": "",
    "text": "The following table….\n\n\n\n\n\n\n\n\nNotation\nDefinition\nSection\n\n\n\n\n\\(_{rc}\\)\nIndex for the rows and columns of a matrix\n2  Background, 3  Methods\n\n\n\\(m_{rc}\\)\nConfusion martix: the number of instances where the actual class is \\(r\\) and the predicted class is \\(c\\). Where \\(r\\), is the row index, representing the actual class (reference) and \\(c\\) is the column index, representing the predicted class.\n2  Background\n\n\n\\(m\\)\nThe total number of instances in primary study’s dataset (sum of all cells).\n”\n\n\n\\(s\\)\nCorrectly classified instances\n”\n\n\n\\(n\\)\nTotal number of primary studies used in this study. In Figure 3.3 \\(n\\) is the number of studies in each section.\n3  Methods\n\n\n\\(j\\)\nThe study index: \\(j= 1,...,n\\)\n”\n\n\n\\(k_j\\)\ntotal number of effect sizes in the \\(j-\\)th study\n”\n\n\n\\(i\\)\nThe effect size index within a study. If a study reports two outcomes \\(i: \\{1, 2\\}\\). \\(i = 1, ..., k_j\\)\n”\n\n\n\\(m_{ij}, s_{ij}\\)\nThe total number of instances (the sample size) and correctly predicted class for each effect size with in each study\n”\n\n\n\\(k\\)\nTotal number of effect sizes gathered.\n”\n\n\n\\(\\theta, \\hat{\\theta}\\)\nTrue and observed effect size (overall accuracy)\n”\n\n\n\\(\\kappa_j\\)\nStudy average effect size.\n”\n\n\n\\(\\mu\\)\nSummary (population) effect size\n”\n\n\n\\(\\sigma^2_{\\text{level2}}\\)\nWithin-study heterogeneity\n”\n\n\n\\(\\sigma^2_{\\text{level3}}\\)\nBetween-study heterogeneity\n”",
    "crumbs": [
      "Table of Notation"
    ]
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In 2015, all United Nations member states adopted the Sustainable Development Goals (SDGs) to address global challenges such as climate change, environmental degradation, poverty, and inequality (UN DESA, 2023; UN-GGIM:Europe, 2019). This international plan outlines 17 global goals to achieve a better and more sustainable future (UN DESA, 2023; UN-GGIM:Europe, 2019; United Nations, 2024). Having passed the midpoint of the SDGs’ timeline with significant setbacks, the critical role of timely and high-quality data has never been more apparent (UN DESA, 2023; United Nations, 2024). These data are vital to identifying challenges, formulating evidence-based solutions, monitoring the implementation of solutions, and making essential course corrections (UN-GGIM:Europe, 2019). However, despite this necessity for high-quality data, traditional monitoring approaches, such as household- or field-level surveys (ground-acquired data), remain the primary source of data collection for key indicators of SDGs by National Statistical Institutes (NSIs) (Burke et al., 2021; UN-GGIM:Europe, 2019). These methods are expensive and time-consuming to conduct (Burke et al., 2021). As a result, the frequency of ground-acquired data varies significantly around the world; for example, the most recent agricultural census for 24% of the world’s countries was more than 15 years ago (Burke et al., 2021). Recognizing this challenge, both the United Nations SDG Report (2023, p. 49) and the Global Working Group on Big Data for Official Statistics underscore the importance of innovative methodology and data sources, including remote sensing and machine learning, to enhance the monitoring and implementation of the SDGs (UN-GGIM:Europe, 2019; United Nations, 2017).  \nRemote sensing — data collected from a distance via satellite, aircraft, or drones — offers a cost-effective approach for monitoring wide-ranging geographic areas (Khatami et al., 2016; Maso et al., 2023; UN-GGIM:Europe, 2019; Zhao et al., 2022). Remote sensing imagery has been limited to agricultural and socioeconomic applications for decades (Burke et al., 2021; Lavallin & Downs, 2021; Zhang et al., 2022). For instance, the Laboratory for Applications of Remote Sensing (LARS) has utilized satellite data and machine learning methods for crop identification since the 1960s (Holloway & Mengersen, 2018). However, in recent years, there has been a considerable increase in the spatial, spectral, and temporal resolution of remote sensing data, alongside a significant increase in free sensor data and computational power for complex data analysis (Burke et al., 2021; Thapa et al., 2023; Zhang et al., 2022). The magnitude of possible applications and increased availability of remote sensing data have rapidly increased the number of published research papers in this field (Burke et al., 2021; Khatami et al., 2016). Earth observation satellites alone can measure 42% of the SDG targets (Zhang et al., 2022).\nDespite the increased research, machine learning and remote sensing for SDG monitoring, there is still a lack of comprehensive understanding regarding the factors that determine the performance of these models across different contexts. The success of machine learning models in remote sensing depends on various factors, including the quality and resolution of input data, the choice of algorithm, the sample’s representativeness, and the complexity of the landscape (Heydari & Mountrakis, 2018; Lu & Weng, 2007). Additionally, model performance is often evaluated using localized datasets, which can limit the generalisability of findings and the ability to apply these models in broader contexts (Burke et al., 2021; Khatami et al., 2016; United Nations, 2017).\nAlthough the uptake of remote sensing data by NSIs has been slow, many NSIs are now capitalizing on the potential of using new and consistent data sources and methodologies to support and inform official statistics (United Nations, 2017). These can be generated by combining geospatial information, RS, and other big data sources, allowing for the filling of data gaps, providing information where no measurements were previously made, and improving the temporal and spatial resolutions of data (e.g., daily updates on crop area and yield statistics). Despite these advances, this paradigm shift from traditional statistical methods—such as counting and measuring by humans—towards estimation from sensors, simulation, and modeling, presents challenges (United Nations, 2017). It requires convincing, statistically sound results, rigorous validation, and a significant shift in resources within institutions to adapt to the higher spatial and temporal resolutions necessary to address emerging policy questions (United Nations, 2017).\nA meta-analysis statistically combines the body of evidence on a specific topic, aiming to produce unbiased summaries of evidence (Iliescu et al., 2022). There are many potential methods to choose from to combine results. One choice that is made when conducting a meta-analysis is whether to use the study’s sample size to weigh the result of each study (sample-weighted estimate) or an unweighted approach, which treats all results equally, disregarding sample size (Hall & Rosenthal, 2018). The current standard in meta-analysis research is to use the sample-weighted estimate (Hall & Rosenthal, 2018). The literature examined in this study found that previous meta-analyses investigating the performance of machine learning models on remote sensing data have predominantly relied on unweighted approaches. This meta-analysis is performed on peer-reviewed research articles that use machine learning methods and remote sensing data to monitor SDGs. This study aims to: (1) estimate the average performance (summary effect size); (2) determine the degree of heterogeneity within and across studies; (3) assess whether specific study features influence model performance; and (4) compare the sample-weighted and unweighted estimate summary effect.\n\n\n\n\n\nBurke, M., Driscoll, A., Lobell, D. B., & Ermon, S. (2021). Using satellite imagery to understand and promote sustainable development. Science, 371(6535), eabe8628. https://doi.org/10.1126/science.abe8628\n\n\nHall, J. A., & Rosenthal, R. (2018). Choosing between random effects models in meta-analysis: Units of analysis and the generalizability of obtained results. Social and Personality Psychology Compass, 12(10), e12414. https://doi.org/10.1111/spc3.12414\n\n\nHeydari, S. S., & Mountrakis, G. (2018). Effect of classifier selection, reference sample size, reference class distribution and scene heterogeneity in per-pixel classification accuracy using 26 Landsat sites. Remote Sensing of Environment, 204, 648–658. https://doi.org/10.1016/j.rse.2017.09.035\n\n\nHolloway, J., & Mengersen, K. (2018). Statistical Machine Learning Methods and Remote Sensing for Sustainable Development Goals: A Review. Remote Sensing, 10(9), 1365. https://doi.org/10.3390/rs10091365\n\n\nIliescu, D., Rusu, A., Greiff, S., Fokkema, M., & Scherer, R. (2022). Why We Need Systematic Reviews and Meta-Analyses in the Testing and Assessment Literature. European Journal of Psychological Assessment, 38(2), 73–77. https://doi.org/10.1027/1015-5759/a000705\n\n\nKhatami, R., Mountrakis, G., & Stehman, S. V. (2016). A meta-analysis of remote sensing research on supervised pixel-based land-cover image classification processes: General guidelines for practitioners and future research. Remote Sensing of Environment, 177, 89–100. https://doi.org/10.1016/j.rse.2016.02.028\n\n\nLavallin, A., & Downs, J. A. (2021). Machine learning in geography–Past, present, and future. Geography Compass, 15(5), e12563. https://doi.org/10.1111/gec3.12563\n\n\nLu, D., & Weng, Q. (2007). A survey of image classification methods and techniques for improving classification performance. International Journal of Remote Sensing, 28(5), 823–870. https://doi.org/10.1080/01431160600746456\n\n\nMaso, J., Zabala, A., & Serral, I. (2023). Earth Observations for Sustainable Development Goals. Remote Sensing, 15(10), 2570. https://doi.org/10.3390/rs15102570\n\n\nThapa, A., Horanont, T., Neupane, B., & Aryal, J. (2023). Deep Learning for Remote Sensing Image Scene Classification: A Review and Meta-Analysis. Remote Sensing, 15(19), 4804. https://doi.org/10.3390/rs15194804\n\n\nUN DESA. (2023). The Sustainable Development Goals Report 2023: Special Edition. United Nations. https://doi.org/10.18356/9789210024914\n\n\nUN-GGIM:Europe. (2019). The territorial dimension in SDG indicators: Geospatial data analysis and its integration with statistical data. Instituto Nacional de Estatística. https://un-ggim-europe.org/wp-content/uploads/2019/05/UN_GGIM_08_05_2019-The-territorial-dimension-in-SDG-indicators-Final.pdf\n\n\nUnited Nations. (2017). Earth observations for official statistics: Satellite imagery and geospatial data task team report. https://unstats.un.org/bigdata/task-teams/earth-observation/UNGWG_Satellite_Task_Team_Report_WhiteCover.pdf\n\n\nUnited Nations. (2024). The sustainable development goals report 2024. https://unstats.un.org/sdgs/report/2024/The-Sustainable-Development-Goals-Report-2024.pdf\n\n\nZhang, Y., Liu, J., & Shen, W. (2022). A Review of Ensemble Learning Algorithms Used in Remote Sensing Applications. Applied Sciences, 12(17), 8654. https://doi.org/10.3390/app12178654\n\n\nZhao, Q., Yu, L., Du, Z., Peng, D., Hao, P., Zhang, Y., & Gong, P. (2022). An Overview of the Applications of Earth Observation Satellite Data: Impacts and Future Trends. Remote Sensing, 14(8), 1863. https://doi.org/10.3390/rs14081863",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/background.html",
    "href": "chapters/background.html",
    "title": "2  Background",
    "section": "",
    "text": "Remote Sensing\n# data for the number of satellites\nUCS_Satellite_Database &lt;- read.csv(\"figures/UCS-Satellite-Database_5-1-2023.csv\")\n\nUCS_Satellite_Database_EarthObs &lt;- \n  UCS_Satellite_Database |&gt; \n  mutate(Date_Launch = as.Date(Date_Launch, format = \"%Y-%m-%d\"))|&gt;\n  filter(!grepl(\"Military\", Users))|&gt;\n  filter(grepl(\"Earth\",Purpose ))\n\nprop_after_2020&lt;- sum(year(UCS_Satellite_Database_EarthObs$Date_Launch)&gt;=2020,\n                      na.rm = TRUE)/ nrow(UCS_Satellite_Database_EarthObs)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#remote-sensing",
    "href": "chapters/background.html#remote-sensing",
    "title": "2  Background",
    "section": "",
    "text": "In the broadest sense, remote sensing involves acquiring information about an object or phenomenon without direct contact (Campbell & Wynne, 2011). More specifically, remote sensing refers to gathering data about land or water surfaces using sensors mounted on aerial or satellite platforms that record electromagnetic radiation reflected or emitted from the Earth’s surface (Campbell & Wynne, 2011, p. 6). The origins of remote sensing lie with the development of photography in the 19th century, with the earliest aerial or Earth Observation photographs taken with cameras mounted on balloons, kites, pigeons, and aeroplanes. (Burke et al., 2021; Campbell & Wynne, 2011, p. 7). The first mass use of remote sensing was during World War I with aerial photography. The modern era of satellite-based remote sensing started with the launch of Landsat 1 in 1972, the first satellite specifically designed for Earth Observation (Campbell & Wynne, 2011, p. 15). Today, remote sensing technology enables frequent and systematic collection of data about the Earth’s surface with global coverage, revolutionizing our ability to monitor and analyze the Earth’s surface (Burke et al., 2021; NASA, 2019). As of May 2023, roughly 1039 active nonmilitary Earth Observation satellites are in orbit; 51% were launched in 2020 (UCS, 2021).\n\n\n\n\n\nggplot(data = UCS_Satellite_Database_EarthObs|&gt; \n         group_by(week = lubridate::floor_date(Date_Launch, 'week'))|&gt;\n         summarise(n = n()),\n       aes(x = week, y = n)) +\n  geom_line(aes(y = cumsum(n)), na.rm = TRUE) +\n  \n  scale_x_date(date_breaks = \"5 years\", date_labels = \"%Y\") +\n  \n  labs(y = \"Number of Satellite\", x = \"Date\", \n        title = \"Number of Satellites Launched Over Time\") +\n  common_theme\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Number of active satellites by date of launch. Data acquired from UCS (2021).\n\n\n\n\nSensors on remote sensing devices such as satellites measure electromagnetic radiation reflected by objects on the Earth’s surface. This is done in two different ways: passive and active. Passive sensors rely on natural energy sources, like sunlight, to record incident energy reflected off the Earth’s surface. While active sensors generate their own energy, which is emitted and then measured as it reflects back from with the Earth’s surface (NASA, 2019).\n\n\n\n\n\n\n\nFigure 2.2: Illustration of a passive sensor and an active sensor. Source: NASA (2019) Applied Passive Sciences Remote Sensing Training Program.\n\n\n\n\nComponents of the Earth’s surface have different spectral signatures — i.e., reflect, absorb, or transmit energy in different amounts and wavelengths (Campbell & Wynne, 2011). Remote sensing devices have several sensors that measure specific ranges of wavelengths in the electromagnetic spectrum; these are referred to as spectral bands (e.g. visible light, infrared, or ultraviolet radiation) (NASA, 2019; SEOS, 2014). By capturing information from particular bands the spectral signatures of surfaces can be used to identify objects on the ground. Figure 2.3 illustrates the differences between the spectral signatures of soil, green vegetation, and water across various wavelengths. The grey bands in the figure represent the specific spectral bands on the Landsat TM satellite (SEOS, 2014). The distinct reflectance properties of each material within these bands enable the differentiation of surface materials, making it possible to identify different land cover types. This information can be used directly for classification, or it can be combined into indices—such as the Normalized Difference Vegetation Index (NDVI)—to enhance the detection of specific features like vegetation health and coverage (Campbell & Wynne, 2011; NASA, 2019). The \\(NDVI\\) uses red light and near-infrared (NIR) —given by \\(\\frac{NIR - Red}{NIR + Red}\\) — to distinguish green vegetation. Higher \\(NDVI\\) values indicate green vegetation as more red light is absorbed, whereas lower values correspond to non-vegetated areas where more red light is reflected.\n\n\n\n\n\n\n\nFigure 2.3: Spectral signatures of soil, green vegetation, and water across different wavelengths, representing the portion of incident radiation that is reflected by each material as a function of wavelength. The grey bands indicate the spectral ranges (channels) of Landsat TM satellite. Bands 1-3 capture visible light (Blue, Green, Red), while Band 4 captures near-infrared (NIR), and Bands 5 and 7 cover parts of the intermediate infrared spectrum. These spectral bands allow for the differentiation of various surface materials based on their unique reflectance properties. Source: Siegmund and Menz (2005) as cited and modified by SEOS (2014).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#machine-learning",
    "href": "chapters/background.html#machine-learning",
    "title": "2  Background",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nMachine learning techniques such as neural networks, random forests, and support vector machines have long been applied for spatial data analysis and geographic modeling (Haddaway et al., 2022; Lavallin & Downs, 2021). Compared to using indices alone, machine learning techniques enhance the accuracy and efficiency of data analysis and interpretation processes making it possible to analyze large volumes of data effectively. Which is particularly useful for handling the high complexity and dimensionality of remote sencing data. In recent years, the application of machine learning techniques in remote sensing has surged, driven by the increasing availability of large datasets and advancements in computational power (UN-GGIM:Europe, 2019; Y. Zhang et al., 2022). These machine learning models can be grouped into four main types according to the aims of analyses: classification, clustering, regression, and dimension reduction. Table 2.1 describes this grouping as well as giving examples. It is important to note that recent trends in machine learning and remote sensing analyses use hybrid or ensemble approaches using a combination of these groups (UN-GGIM:Europe, 2019). For a thorough review of these methods see UN-GGIM:Europe (2019).\n\n\n\n\nTable 2.1: Categories of machine learning methods grouped according to the analytic aim\n\n\n\nread_excel(\"figures/summary_tables.xlsx\", sheet = \"ML_cat\")|&gt;\n  kable(booktabs = TRUE, linesep = \"\") |&gt;\n  kable_styling(font_size = 10, \n                full_width = FALSE)|&gt;\n  column_spec(1, width = \"2cm\")|&gt;\n  column_spec(2, width = \"14cm\")|&gt;\n  \n  footnote(c(\"Adapted from UN-GGIM:Europe (2019) and Haddaway et al.(2022).\"), \n           threeparttable=TRUE)\n\n\n\n\nAnalysis aim\nExplanation\n\n\n\n\nClassification\nAssigning objects to known classes based on input variables. For example, categorizing pixels in an image into crop types using a model trained on known data.\n\n\nRegression\nPredict a numeric (discrete or continuous) response variable based on input variables, similar to classification but with numeric outputs. An example is predicting crop yield from Earch Observation image data.\n\n\nClustering\nGroups objects based on input variables without pre-defined classes, identifying similarities among the objects. This can help in grouping pixels in an image for further inspection.\n\n\nDimension reduction\nReduces a large set of variables to a smaller set that retains most of the original information. This can simplify analysis or generate new variables like indices (e.g., Vegetation Index) for interpretation.\n\n\n\nNote: \n\n\n\n Adapted from UN-GGIM:Europe (2019) and Haddaway et al.(2022).\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo verify these analyses performance metrics are used. For classification tasks, this involves creating a confusion matrix — a cross-tabulation of class labels assigned to model predictions and reference data (ground truth). In a confusion matrix the correctly classified instances are on the diagonal, and the off-diagonal cells indicate which classes are confused (i.e., are incorrectly classified). In remote sensing applications, accuracy assessments are undertaken on a pixel, group of pixels (e.g. block), or an object level (Stehman & Foody, 2019).\n\n\n\n\nTable 2.2: Confusion matrix of four classes\n\n\n\nconfusion &lt;- data.frame(\n  Reference = c(\"Class 1\",\"Class 2\",\"Class 3\",\"Class 4\", \"Total\", \"User's accuracy\"), \n  #Predictions \n  c1 = c(\"$m_{11}$\", \"$m_{21}$\", \"$m_{31}$\", \"$m_{41}$\", \"$m_{.1}$\", \"$m_{11}/m_{.1}$\"),\n  c2 = c(\"$m_{12}$\", \"$m_{22}$\", \"$m_{32}$\", \"$m_{42}$\", \"$m_{.2}$\", \"$m_{22}/m_{.2}$\"),\n  c3 = c(\"$m_{13}$\", \"$m_{23}$\", \"$m_{33}$\", \"$m_{43}$\", \"$m_{.3}$\", \"$m_{33}/m_{.3}$\"),\n  c4 = c(\"$m_{14}$\", \"$m_{24}$\", \"$m_{33}$\", \"$m_{44}$\", \"$m_{.4}$\", \"$m_{44}/m_{.4}$\"),\n  total = c(\"$m_{1.}$\", \"$m_{2.}$\", \"$m_{3.}$\", \"$m_{4.}$\", \"$m$\", \"\"),\n  pa = c(\"$m_{11}/m_{1.}$\", \"$m_{22}/m_{2.}$\",\n         \"$m_{33}/m_{3.}$\", \"$m_{44}/m_{4.}$\", \"\", \"\")\n)\nconfusion|&gt;\n  kable(booktabs = TRUE, linesep = \"\", escape = FALSE, \n        col.names = c(\"Reference\",\"Class 1\",\"Class 2\",\"Class 3\",\"Class 4\", \"Total\", \"Producer's accuracy\")\n        ) |&gt;\n  kable_styling(font_size = 10, \n                full_width = FALSE)|&gt;\n  add_header_above(c(\"\", \"Predictions\" = 6), bold = TRUE) |&gt;\n  row_spec(0, bold = TRUE)|&gt;\n  column_spec(1, bold = TRUE)|&gt;\n  footnote(c(\"Confusion matrix for a classification with four classes, where the rows ($r$) represent the reference (observed) classification and the columns ($c$) represent the predicted classes. $m_{rc}$ is the number of instances predicted in reference class $r$ and predicted class $c$, and $m$ is the total number of instances (i.e., the number of pixels/objects classified).\"), \n           threeparttable=TRUE, escape=FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictions\n\n\n\nReference\nClass 1\nClass 2\nClass 3\nClass 4\nTotal\nProducer's accuracy\n\n\n\n\nClass 1\n$m_{11}$\n$m_{12}$\n$m_{13}$\n$m_{14}$\n$m_{1.}$\n$m_{11}/m_{1.}$\n\n\nClass 2\n$m_{21}$\n$m_{22}$\n$m_{23}$\n$m_{24}$\n$m_{2.}$\n$m_{22}/m_{2.}$\n\n\nClass 3\n$m_{31}$\n$m_{32}$\n$m_{33}$\n$m_{33}$\n$m_{3.}$\n$m_{33}/m_{3.}$\n\n\nClass 4\n$m_{41}$\n$m_{42}$\n$m_{43}$\n$m_{44}$\n$m_{4.}$\n$m_{44}/m_{4.}$\n\n\nTotal\n$m_{.1}$\n$m_{.2}$\n$m_{.3}$\n$m_{.4}$\n$m$\n\n\n\nUser's accuracy\n$m_{11}/m_{.1}$\n$m_{22}/m_{.2}$\n$m_{33}/m_{.3}$\n$m_{44}/m_{.4}$\n\n\n\n\n\nNote: \n\n\n\n\n\n\n\n\n Confusion matrix for a classification with four classes, where the rows ($r$) represent the reference (observed) classification and the columns ($c$) represent the predicted classes. $m_{rc}$ is the number of instances predicted in reference class $r$ and predicted class $c$, and $m$ is the total number of instances (i.e., the number of pixels/objects classified).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this matrix, performance measures such as overall accuracy are derived (FAO, 2016; Stehman & Foody, 2019; UN-GGIM:Europe, 2019). Where the overall accuracy is the total number of successful classifications, \\(s\\) over total number of instances, \\(m\\).\n\\[\n\\text{Overall Accuracy (OA)} = \\frac{\\sum^q_{r=1}m_{rr}}{m}= \\frac{s}{m}\n\\tag{2.1}\\]\nIf the unit of accuracy assessment is a pixel, then overall accuracy is the proportion of pixels classified correctly. Other metrics include the reliability (User’s accuracy) and sensitivity (recall or Producer’s accuracy). Reliability is the correct classifications for a particular class divided by the column total (\\(m_{.c}\\)) and sensitivity is correct classifications over the row total (\\(m_{r.}\\)). It is important to consider the purpose of the map when evaluating its accuracy, as overall accuracy may not reflect the accuracy of specific classes. Factors such as sample size, class stability, class proportions, and landscape variability influence the overall accuracy (FAO, 2016; see UN-GGIM:Europe, 2019).\n\nAustralia Land Cover Mapping\nTo illustrate how remote sensing data and machine leaning can be used to support ecological sustainable development, Owers et al. (2022) developed an approach to monitor and map land cover across Australia using techniques. Their study utilized Landsat sensor data archive through Digital Earth Australia to generate annual land cover maps from 1988 to 2020 at a 25-meter resolution. The study used random forest and artificial neural networks to classify individual pixels according to the FAO’s Land Cover Classification System (LCCS) framework.\n\n\n\n\n\n\n\n\nFigure 2.4: Land cover mapping created by Owers et al. (2022) using Landstat data to make continent-wide classifications using the LCCS frame work which differentiates six (classes) land cover types: cultivated terrestrial vegetation (CTV), natural terrestrial vegetation (NTV), natural aquatic vegetation (NAV), artificial surfaces (AS), bare surfaces (BS), and water bodies (W).\n\n\n\n\nTo produce such maps using a topographical field survey is impractical, given Australia’s size (\\(7,688,287 \\text{ km}^2\\)). While field surveys are the most accurate method of generating training sample data, they are labor-intensive, time-consuming, and expensive (C. Zhang & Li, 2022). A topographical survey of just 20 hectares (\\(0.2 \\text{ km}^2\\)) takes a team of four people approximately five days to complete, even though the resulting topographical map would have a high resolution of 0.5 meters (L.A. Mbila, personal communication, January 26, 2024). In Owers et al. (2022), experts visually inspected the satellite imagery to validate the training and test data. While this is a less labor-intensive, costly and time-consuming than field surveys it still requires significant effort and expertise.\nIn contrast to the challenges associated with field surveys, remote sensing provides an efficient method for the continuous monitoring of large areas that would otherwise be inaccessible (Owers et al., 2022; C. Zhang & Li, 2022). Thefore, the potential applications are numerous. Examples include monitoring of land use and degradation, forestry, biodiversity, agriculture, disaster prediction, water resources, public health, urban planning, poverty, and the management and preservation of world heritage sites (Anshuka et al., 2019; Campbell & Wynne, 2011; Ekmen & Kocaman, 2024; O. Hall et al., 2023; Lavallin & Downs, 2021; Maso et al., 2023).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#australia-land-cover-mapping",
    "href": "chapters/background.html#australia-land-cover-mapping",
    "title": "2  Background",
    "section": "Australia Land Cover Mapping",
    "text": "Australia Land Cover Mapping\nTo illustrate how remote sensing data and machine leaning can be used to support ecological sustainable development, Owers et al. (2022) developed an approach to monitor and map land cover across Australia using techniques. Their study utilized Landsat sensor data archive through Digital Earth Australia to generate annual land cover maps from 1988 to 2020 at a 25-meter resolution. The study used random forest and artificial neural networks to classify individual pixels according to the FAO’s Land Cover Classification System (LCCS) framework.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#previous-reviews",
    "href": "chapters/background.html#previous-reviews",
    "title": "2  Background",
    "section": "Previous Reviews",
    "text": "Previous Reviews\n\nNumerous studies have previously examined the application of remote sensing for SDG monitoring. However, existing reviews are typically either limited to specific contexts, such as the use of satellite data for poverty estimation (O. Hall et al., 2023) or focus on descriptive results (see Yin et al., 2023). The existing reviews either apply methodology that aligns more closely with Synthesis Without Meta-Analysis (Campbell et al., 2020) —for example, Thapa et al. (2023) and Ekmen & Kocaman (2024) — or apply unweighted meta-analysis techniques, such as Khatami et al. (2016) and O. Hall et al. (2023) ). In unweighted meta-analysis all studies are treated equally regardless of their sample size, quality, or variance (J. A. Hall & Rosenthal, 2018). However, it is more common in traditional applications of meta-analysis, to use the sample sizes when aggregating individual studies (J. A. Hall & Rosenthal, 2018). However, to my knowledge, no examples of a weighted meta-analysis applied to predictive performance in remote sensing data have been conduced, highlighting a gap that this study aims to address.\n\n\n\n\n\nAnshuka, A., Ogtrop, F. F. van, & Willem Vervoort, R. (2019). Drought forecasting through statistical models using standardised precipitation index: A systematic review and meta-regression analysis. Natural Hazards, 97(2), 955–977. https://doi.org/10.1007/s11069-019-03665-6\n\n\nBurke, M., Driscoll, A., Lobell, D. B., & Ermon, S. (2021). Using satellite imagery to understand and promote sustainable development. Science, 371(6535), eabe8628. https://doi.org/10.1126/science.abe8628\n\n\nCampbell, McKenzie, J. E., Sowden, A., Katikireddi, S. V., Brennan, S. E., Ellis, S., Hartmann-Boyce, J., Ryan, R., Shepperd, S., Thomas, J., Welch, V., & Thomson, H. (2020). Synthesis without meta-analysis (SWiM) in systematic reviews: reporting guideline. BMJ, 368, l6890. https://doi.org/10.1136/bmj.l6890\n\n\nCampbell, & Wynne, R. H. (2011). Introduction to remote sensing (5th ed). Guilford Press.\n\n\nEkmen, O., & Kocaman, S. (2024). Remote sensing for UN SDGs: A global analysis of research and collaborations. The Egyptian Journal of Remote Sensing and Space Sciences, 27(2), 329–341. https://doi.org/10.1016/j.ejrs.2024.04.002\n\n\nFAO, F. and A. O. (2016). Map accuracy assessment and area estimation practical guide., http://www.fao.org/3/a-i5601e.pdf\n\n\nHaddaway, N. R., Bannach-Brown, A., Grainger, M. J., Hamilton, W. K., Hennessy, E. A., Keenan, C., Pritchard, C. C., & Stojanova, J. (2022). The evidence synthesis and meta-analysis in R conference (ESMARConf): Levelling the playing field of conference accessibility and equitability. Systematic Reviews, 11(1), 113. https://doi.org/10.1186/s13643-022-01985-6\n\n\nHall, J. A., & Rosenthal, R. (2018). Choosing between random effects models in meta-analysis: Units of analysis and the generalizability of obtained results. Social and Personality Psychology Compass, 12(10), e12414. https://doi.org/10.1111/spc3.12414\n\n\nHall, O., Dompae, F., Wahab, I., & Dzanku, F. M. (2023). A review of machine learning and satellite imagery for poverty prediction: Implications for development research and applications. Journal of International Development, 35(7), 1753–1768. https://doi.org/10.1002/jid.3751\n\n\nKhatami, R., Mountrakis, G., & Stehman, S. V. (2016). A meta-analysis of remote sensing research on supervised pixel-based land-cover image classification processes: General guidelines for practitioners and future research. Remote Sensing of Environment, 177, 89–100. https://doi.org/10.1016/j.rse.2016.02.028\n\n\nLavallin, A., & Downs, J. A. (2021). Machine learning in geography–Past, present, and future. Geography Compass, 15(5), e12563. https://doi.org/10.1111/gec3.12563\n\n\nMaso, J., Zabala, A., & Serral, I. (2023). Earth Observations for Sustainable Development Goals. Remote Sensing, 15(10), 2570. https://doi.org/10.3390/rs15102570\n\n\nNASA. (2019). What is Remote Sensing? https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing\n\n\nOwers, C. J., Lucas, R. M., Clewley, D., Tissott, B., Chua, S. M. T., Hunt, G., Mueller, N., Planque, C., Punalekar, S. M., Bunting, P., Tan, P., & Metternicht, G. (2022). Operational continental-scale land cover mapping of Australia using the Open Data Cube. International Journal of Digital Earth, 15(1), 1715–1737. https://doi.org/10.1080/17538947.2022.2130461\n\n\nSEOS. (2014). Introduction to remote sensing. https://seos-project.eu/remotesensing/remotesensing-c01-p06.html\n\n\nStehman, S. V., & Foody, G. M. (2019). Key issues in rigorous accuracy assessment of land cover products. Remote Sensing of Environment, 231, 111199. https://doi.org/10.1016/j.rse.2019.05.018\n\n\nThapa, A., Horanont, T., Neupane, B., & Aryal, J. (2023). Deep Learning for Remote Sensing Image Scene Classification: A Review and Meta-Analysis. Remote Sensing, 15(19), 4804. https://doi.org/10.3390/rs15194804\n\n\nUCS. (2021). Union of Concerned Scientists (UCS) Satellite Database. https://www.ucsusa.org/resources/satellite-database\n\n\nUN-GGIM:Europe. (2019). The territorial dimension in SDG indicators: Geospatial data analysis and its integration with statistical data. Instituto Nacional de Estatística. https://un-ggim-europe.org/wp-content/uploads/2019/05/UN_GGIM_08_05_2019-The-territorial-dimension-in-SDG-indicators-Final.pdf\n\n\nYin, C., Peng, N., Li, Y., Shi, Y., Yang, S., & Jia, P. (2023). A review on street view observations in support of the sustainable development goals. International Journal of Applied Earth Observation and Geoinformation, 117, 103205. https://doi.org/10.1016/j.jag.2023.103205\n\n\nZhang, C., & Li, X. (2022). Land Use and Land Cover Mapping in the Era of Big Data. Land, 11(10), 1692. https://doi.org/10.3390/land11101692\n\n\nZhang, Y., Liu, J., & Shen, W. (2022). A Review of Ensemble Learning Algorithms Used in Remote Sensing Applications. Applied Sciences, 12(17), 8654. https://doi.org/10.3390/app12178654",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html",
    "href": "chapters/methods.html",
    "title": "3  Methods",
    "section": "",
    "text": "Specific inclusion and exclusion criteria\nAfter removing review articles and non-research papers, a total of 811 relevant articles remained. Of these potentially relevant papers, 35% were published in 2023, highlighting the growth of research in this field. The trend, as illustrated in Figure 3.1, is consistent with other similar research, for example, Ekmen & Kocaman (2024), which reported a sharp increase in publications related to ML and RS for SDG monitoring.\nDue to the large number of papers remaining, a random sample of 200 articles was drawn for title and abstract screening. These potentially relevant articles were screened independently by three reviewers (the author and two internal supervisors) using the R package metagear (Lajeunesse, 2016). The papers were selected according to the following criteria: a) publications utilizing remote sensing and ML techniques, (b) indication of a quality assessment for example overall accuracy. Table 3.2 shows the words highlighted in the abstract screening phase to aid the reviewers and Figure 3.2 shows the user interface highlighting these keywords.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html#formulating-the-review-question-and-protocol",
    "href": "chapters/methods.html#formulating-the-review-question-and-protocol",
    "title": "3  Methods",
    "section": "Formulating the review question and protocol",
    "text": "Formulating the review question and protocol\nThe PICOTS (population, intervention, comparison, outcome, timing, and setting) system was used to frame the review aims for this analysis (Debray et al., 2017). Based on this framework, the question was formulated as follows: In studies focused on SDGs, how heterogeneous is the performance of ML applied to various remote sensing applications, and what study features account for any observed differences in model performance?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html#specific-inclusion-and-exclusion-criteria",
    "href": "chapters/methods.html#specific-inclusion-and-exclusion-criteria",
    "title": "3  Methods",
    "section": "",
    "text": "Figure 3.1: Publication increase between 2018 and 2022.\n\n\n\n\n\n\n\nTable 3.2: Keywords\n\n\n\n\n\n\n\nCategory\nKeywords\n\n\n\n\nGeneral\nempirical, result, predictive, analysis, sustainable development goal, sustainable development\n\n\nData related\nremotely sensed, remote sensing, satellite, earth observation\n\n\nModels\ndeep learning, machine learning, classification, classifier, regression, supervised, test set, training set, cart, svm, rf, ann, random forest, support vector machine, regression tree, decision tree, neural network, boosting, bagging, gradient, bayes\n\n\nQuality metrics\noverall accuracy, accuracy, coefficient of determination, rmse, mse, f1, precision, auc, roc, recall, sensitivity, specificity, mean absolute error, error, mae\n\n\nTo omit\nsystematic review, meta-analysis, review\n\n\n\nNote: \n\n\n\n Keywords highlighted by the `metagear` user interface during abstract screening phase as a visual cue to speed up the screening process.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Metagear graphical user interface: Example of the metagear abstract screener interface, with key words highlighted. On the bottom left the reviewer can select whether the paper is relevant.\n\n\n\n\nAs shown in Figure 3.3, of the 200 abstracts screened only 57 were deemed potentially relevant by all three reviewers. To have comparable performance metrics it decided to focus on papers related to classification. The titles and abstracts of the 57 articles were screened using metagear dividing them to classification (40) and regression (17) papers. In the 40 papers, overall accuracy was the most commonly reported outcome metric and therefore it was decided to include all papers that report overall accuracy.\n\n\n\n\n\n\n\nFigure 3.3: PRIMSA flow diagram of manuscript selection. The records were identified from databases including Web of Science (WOS), ScienceDirect, PubMed, Journal Storage (JSTOR), American Geophysical Union Publications (Agupubs), EBSCO, IEEE Xplore, Multidisciplinary Digital Publishing Institute (MDPI), ProQuest, and Taylor & Francis Online (Tandfonline), no papers were gathered from official registers. Note: number of records removed four where not journal articles and 27 were omitted for being reviews. A random sample of 200 of the total 884 was drawn and reviewed by three independent reviewers. A total of 57 records were left, 40 of which were deemed to be classification papers and the full text screened.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html#feature-collection",
    "href": "chapters/methods.html#feature-collection",
    "title": "3  Methods",
    "section": "Feature collection",
    "text": "Feature collection\nUsing the first 10 papers and previous systematic reviews, a list of potential study features was created and structured in a table for data collection. Table 3.3 outlines all the extracted features and study identification information. The features in the table are grouped according to their use in the analysis. These features include methodology and data characteristics, which provide information about the complexity of the classification tasks (e.g., the number of output classes) and the proportion of the majority class, indicating potential class imbalance issues that can affect the performance of classification models. Remote sensing-specific information was also gathered, including the type of devices, spectral bands, and spatial resolution to assess how data collection impacts performance. The reported overall accuracy is the effect size of interest, and the sample size is important for the weighted meta-analysis. The other features are used to help explain some variation in effect sizes. The sample size is also used as a feature, as larger sample sizes might influence overall accuracy. Of the extracted features, the number of spectral bands and spatial resolution were categorized due to high levels of non-reporting. The type of remote sensing device was excluded because only one study did not use satellite data, and the specifics of the spectral bands used were too different to make meaningful groups. Several potentially useful features were not recorded, including temporal resolution (the frequency of data collection) and pre-processing steps, which also impact the performance of the model. These were excluded as the differences between papers were too large to make groups. The number of citations was gathered using the Local Citation Network web app, which collects article metadata from OpenAlex—a bibliographic catalog of scientific papers (Priem et al., 2022)1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html#statistical-analysis",
    "href": "chapters/methods.html#statistical-analysis",
    "title": "3  Methods",
    "section": "Statistical analysis",
    "text": "Statistical analysis\nA meta-analysis is a statistical method that aggregates results from several primary studies to assess and interpret the collective evidence on a specific topic or research question. Specifically, the aim is to (a) determine the average (summary) effect, (b) establish the degree of heterogeneity between effect sizes, and (c) access if study characteristics can explain any of the heterogeneity of the effect sizes (Cheung, 2014). In this case the effect size (dependent variable) of interest is the overall accuracy. Let \\(\\hat{\\theta}_{ij}\\) be the \\(i-\\)th observed effect size in study \\(j\\) (where \\(i = 1, ..., k_j\\), \\(j = 1, ..., n\\)). From Equation 2.1, the overall accuracy is the proportion of correctly classified instances, therefore, the effect size is:\n\\[\n\\begin{aligned}\n&\\ \\hat{\\theta}_{ij} = \\frac{s_{ij}}{m_{ij}}\\\\\n&\\ v_{ij} = \\frac{\\hat{\\theta}_{ij}(1-\\hat{\\theta}_{ij})}{m_{ij}}\n\\end{aligned}\n\\tag{3.1}\\] where \\(s_{ij}\\) is the number of successful predictions and \\(m_{ij}\\) is total number of pixels or objects classified.\n\nWeighted Approach\nBefore conducting the meta-analysis, first the structure of the collected data and assumption of independence of effect sizes need to be addressed. In the context of this research, dependencies are introduced since all reported effect sizes from each study are included. The degree of dependence between effect sizes can be categorized as either known or unknown (Cheung, 2014). Multivariate meta-analytic techniques use known dependencies reported in the primary studies, such as reported correlation coefficients (Cheung, 2014). However, dependency estimates between outcomes are rarely reported (Assink & Wibbelink, 2016). Therefore, to model these unknown dependencies a 3-level random-effects meta-analytic model is used. The three-level meta-analysis approach models three different variance components distributed over three levels:\nAt level 1, the sampling variance of the effect sizes is modeled as: \\[\n\\begin{aligned}\n&\\ \\text{Level 1:  } \\hat{\\theta}_{ij} = \\theta_{ij} + \\epsilon_{ij}, \\\\\n&\\  \\epsilon_{ij} \\sim \\mathcal{N}(0, v_{ij}).\\\\\n\\end{aligned}\n\\tag{3.2}\\]\nThe observed overall accuracy \\(\\hat{\\theta}_{ij}\\) is an estimate of overall accuracy from experiment \\(i\\) in study \\(j\\) and is modelled as the true overall accuracy, \\(\\theta_{ij}\\) and error component \\(\\epsilon_{ij}\\) which is normally distributed with mean \\(0\\) and known variance \\(v_{ij}\\). A model that only takes into account sampling variance is referred to as a fixed-effects model, where it is assumed that all studies included in the meta-analysis share a single true effect size, and therefore, the only source of variation between effect sizes is the sampling variance. The fixed-effects model assumes homogeneity across studies and allows for conditional inference about the specific set of studies included in the analysis, without accounting for variability that might arise from differences between studies. The inclusion of the random effects (at level 2 and 3) means that as well as sampling variance, the heterogeneity due to differing between and within study features are also taken into account (Harrer et al., 2022; Schwarzer et al., 2015, p. 34; Wang, 2023). Therefore, the addition random effect components allow one to make unconditional inferences about the population from which the included studies are a random sample.\nAt level 2, within-study heterogeneity (\\(\\sigma^2_{\\text{level2}}\\)) is modelled as: \\[\n\\begin{aligned}\n&\\ \\text{Level 2:  } \\theta_{ij} = \\kappa_j + \\zeta_{ij}, \\\\\n&\\  \\zeta_{ij} \\sim \\mathcal{N}(0, \\sigma^2_{\\text{level2}}).\\\\\n\\end{aligned}\n\\tag{3.3}\\] The true overall accuracy \\(\\theta_{ij}\\) is modelled as the average overall accuracy, \\(\\kappa_{j}\\) of study \\(j\\) and study-specific heterogeneity \\(\\zeta_{ij}\\), which is normally distributed with mean \\(0\\) and variance \\(\\sigma^2_{\\text{level2}}\\).\nLastly, level 3, the variance between heterogeneity (\\(\\sigma^2_{\\text{level3}}\\)) is modelled as: \\[\n\\begin{aligned}\n&\\ \\text{Level 3:  } \\kappa_j = \\mu + \\xi_{j}, \\\\\n&\\  \\xi_{j} \\sim \\mathcal{N}(0, \\sigma^2_{\\text{level3}}).\\\\\n\\end{aligned}\n\\tag{3.4}\\]\nThe average overall accuracy \\(\\kappa_{j}\\) of study \\(j\\) is modelled as the average population effect \\(\\mu\\) and between-study heterogeneity \\(\\xi_{j}\\), which is normally distributed with mean \\(0\\) and variance \\(\\sigma^2_{\\text{level3}}\\). Combined, the three-level meta-analysis models the observed effect size modelled as the sum of the average population effect \\(\\mu\\) and these three error components: \\[\n\\hat{\\theta}_{ij} = \\mu + \\xi_j + \\zeta_{ij} + \\epsilon_{ij}.\n\\tag{3.5}\\]\nFor the expected value of the observed effect size to be the population average, \\(\\mathbb{E}(\\hat{\\theta}_{ij}) = \\mu\\), the random effects at the different levels and the sampling variance are assumed independent: \\(\\text{Cov}(\\xi_j, \\zeta_{ij}) = \\text{Cov}(\\xi_j, \\epsilon_{ij}) = \\text{Cov}(\\zeta_{ij}, \\epsilon_{ij}) = 0\\). Therefore, (1) unconditional sampling variance of the effect size is the sum of level 3 and level 2 heterogeneity, and the known sampling variance: \\(\\text{Var}(\\hat{\\theta}_{ij}) = \\sigma^2_{\\text{level3}} +\\sigma^2_{\\text{level2}} + v_{ij}\\), (2) the effect sizes within the same study share the same covariance \\(\\text{Cov}(\\hat{\\theta}_{ij}, \\hat{\\theta}_{lj}) = \\sigma^2_{\\text{level3}}\\), and (3) the effect sizes in different studies are independent \\(\\text{Cov}(\\hat{\\theta}_{ij}, \\hat{\\theta}_{zu}) = 0\\) (Cheung, 2014)2.\nThe random-effects model can be extended to a mixed-effects model (also referred to as a meta-regression) by including study characteristics as covariates (predictors). Let \\(x\\) denote the value covariate, where \\(b'\\) refers to the number of covariates included in the model. These covariates can be either \\(x_{ij}\\) for a level-2 covariate or \\(x_j\\) for a level-3 covariate. The mixed-effect model defined as: \\[\n\\hat{\\theta}_{ij} = \\mu + \\beta_1 x_{ij1} + .... + \\beta_{b'} x_{jb'} + \\xi_j + \\zeta_{ij} + \\epsilon_{ij}\n\\tag{3.6}\\] The assumptions here remain the same as Equation 3.5, but the heterogeneity (\\(\\sigma^2_{\\text{level3}}, \\sigma^2_{\\text{level2}}\\)) is the variability among the true effects which is not explained by the included covariates (Cheung, 2014; Viechtbauer, 2010). The aim of the mixed-effects model is to examine the extent to which the included covariates in the model influence the overall population average \\(\\mu\\) and the heterogeneity \\(\\sigma^2_{\\text{level3}}\\) and \\(\\sigma^2_{\\text{level2}}\\) (Viechtbauer, 2010).\nIn this way, meta-analytic models are essentially, special cases of the general linear (mixed effects) model with heteroscedastic sampling variances which are assumed to be known (Viechtbauer, 2010). Therefore, the random- and mixed-effects models are fit by first by estimating the amount of (residual) heterogeneity (\\(\\sigma^2_{\\text{level2}}\\) and \\(\\sigma^2_{\\text{level3}}\\)), and then, the parameters defined above are estimated via weighted least squares with weights. There are several methods to estimate \\(\\sigma^2_{\\text{level2}}\\) and \\(\\sigma^2_{\\text{level3}}\\) heterogeneity — see Veroniki et al. (2015) for different methods and specifics. This study uses the (restricted) maximum likelihood method (ML and REML). The estimated heterogeneity terms are then used to aggregate the primary study results using inverse-variance weighting (Borenstein et al., 2009). In inverse-variance weighting, the effect size estimates with the lowest variance (higher sample sizes) are given more weight because they are more precise (Viechtbauer, 2010). If the model was only taking into account the sampling variance then the weights are equal to \\(w_{ij} = 1/v_{ij}\\). In this case there are three sources of heterogeneity the sum of which the is the model implied variances of the estimates: \\(w_{ij} = 1/(\\hat{\\sigma}^2_{\\text{level3}}+\\hat{\\sigma}^2_{\\text{level2}}+v_{ij})\\). However, covariance between the effects needs to be taken into account, therefore the marginal variance-covariance matrix of the estimates.\nTo calculate the weights, let \\(\\mathbf{y}\\) be a the vector of observed effects (\\(\\hat{\\theta}_{ij}\\)) of length \\(n\\) (\\(\\mathbf{y} = \\hat{\\theta}_1, ....., \\hat{\\theta}_n\\)). The observations are organized as a series of independent groups, where the marginal variance-covariance matrix (\\(\\mathbf{M}\\)) of the estimates account for the variance structure of the data. Since the effect sizes from different studies are assumed to be independent, the matrix takes a block-diagonal form. Where each block corresponds to a single study, with the diagonal elements representing the total variance for each outcome, and the off-diagonal elements within each block representing the shared between-study variance. The blocks themselves are independent, reflecting the assumption that there is no covariance between outcomes from different studies.\n\\[\n\\mathbf{M} = \\begin{pmatrix}\n\\hat{\\sigma}^2_{\\text{level3}} + \\hat{\\sigma}^2_{\\text{level2}} + v{_{\\text{1}}} &\n\\hat{\\sigma}^2_{\\text{level3}} & 0 & 0 &... & 0   \\\\\n\\hat{\\sigma}^2_{\\text{level3}} &\n\\hat{\\sigma}^2_{\\text{level3}} + \\hat{\\sigma}^2_{\\text{level2}} + v{_{\\text{2}}} &   \n0 & 0 &...&0 \\\\\n\\vdots & \\vdots  & \\vdots & \\ddots  & \\vdots  & \\vdots \\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2_{\\text{level3}} + \\hat{\\sigma}^2_{\\text{level2}}\n+ v{_{\\text{n-1}}} &\n\\hat{\\sigma}^2_{\\text{level3}} \\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2_{\\text{level3}} &\n\\hat{\\sigma}^2_{\\text{level3}} + \\hat{\\sigma}^2_{\\text{level2}} + v{_{\\text{n}}}\n\\end{pmatrix}\n\\tag{3.7}\\]\nLet \\(\\mathbf{W} = \\mathbf{M^{-1}}\\) be the weight matrix, where, \\(w_{rc}\\) correspond to the \\(r\\)-th row and the \\(c\\)-th column of \\(\\mathbf{W}\\) and let \\(\\hat{\\theta_r}\\) denote the \\(r\\)-th estimate, with \\(r = 1, ...., k\\). Then the estimate of summary effect size \\(\\hat{\\mu}\\) for the random-effects model, without covariances, i.e., intercept-only model, is given by (Pustejovsky, 2020; Viechtbauer, 2020)\n\\[\n\\begin{aligned}\n&\\ \\hat{\\mu} = \\frac{ \\sum_{r= 1}^{k} (\\sum_{c=1}^{k} w_{rc}) \\hat{\\theta}_{r}}\n{\\sum_{r=1}^{k}\\sum_{c= 1}^{k} w_{rc}}\\\\\n&\\ \\text{with } \\\\\n&\\ \\overline{\\sigma}^2 = \\text{Var}(\\hat{\\mu}) = \\frac{1}{\\sum_{r=1}^{k}\\sum_{c= 1}^{k} w_{rc}} \\\\\n\\end{aligned}\n\\tag{3.8}\\]\nThis is equivalent to the generalized least squares estimate for the fixed effects (Viechtbauer, 2020); \\[\n\\mathbf{b}= (\\mathbf{X'WX})^{-1}\\mathbf{X'W}\\mathbf{y}\n\\tag{3.9}\\] Where \\(\\mathbf{X}\\) is the design matrix corresponding to the fixed effects, in the random-effects model case this is a single column of 1’s as there are no predictors, but in the mixed effects model, \\(\\mathbf{X}\\) has \\(b'+1\\) columns. In the mixed effects case the estimated parameters are \\(\\mu\\) and \\(\\beta_{b'}\\)’s (\\(\\mathbf{b}\\)). Following the recommendation of Assink & Wibbelink (2016), t-distribution was applied to assess the significance of individual regression coefficients in meta-analytic models, as well as to construct confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html#footnotes",
    "href": "chapters/methods.html#footnotes",
    "title": "3  Methods",
    "section": "",
    "text": "The idea to add the number of citations was added after the analysis was mostly completed. This suggestion was made during a discussion of the project after the preliminary results were presented to the methodology team at the CBS.↩︎\nLike \\(i\\), \\(l\\) refers to an effect size within the same study \\(j\\). \\(z\\) and \\(u\\) refer to effect sizes in different clusters, where \\(u \\neq j\\) effect sizes are independent.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/results.html",
    "href": "chapters/results.html",
    "title": "4  Results",
    "section": "",
    "text": "Descriptive Statistics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/results.html#descriptive-statistics",
    "href": "chapters/results.html#descriptive-statistics",
    "title": "4  Results",
    "section": "",
    "text": "A total of \\(n = 20\\) studies with \\(k = 86\\) effect sizes were included in this analysis, with each primary study reported between one and 27 results (\\(1 \\leq k_j \\leq 27\\)). The research area of these studies span 18 countries, Figure 4.1 (a) shows a map indicating the location of each effect size. These primary studies were grouped into three different SDG goals: SDG 2 (Zero Hunger), SDG 11 (Sustainable Cities), and SDG 15 (Life on Land).\n\n\n\n\n\n\n\n\n\n\n(a) Map of researched locations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Reported overall accuracy by study\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.1: Study location and range of reported overall accuracy, colour-coded by SDG goal. Individual outcomes shown as points and mean overall accuracy represented by triangles.\n\n\n\n\nFigure 4.1 (b) and Table 4.1 (bellow) show, the reported overall accuracies are not centered around 0.5. Therefore, a transformation is required. Figure 4.2 shows the distribution of observed overall accuracy as well as the logit and FT transformation values. FT visually performs better than the Logit transformation. However the Shapiro-Wilk Normality Test shows that the distribution of the FT transformed overall accuracy still departed significantly from normality (\\(W =\\) 0.93, p-value &lt; 0.01). Nevertheless, conducting a meta-analysis remains justified, as these statistical models are generally robust against violations of normality (McCulloch & Neuhaus, 2011).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.2: Distribution of the observed overall accuracy and transformed by logit and FT transformation.\n\n\n\n\nTable 4.1 summarises the overall accuracy (effect size of interest), study sample size and the collected study features, including the study features such as sample size, overall accuracy, types of machine learning models used and SDG goal targeted. For the meta-analysis the range of the sample size (259 - 75782016) and overall accuracy (0.6504 - 1) are of importance. Most studies used Neural Networks (48%), followed by Tree-Based Models (45%), and a small portion used other types of models (7%). Regarding SDGs, 44% of the studies aimed at SDG 11 (Sustainable Cities), 43% targeted SDG 15 (Life on Land), and 13% focused on SDG 2 (Zero Hunger).\n\n\n\n\n\nTable 4.1: Summary table\n\n\n\n\n\n\n\nFeature\nStatistic\n\n\n\n\nOverall Accuracy\n0.90 (0.65 - 1.00)\n\n\nStudy Features\n\n\nNumeric b\n\n\nSample Size\n6,401,352.08 (259.00 - 75,782,016.00)\n\n\nNumber of Citations\n14.84 (2.00 - 68.00)\n\n\nNumber of Classes\n3.71 (2.00 - 13.00)\n\n\nMajority-class Proportion\n0.72 (0.14 - 1.00)\n\n\nCategorical c\n\n\nPublication Year\n\n\n\n2018\n7 (8.1%)\n\n\n2019\n4 (4.7%)\n\n\n2020\n30 (35%)\n\n\n2021\n6 (7.0%)\n\n\n2022\n13 (15%)\n\n\n2023\n26 (30%)\n\n\nSDG Theme\n\n\n\nSDG11: Sustainable Cities\n38 (44%)\n\n\nSDG15: Life on Land\n37 (43%)\n\n\nSDG2: Zero Hunger\n11 (13%)\n\n\nClassification Type\n\n\n\nObject-level\n46 (53%)\n\n\nPixel-level\n36 (42%)\n\n\nUnclear\n4 (4.7%)\n\n\nModel Group\n\n\n\nNeural Networks\n41 (48%)\n\n\nOther\n6 (7.0%)\n\n\nTree-Based Models\n39 (45%)\n\n\nAncillary Data\n\n\n\nRemote Sensing Only\n71 (83%)\n\n\nAncillary Data Included\n15 (17%)\n\n\nIndices\n\n\n\nNot Used\n23 (27%)\n\n\nUsed\n63 (73%)\n\n\nRemote Sensing Type\n\n\n\nActive\n11 (13%)\n\n\nCombined\n7 (8.1%)\n\n\nNot Reported\n7 (8.1%)\n\n\nPassive\n61 (71%)\n\n\nDevice Group\n\n\n\nLandsat\n15 (17%)\n\n\nNot Reported\n7 (8.1%)\n\n\nOther\n44 (51%)\n\n\nSentinel\n20 (23%)\n\n\nNumber of Spectral Bands\n\n\n\nLow\n18 (21%)\n\n\nMid\n26 (30%)\n\n\nNot Reported\n42 (49%)\n\n\nSpatial Resolution\n\n\n\n&gt;1 metres\n7 (8.1%)\n\n\n10-30 metres\n39 (45%)\n\n\nNot Reported\n40 (47%)\n\n\nConfusion Matrix\n\n\n\nNot Reported\n23 (27%)\n\n\nReported\n63 (73%)\n\n\n\na Effect size of interest, b. Numeric: mean (min - max), c. Categorical variables: number of effect sizes (%)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: Categorical study features\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Numeric study features",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/results.html#meta-analysis",
    "href": "chapters/results.html#meta-analysis",
    "title": "4  Results",
    "section": "Meta-analysis",
    "text": "Meta-analysis\n\nThe forest plot below (Figure 4.5) compares the overall accuracy effect size across studies using both weighted and unweighted models, with error bars which correspond to the weighted model — at this scale there is no discernible difference between the error bars of the two models. Each study is given with the number of estimates per study \\(k_j\\), and study average effect size (\\(\\kappa_j\\)), with 95% confidence intervals (CI), both for the weighted and unweighted model. Of the 20 primary studies included, six reported only one effect. Based on the unweighted model, the average accuracy of machine learning methods applied to remote sensing data is 0.90 (95% CI[0.85; 0.94]). While the three-level meta-analytic model produced an average accuracy of 0.89 (95% CI[0.85; 0.93]). This implies, that on average, the machine learning methods correctly classify around 90% of the time when applied to remote sensing data.\n\n\n\n\n\n\n\n\nFigure 4.5: Forest plot for both the weighted and unweighted model. \\(k_j\\) is number of reported overall accuracy estimates per study, the corresponding average effect size(\\(\\kappa_j\\)) and confidence interval per study for both models is given on the right. The pooled summary effect size based on the three-level RE meta-analytic and unweighted model are given on the bottom.\n\n\n\n\n\nThe heterogeneity metrics Cochran’s Q indicate significant heterogeneity of the reported overall acccuracies. The percentage of the variance attribution is \\(I^2_{\\text{level3}}\\) = 63.62% which is the fraction of the variation that can be attributed to between-study, and \\(I^2_{\\text{level2}}\\) = 36.38% which is within-study heterogeneity, with negligible fixed effect variance (variance due to sampling error). The \\(I^2\\) value of 100% indicates that all the observed variability in effect sizes across studies is due to heterogeneity rather than sampling error, suggesting substantial differences between the studies and a high degree of variation in their results.\n\nModel Selection\nUsing the multi-model inference function, a total of 31,298 models were fitted. Figure 4.6, illustrates the predictor importance after evaluating all possible combinations of predictors to identify which combination provides the best fit and which predictors are most influential. Higher importance values indicate more consistent inclusion in high-weight models. The majority class proportion is the most important predictor, followed by the inclusion of ancillary data. Less influential predictors include used of indices, sample size, publication year, and the number of classes in the study. Meanwhile, factors such as classification type, SDG goal, machine learning group, spatial resolution, and citation count have minimal importance in the overall model performance (i.e., where not included in the models top performing models according to AIC).\n\n\n\n\n\n\n\n\n\nFigure 4.6: Model-averaged predictor importance plot with a reference line at 0.8 a commonly used as a threshold to indicate important predictors.\n\n\n\n\nTable 4.2 shows the results of the multi-model inference. The significant study features are the Majority-class Proportion and the inclusion of ancillary data. Interestingly, the use of ancillary data has a negative effect on overall accuracy.\n\n\n\nTable 4.2: Multimodel inference coefficients and feature importance. The estimated coefficients (b) and standard error (SE) are in FT transformed scale.\n\n\n\n\n\n\n\nFeature\nCategory\nImportance\nb\nSE\nz.value\np\n\n\n\n\nIntercept\n\n\n1.29\n7.85\n0.16\n0.869\n\n\nMajority-class Proportion\n\n1\n0.47\n0.08\n6.15\n&lt; .0001\n\n\nAncillary Data\n\n0.92\n\n\n\n\n\n\n\nRemote Sensing Only\n\n-0.12\n0.05\n2.33\n0.02\n\n\nIndices\n\n0.39\n\n\n\n\n\n\n\nUsed\n\n0.03\n0.04\n0.67\n0.5\n\n\nNumber of Spectral Bands\n\n0.38\n\n\n\n\n\n\n\nMid\n\n0.05\n0.06\n0.72\n0.471\n\n\n\nNot Reported\n\n0.02\n0.04\n0.55\n0.581\n\n\nConfusion Matrix\n\n0.16\n\n\n\n\n\n\n\nReported\n\n0.01\n0.02\n0.29\n0.776\n\n\nSample Size\n\n0.13\n0\n0\n0.1\n0.922\n\n\nNumber of Classes\n\n0.11\n0\n0\n0.19\n0.846\n\n\nPublication Year\n\n0.1\n0\n0\n0.06\n0.952\n\n\nRemote Sensing Type\n\n0.03\n\n\n\n\n\n\n\nCombined\n\n0.01\n0.03\n0.17\n0.869\n\n\n\nNot Reported\n\n0\n0.02\n0.04\n0.971\n\n\n\nPassive\n\n0\n0.02\n0.16\n0.87\n\n\nSpatial Resolution\n\n0.02\n\n\n\n\n\n\n\n10-30 metres\n\n0\n0.07\n0.01\n0.99\n\n\n\nNot Reported\n\n0\n0.07\n0\n0.996\n\n\nSDG Theme\n\n0.02\n\n\n\n\n\n\n\nSDG15: Life on Land\n\n0\n0.01\n0.1\n0.924\n\n\n\nSDG2: Zero Hunger\n\n0\n0.01\n0.08\n0.939\n\n\nClassification Type\n\n0.02\n\n\n\n\n\n\n\nPixel-level\n\n0\n0.01\n0.06\n0.955\n\n\n\nUnclear\n\n0\n0.01\n0.05\n0.958\n\n\nModel Group\n\n0.01\n\n\n\n\n\n\n\nOther\n\n0\n0.01\n0.04\n0.965\n\n\n\nTree-Based Models\n\n0\n0.01\n0.05\n0.961\n\n\nDevice Group\n\n0\n\n\n\n\n\n\n\nNot Reported\n\n0\n0.01\n0.05\n0.956\n\n\n\nOther\n\n0\n0\n0.05\n0.963\n\n\n\nSentinel\n\n0\n0.01\n0.05\n0.959\n\n\nNumber of Citations\n\n0\n0\n0\n0.01\n0.995\n\n\n\n\n\n\n\n\n\n\n\nMultimodel inference something about best 5 models and comparing AIC\n\n\n\n\nTable 4.3: Set of 5 best-ranked models and intercept only model ordered by AIC\\(_c\\)\n\n\n\n\n\n\n\nCandidate models\ndf\nAIC_c\nAkaike weights\n\n\n\n\nAncillary Data + Majority-class Proportion + Indices\n5\n-115.46\n0.39\n\n\nAncillary Data + Majority-class Proportion + Number of Spectral Bands\n6\n-114.42\n0.23\n\n\nAncillary Data + Majority-class Proportion\n4\n-114.13\n0.20\n\n\nAncillary Data + Confusion Matrix + Majority-class Proportion + Number of Spectral Bands\n7\n-113.08\n0.12\n\n\nAncillary Data + Majority-class Proportion + Number of Spectral Bands + Sample Size\n7\n-111.65\n0.06\n\n\nIntercept-Only\n2\n-41.93\n0.00\n\n\n\n\n\n\n\n\n\n\n\nTable 4.4 shows the estimated coefficients for the best fit model (lowest AIC), both in the FT transformed scale (b) and on the natural scale (b back-transformed). This shows that the proportion of majority class has the largest positive impact on the model’s outcome (b = 0.39, p &lt; .001), while the inclusion of ancillary data has a small negative effect (b = -0.11, p = 0.029) but a small but positive effect when back-transfored . The use of indices has a minimal and non-significant effect (b = 0.06, p = 0.131).\n\n\n\n\nTable 4.4: Results of the best fit model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nback-transfromed scale\n\n\n\nPredictor\nb\nSE\nt\np\nb_BT\nCI\n\n\n\n\nintercept\n0.99\n0.06\n17.22\n0.000\n0.70\n[0.58, 0.8]\n\n\nfraction_majority_class\n0.39\n0.08\n4.93\n0.000\n0.15\n[0.05, 0.27]\n\n\nancillaryAncillary Data Included\n-0.11\n0.05\n-2.22\n0.029\n0.01\n[0.04, 0]\n\n\nindicesUsed\n0.06\n0.04\n1.53\n0.131\n0.00\n[0, 0.02]\n\n\n\nNote: \n\n\n\n\n\n\n\n\n The estimated coefficients (b), standard errors (SE) on the FT transformed scale, t-statistics, and p-values. Additionally, the coefficients (b) and their confidence intervals (CI) are shown on the back-transformed scale.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.5: Results for heterogeneity and covariates tests for intercept only model, individual covariates as well as the best model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\n\n\n\nParamter\nIntercept Only\nMajority-class Proportion\nAncillary Data\nAncillary Data + Majority-class Proportion + Indices\n\n\n\n\nsig_lvl2\n0.01\n0.009\n0.01\n0.009\n\n\nsig_lvl3\n0.017\n0.007\n0.015\n0.005\n\n\nQE\n12161784\n11458055\n12035286\n11440331\n\n\ndf_Q\n85\n84\n84\n82\n\n\np_Q\n0\n0\n0\n0\n\n\nF\nNA\n27\n3\n13\n\n\ndf_F\nNA\n1\n1\n3\n\n\np_F\nNA\n0\n0.117\n0\n\n\nI2_lvl2\n36.38\n57.29\n40.47\n63.46\n\n\nI2_lvl3\n63.62\n42.71\n59.53\n36.54\n\n\nR2_lvl2\nNA\n7.8\n-1.4\n8.6\n\n\nR2_lvl3\nNA\n60.7\n14.7\n69.9\n\n\n\nNote: \n\n\n\n\n\n\n Test statistic, degrees of freedom and respective p values are provide. This table allows heterogeneity at level 2 and 3 can be compared between the incetept only model, Majority-class Proportion and Adncillary Data only models, as well as the combinded model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.5 shows the parameter estimates of the meta-analysis comparing the intercept only and three mixed effects models: (1) with the Majority-class Proportion as the only covariate, (2) use Ancillary Data only, and (3) the best fit model (from Table 4.4). Majority-class Proportion explains more of the between study heterogeneity, as shown by the difference in \\(\\sigma^2_{\\text{level2}}\\) between the intercept only and the Majority-class Proportion. The use of Ancillary Data explains relatively little between study heterogeneity and negligible within study heterogeneity. The combined model explains the most heterogeneity. This shift is also reflect in the \\(I^2\\). The total \\(I^2\\) consistently being 100% in both models indicates that almost none of the variation between effect sizes can be attributed to sampling error, this might suggest that the included studies are too different from each to compare (see discussion for apples and oranges problem). All models show significant heterogeneity (Cochran’s Q, p &lt; 0.001) results. The \\(R^2\\)values show that the covariates in the combined mixed effects model explain 69.9% of the variance at level 3 and 8.6% at level 2.\n\nFigure 4.7 illustrates the relationship between the proportion of the majority class and overall accuracy of the individual studies included in the meta-analysis. The plot is based on combined mixed effects model, with the solid black line representing the fitted regression line and the shaded area indicating the 95% confidence interval. Each point (bubble) represents a study, with its size proportional to the weight it received in the analysis (larger points indicate studies with more influence). The plot shows that as the proportion of the majority class increases, overall accuracy tends to improve.\n\n\n\n\n\n\n\nFigure 4.7: Bubble plot showing the observed effect size, overall accuracy of the individual studies plotted against a the proportion of the majority class. Based on the mixed-effects meta-regression model, the overall accuracy as a function of proportion of the majority with corresponding 95% confidence interval bounds. The size of the points are proportional to the weight that the observation received in the analysis, while the color of the points is unique to each study, with the lowest overall actuary from each study labeled with the first author and publication year.\n\n\n\n\nFigure 4.8 shows the observed overall accuracy against the predicted overall accuracy’s made by combined mixed effects model. The points are coloured by the addition of ancillary information in the primary study. It appears that the addition of ancillary information leads to a lower overall accuracy, however, this could be due to a number of unmeasured factors, such as study’s with more complicated classifications (more similar classes) adding accuracy data. As Figure 4.8 shows Model 2 over estimates the overall accuracy — the fit regression line (in grey) is above the line of perfect agreement (\\(y = x\\), in black).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.8: Observed and predicted overall accuracy. The colour indicates whether the addition of ancillary data in the primary study’s model. The line of perfect agreement \\(y = x\\) is in black and fit regression of the points in grey.\n\n\n\n\n\n\n\nMcCulloch, C. E., & Neuhaus, J. M. (2011). Misspecifying the shape of a random effects distribution: Why getting it wrong may not matter. Statistical Science, 26(3), 388–402. https://doi.org/10.1214/11-STS361",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/discussion.html",
    "href": "chapters/discussion.html",
    "title": "5  Discussion",
    "section": "",
    "text": "Central Findings\nThe results of this meta-analysis demonstrate the considerable variability in the predictive performance of machine learning models applied to remote sensing data for SDGs. Some of this variability could be attributed to the proportion of the majority class as well as the inclusion of ancillary data. The type of model, whether neural networks and tree-based models or the SDG studied, showed no differences in overall accuracy. Unsurprisingly, the proportion of the majority class significantly affected the overall accuracy of machine learning models. While the use of ancillary data in primary studies has a small but significant negative effect on overall accuracy performance, which is counterintuitive. Perhaps this is explained by other variables not captured in this study, for example researchers addressing more complex classification problems use models with ancillary data. No other significant effects were found in the study features examined in this study.\nTo the best of available knowledge, this is the only meta-analysis of remote sensing methods that utilized weighted estimates. The use of a three-level random effects model enabled the decomposition of variance into within-study and between-study components, offering insights into the observed heterogeneity. No meaningful difference was found between the weighted and unweighted approaches.\n\n\nLimitations\n\nNumber of reviewers: From the 200 studies randomly sampled, three reviewers assessed whether full-text screening should be conducted. Only 57 papers were agreed upon by all three reviewers, while each reviewer thought between 77 and 81 studies could have been included. This highlights the subjectivity of the selection process and the importance of having multiple reviewers. The full-text screening was only conducted by one person which means that this subjectively or potential mistakes were missed in the final dataset. This issue is exasperated by the inconsistent reporting on methods in this field. For example, one feature that could not be included in the analysis was whether the results reported were derived from the training or test set because it was very unclear in some of the selected studies.\nSample size: This study included a total of 20 studies. While several simulations studies suggest that a three-level meta-analysis can yield accurate results with as few as 20 to 40 studies (Hedges et al., 2010), this analysis is at the lower bound, and the included studies exhibit considerable variability, making the statistical power a concern. Polanin (2014) suggests a minimum of 40 studies is generally recommended to ensure robust results. Furthermore, a relatively high proportion of the studies (6 out of 20) reported only one result (\\(k_j = 1\\)), limiting the ability to assess within-study variability. The small sample size inherently increases the potential for bias and may affect the reliability of the findings (Polanin, 2014).\nChoice of effect size: While overall accuracy is widely used, it does not capture the complexity of model performance, especially in studies with imbalanced classes. To illustrate the problem, if 99% of the data belongs to class A, a model that always predicts class A—without any regard to the predictors—will achieve an overall accuracy of 99%, despite essentially doing nothing and failing to capture meaningful patterns. For more specific details on the issues related to the use of overall accuracy, see Foody (2020) and Stehman & Foody (2019). Alternative metrics include Matthews’ correlation coefficient, F1 score, Somers’ D, and average precision. Unfortunately, these metrics are rarely reported in the studies analyzed here. Moreover, some of these alternatives are also sensitive to class imbalance and must be corrected to ensure comparability across studies (Burger & Meertens, 2020).\nPublication bias: This study only examined published results, which introduces publication bias—a well-documented effect where studies with positive results are more likely to be published, while negative or neutral findings remain unpublished (Borenstein et al., 2009; Bozada et al., 2021; Hansen et al., 2022; Harrer et al., 2022). This bias can lead to an overestimation of effects, as demonstrated in this study, where the average overall accuracy around 90%. Accuracy, is easy to understand and compute, as addressed it does not take into account class imbalance. When models are developed and tuned to maximize accuracy on training data, they often perform poorly on unseen data, inflating the performance metrics, but the reporting of training and test results was inconsistent.\nStudy features included: The analysis would have benefited from the inclusion of more study features. For example, to better understand the effect of ancillary data, a feature representing the complexity of the problem addressed by the primary study could explain the negative effect of additional information on a prediction model. It is also important to note that most of the study features included in this research were between-study covariates and did not differ within studies, which explains why only the between-study heterogeneity was reduced. Furthermore, due to the small sample size, it was necessary to aggregate the study features into broad categories, which limited the granularity of the analysis.\nApples and oranges problem: The \\(I^2\\) result of effectively 100% may indicate that the included studies are too different to statistically compare. This is often referred to as the “apples and oranges problem” (Harrer et al., 2022, Chapter 1). The extent to which primary studies can differ while still being meaningfully combined in a meta-analysis is debated. However, when Robert Rosenthal, a pioneer in meta-analysis, was asked whether combining studies with significant differences is valid his response was “combining apples and oranges makes sense if your goal is to produce a fruit salad” (Borenstein et al., 2009, Chapter 40, pp. 357). In this case, despite the diverse research aims of the included studies, the objective is to draw general conclusions about machine learning applications in remote sensing for SDG monitoring. This approach can be viewed as a “fruit salad” with potential for broad applicability across different SDG contexts. However, this again raises the issue of sample size, as a large sample is required to ensure sufficient statistical power to draw confident conclusions.\nCochran’s Q and large sample sizes: Another limitation is the reliance on Cochran’s Q for testing heterogeneity. While widely used, the power of the Q-statistic is dependent on the number of included effect sizes (\\(k\\)) and the precision of the studies i.e., the sample size with in that study (\\(m_{ij}\\)). In cases with large sample-sizes, the Q statistic becomes highly sensitive to even minor differences between studies. The Q-statistic is “overpowered”, which result in the detection of statistically significant heterogeneity even when the actual differences between studies are small. This sensitivity may exaggerate the extent of heterogeneity, potentially lead to misleading conclusions about the variability among the included studies. Little research has been done on the effect of very large primary-sample-sizes since meta-analysis typically compile studies who’s unit of analysis in on a patient level. Primary-sample-sizes in the millions is not a common issue.\nTransformation of the effect size: In general model selection at the transformed level presents limitations, as the relevance of features is assessed on the transformed scale, which may not directly translate to the original effect size after back-transformation. This complicates the interpretation of results, since conclusions drawn on the transformed scale may not have the same meaning when applied to the original data. Additionally, the use of FT transformation is contested in the literature because of several important limitations (Doi & Xu, 2021; Lin & Xu, 2020; Röver & Friede, 2022; Schwarzer et al., 2019). First, the FT is notably unintuitive, specifically the calculation of variance relies on the structure of an arcsine function’s derivative. Second, back-transforming the pooled effect size using certain methods—such as the harmonic mean of primary sample sizes—can lead to misleading results (Doi & Xu, 2021; Lin & Xu, 2020; Röver & Friede, 2022; see Schwarzer et al., 2019; Wang, 2023). However, in this analysis, the pooled variance, rather than the harmonic mean, was used for back-transformation, addressing the main issue debated in the literature. Nevertheless, the choice of back-transformation method significantly influences the outcome, and justifying a specific method is especially challenging in a multilevel data structure. Lastly, in a random-effects model the true (transformed) proportion is assumed to follow a normal distribution between studies, the FT transformation potentially violates this assumption as the arcsine function has a bounded domain (Röver & Friede, 2022).\n\n\n\nImplications for Future Research\nThe limitations identified in this meta-analysis suggest several directions for future research that can enhance the robustness and generalisability of findings related to machine learning applications in remote sensing for SDG monitoring.\n\nSample size and model complexity: One of the primary limitations of this meta-analysis was the small sample size, with \\(n = 20\\) studies included. Future research should aim to expand the pool of included studies. This would mean that interaction effects between the collected study features could also be included in the analysis. The structure of the random effects can also be explored with the application of more sophisticated variance-covariance structures for random effects. This approach, sometimes referred to as dose-response meta-analysis (Viechtbauer, 2024, p. 269), would provide insights into how specific study characteristics influence effect sizes over time or across varying conditions.\nBroader inclusion of performance metrics: This meta-analysis primarily focused on overall accuracy, a commonly used but potentially misleading performance metric, particularly in imbalanced datasets. Future studies should expand the range of performance metrics, incorporating class-specific precision, recall, F1-score, average precision, and AUC to provide a more comprehensive evaluation of model performance. More than one effect size can be modeled using network meta- analysis models (Harrer et al., 2022, Chapter 12). The inclusion of more performance metrics would offer a more nuanced understanding of how models perform under different conditions.\nExploring additional study features and moderators: The present study focused on a limited set of study features, including the proportion of the majority class and the inclusion of ancillary data. Future research should investigate a broader range of potential moderators, such as model complexity, data preprocessing techniques, and environmental or socio-economic factors specific to SDG challenges. By including a more extensive set of features, researchers can better understand the drivers of performance variability and refine model selection for specific applications.\nEffect of large sample size in primary studies: Simulation studies could provide insights into the sensitivity of Cochran’s Q in the context of large sample sizes. Developing less sensitive methods for assessing heterogeneity would improve the reliability of meta-analytic findings, especially when studies involve substantial sample sizes, which can exaggerate minor differences between studies.\nData extraction: In the time frame of this research, the ChatGPT virtual assistant showed significant improvements in data extraction capabilities. Initially, in January 2024, ChatGPT struggled to extract meaningful features. By May 2024, it was capable of accurately filling in all study features directly from the provided papers (in PDF format). Although the improvement was not formally assessed in this study, the difference was striking. Some research has already examined the potential accuracy of large language models (LLMs) in data extraction for meta-analyses, with promising results (Mahuli et al., 2023). However, for this thesis, ChatGPT was not used for formal data extraction. Instead, traditional manual extraction methods were employed to ensure accuracy. Further investigation into the accuracy of LLMs for meta-analysis is required. LLMs can expedite the data extraction process, potentially addressing challenges related to the limited number of included studies. Another unrelated recommendation to improve data extraction would be for journals to require results and specific features to be submitted separately in addition to the manuscript so that the journals themselves can report trends in outcomes.\n\n\n\n6 Conclusion\nThis meta-analysis provides insights into the variability of machine learning models used for remote sensing in SDG monitoring. First, (Research Question 1) the average performance of machine learning models was found to be high, but strongly influenced by class imbalance. This finding reinforces the limitations of overall accuracy as a metric for assessing model performance. It highlights the need for a shift towards more balanced and nuanced performance metrics, such as Matthews’ correlation coefficient and F1 score, in future SDG monitoring studies. Second, (Research Question 2) the three-level random-effects model showed a substantial degree of heterogeneity across outcomes. Third (Research Question 3), the role of specific study features was notable: although no significant differences were observed between model types (e.g., neural networks or tree-based models), the proportion of the majority class and the inclusion of ancillary data were important factors. However, the negative impact of ancillary data on model performance requires further investigation, as this counterintuitive finding suggests a need for additional research. Finally, the comparison of sample-weighted and unweighted models (Research Question 4) revealed no substantial difference in average effect size, though the weighted model uncovered significant heterogeneity. Furthermore, more research is needed to improve the robustness and applicability of meta-analyses methods to this field. In particular, the use of Cochran’s Q-statistic is questionable in the context of this analysis, as the very large primary study sample sizes make Q overly sensitive. This can result in the detection of statistically significant heterogeneity, even when the heterogeneity may not be practically meaningful.\nOverall, this study provides a foundation for improving the use of machine learning models in remote sensing for SDG monitoring, yet also highlights the need for more robust and varied methodologies, larger datasets, and a move away from overly simplistic metrics like overall accuracy.\n\n\n\n\nBorenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R. (2009). Introduction to meta-analysis. Wiley.\n\n\nBozada, T., Borden, J., Workman, J., Del Cid, M., Malinowski, J., & Luechtefeld, T. (2021). Sysrev: A FAIR platform for data curation and systematic evidence review. Frontiers in Artificial Intelligence, 4, 685298. https://doi.org/10.3389/frai.2021.685298\n\n\nBurger, J., & Meertens, Q. (2020). The algorithm versus the chimps:on the minima of classifier performance metrics. In L. Cao, W. Kosters, & J. Lijffijt (Eds.), BNAIC/BeneLearn 2020 proceedings (pp. 38–55). BNAIC/BeneLearn. https://bnaic.liacs.leidenuniv.nl/bnaic2020proceedings.pdf\n\n\nDoi, S. A., & Xu, C. (2021). The FreemanTukey double arcsine transformation for the meta-analysis of proportions: Recent criticisms were seriously misleading. Journal of Evidence-Based Medicine, 14(4), 259–261. https://doi.org/10.1111/jebm.12445\n\n\nFoody, G. M. (2020). Explaining the unsuitability of the kappa coefficient in the assessment and comparison of the accuracy of thematic maps obtained by image classification. Remote Sensing of Environment, 239, 111630. https://doi.org/10.1016/j.rse.2019.111630\n\n\nHansen, C., Steinmetz, H., & Block, J. (2022). How to conduct a meta-analysis in eight steps: a practical guide. Management Review Quarterly, 72(1), 1–19. https://doi.org/10.1007/s11301-021-00247-4\n\n\nHarrer, M., Cuijpers, P., Furukawa, T. A., & Ebert, D. D. (2022). Doing meta-analysis with r: A hands-on guide. CRC Press/Taylor & Francis Group. https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\n\nHedges, L. V., Tipton, E., & Johnson, M. C. (2010). Robust variance estimation in meta-regression with dependent effect size estimates. Research Synthesis Methods, 1(1), 39–65. https://doi.org/10.1002/jrsm.5\n\n\nLin, L., & Xu, C. (2020). Arcsine-based transformations for meta-analysis of proportions: Pros, cons, and alternatives. Health Science Reports, 3(3), e178. https://doi.org/10.1002/hsr2.178\n\n\nMahuli, S. A., Rai, A., Mahuli, A. V., & Kumar, A. (2023). Application ChatGPT in conducting systematic reviews and meta-analyses. British Dental Journal, 235(2), 90–92. https://doi.org/10.1038/s41415-023-6132-y\n\n\nPolanin, J. R. (2014). An introduction to multilevel meta-analysis,. https://www.youtube.com/watch?v=rJjeRRf23L8&t=1358s; Campbell Colloquium.\n\n\nRöver, C., & Friede, T. (2022). Double arcsine transform not appropriate for meta-analysis. Research Synthesis Methods, 13(5), 645–648. https://doi.org/10.1002/jrsm.1591\n\n\nSchwarzer, G., Chemaitelly, H., Abu-Raddad, L. J., & Rücker, G. (2019). Seriously misleading results using inverse of freeman-tukey double arcsine transformation in meta-analysis of single proportions. Research Synthesis Methods, 10, 476–483. https://doi.org/10.1002/jrsm.1348\n\n\nStehman, S. V., & Foody, G. M. (2019). Key issues in rigorous accuracy assessment of land cover products. Remote Sensing of Environment, 231, 111199. https://doi.org/10.1016/j.rse.2019.05.018\n\n\nViechtbauer, W. (2024). metafor: Meta-Analysis Package for R. https://doi.org/10.32614/CRAN.package.metafor\n\n\nWang, N. (2023). Conducting Meta-analyses of Proportions in R. Journal of Behavioral Data Science, 3(2), 64–126. https://doi.org/10.35566/jbds/v3n2/wang",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anshuka, A., Ogtrop, F. F. van, & Willem Vervoort, R. (2019).\nDrought forecasting through statistical models using standardised\nprecipitation index: A systematic review and meta-regression analysis.\nNatural Hazards, 97(2), 955–977. https://doi.org/10.1007/s11069-019-03665-6\n\n\nAssink, M., & Wibbelink, C. J. M. (2016). Fitting three-level\nmeta-analytic models in R: A step-by-step tutorial. The Quantitative\nMethods for Psychology, 12(3), 154–174. https://doi.org/10.20982/tqmp.12.3.p154\n\n\nBarendregt, J. J., Doi, S. A., Lee, Y. Y., Norman, R. E., & Vos, T.\n(2013). Meta-analysis of prevalence. Journal of Epidemiology and\nCommunity Health (1979-), 67(11), 974–978. https://doi.org/10.1136/jech-2013-203104\n\n\nBorenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R.\n(2009). Introduction to meta-analysis. Wiley.\n\n\nBorges Migliavaca, C., Stein, C., Colpani, V., Barker, T. H., Munn, Z.,\nFalavigna, M., & on behalf of the Prevalence Estimates Reviews\nSystematic Review Methodology Group (PERSyst). (2020). How are\nsystematic reviews of prevalence conducted? A methodological study.\nBMC Medical Research Methodology, 20(1), 96. https://doi.org/10.1186/s12874-020-00975-3\n\n\nBozada, T., Borden, J., Workman, J., Del Cid, M., Malinowski, J., &\nLuechtefeld, T. (2021). Sysrev: A FAIR platform for data curation and\nsystematic evidence review. Frontiers in Artificial\nIntelligence, 4, 685298. https://doi.org/10.3389/frai.2021.685298\n\n\nBurger, J., & Meertens, Q. (2020). The algorithm versus the\nchimps:on the minima of classifier performance metrics. In L. Cao, W.\nKosters, & J. Lijffijt (Eds.), BNAIC/BeneLearn 2020\nproceedings (pp. 38–55). BNAIC/BeneLearn. https://bnaic.liacs.leidenuniv.nl/bnaic2020proceedings.pdf\n\n\nBurke, M., Driscoll, A., Lobell, D. B., & Ermon, S. (2021). Using\nsatellite imagery to understand and promote sustainable development.\nScience, 371(6535), eabe8628. https://doi.org/10.1126/science.abe8628\n\n\nCampbell, McKenzie, J. E., Sowden, A., Katikireddi, S. V., Brennan, S.\nE., Ellis, S., Hartmann-Boyce, J., Ryan, R., Shepperd, S., Thomas, J.,\nWelch, V., & Thomson, H. (2020). Synthesis without meta-analysis\n(SWiM) in systematic reviews: reporting guideline. BMJ,\n368, l6890. https://doi.org/10.1136/bmj.l6890\n\n\nCampbell, & Wynne, R. H. (2011). Introduction to remote\nsensing (5th ed). Guilford Press.\n\n\nCheung, M. W. L. (2014). Modeling dependent effect sizes with\nthree-level meta-analyses: A structural equation modeling approach.\nPsychological Methods, 19(2), 211–229. https://doi.org/10.1037/a0032968\n\n\nDebray, T. P. A., Damen, J. A. A. G., Snell, K. I. E., Ensor, J., Hooft,\nL., Reitsma, J. B., Riley, R. D., & Moons, K. G. M. (2017). A guide\nto systematic review and meta-analysis of prediction model performance.\nBMJ, i6460. https://doi.org/10.1136/bmj.i6460\n\n\nDoi, S. A., & Xu, C. (2021). The FreemanTukey double\narcsine transformation for the meta-analysis of proportions: Recent\ncriticisms were seriously misleading. Journal of Evidence-Based\nMedicine, 14(4), 259–261. https://doi.org/10.1111/jebm.12445\n\n\nEkmen, O., & Kocaman, S. (2024). Remote sensing for UN SDGs: A\nglobal analysis of research and collaborations. The Egyptian Journal\nof Remote Sensing and Space Sciences, 27(2), 329–341. https://doi.org/10.1016/j.ejrs.2024.04.002\n\n\nFAO, F. and A. O. (2016). Map accuracy assessment and area\nestimation practical guide.,\nhttp://www.fao.org/3/a-i5601e.pdf\n\n\nFoody, G. M. (2020). Explaining the unsuitability of the kappa\ncoefficient in the assessment and comparison of the accuracy of thematic\nmaps obtained by image classification. Remote Sensing of\nEnvironment, 239, 111630. https://doi.org/10.1016/j.rse.2019.111630\n\n\nFreeman, M. F., & Tukey, J. W. (1950). Transformations Related to\nthe Angular and the Square Root. The Annals of Mathematical\nStatistics, 21(4), 607–611. https://doi.org/10.1214/aoms/1177729756\n\n\nGusenbauer, M., & Haddaway, N. R. (2020). Which academic search\nsystems are suitable for systematic reviews or meta‐analyses?\nEvaluating retrieval qualities of Google\nScholar, PubMed, and 26 other resources.\nResearch Synthesis Methods, 11(2), 181–217. https://doi.org/10.1002/jrsm.1378\n\n\nHaddaway, N. R., Bannach-Brown, A., Grainger, M. J., Hamilton, W. K.,\nHennessy, E. A., Keenan, C., Pritchard, C. C., & Stojanova, J.\n(2022). The evidence synthesis and meta-analysis in R\nconference (ESMARConf): Levelling the playing field of\nconference accessibility and equitability. Systematic Reviews,\n11(1), 113. https://doi.org/10.1186/s13643-022-01985-6\n\n\nHall, J. A., & Rosenthal, R. (2018). Choosing between random effects\nmodels in meta-analysis: Units of analysis and the generalizability of\nobtained results. Social and Personality Psychology Compass,\n12(10), e12414. https://doi.org/10.1111/spc3.12414\n\n\nHall, O., Dompae, F., Wahab, I., & Dzanku, F. M. (2023). A review of\nmachine learning and satellite imagery for poverty prediction:\nImplications for development research and applications. Journal of\nInternational Development, 35(7), 1753–1768. https://doi.org/10.1002/jid.3751\n\n\nHansen, C., Steinmetz, H., & Block, J. (2022b). How to conduct a\nmeta-analysis in eight steps: A practical guide. Management Review\nQuarterly, 72(1), 1–19. https://doi.org/10.1007/s11301-021-00247-4\n\n\nHansen, C., Steinmetz, H., & Block, J. (2022a). How to conduct a\nmeta-analysis in eight steps: a practical guide. Management Review\nQuarterly, 72(1), 1–19. https://doi.org/10.1007/s11301-021-00247-4\n\n\nHarrer, M., Cuijpers, P., Furukawa, T. A., & Ebert, D. D. (2022).\nDoing meta-analysis with r: A hands-on guide. CRC Press/Taylor\n& Francis Group. https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\n\nHarrer, M., Cuijpers, P., Furukawa, T., & Ebert, D. D. (2019).\nDmetar: Companion r package for the guide ’doing meta-analysis in\nr’. http://dmetar.protectlab.org/\n\n\nHedges, L. V., Tipton, E., & Johnson, M. C. (2010). Robust variance\nestimation in meta-regression with dependent effect size estimates.\nResearch Synthesis Methods, 1(1), 39–65. https://doi.org/10.1002/jrsm.5\n\n\nHeydari, S. S., & Mountrakis, G. (2018). Effect of classifier\nselection, reference sample size, reference class distribution and scene\nheterogeneity in per-pixel classification accuracy using 26 Landsat\nsites. Remote Sensing of Environment, 204, 648–658. https://doi.org/10.1016/j.rse.2017.09.035\n\n\nHiggins, J. P. T., & Thompson, S. G. (2002). Quantifying\nheterogeneity in a meta-analysis. Statistics in Medicine,\n10, 1539–1558. https://doi:10.1002/sim.1186\n\n\nHolloway, J., & Mengersen, K. (2018). Statistical\nMachine Learning Methods and\nRemote Sensing for Sustainable\nDevelopment Goals: A\nReview. Remote Sensing, 10(9), 1365. https://doi.org/10.3390/rs10091365\n\n\nIliescu, D., Rusu, A., Greiff, S., Fokkema, M., & Scherer, R.\n(2022). Why We Need Systematic Reviews and Meta-Analyses in the Testing\nand Assessment Literature. European Journal of Psychological\nAssessment, 38(2), 73–77. https://doi.org/10.1027/1015-5759/a000705\n\n\nKhatami, R., Mountrakis, G., & Stehman, S. V. (2016b). A\nmeta-analysis of remote sensing research on supervised pixel-based\nland-cover image classification processes: General\nguidelines for practitioners and future research. Remote Sensing of\nEnvironment, 177, 89–100. https://doi.org/10.1016/j.rse.2016.02.028\n\n\nKhatami, R., Mountrakis, G., & Stehman, S. V. (2016a). A\nmeta-analysis of remote sensing research on supervised pixel-based\nland-cover image classification processes: General guidelines for\npractitioners and future research. Remote Sensing of\nEnvironment, 177, 89–100. https://doi.org/10.1016/j.rse.2016.02.028\n\n\nLaird, N. M., & Mosteller, F. (1990). Some Statistical Methods for\nCombining Experimental Results. International Journal of Technology\nAssessment in Health Care, 6(1), 5–30. https://doi.org/10.1017/s0266462300008916\n\n\nLajeunesse, M. J. (2016). Facilitating systematic reviews, data\nextraction, and meta-analysis with the metagear package for r.\n7, 323–330.\n\n\nLavallin, A., & Downs, J. A. (2021). Machine learning in\ngeography–Past, present, and future. Geography\nCompass, 15(5), e12563. https://doi.org/10.1111/gec3.12563\n\n\nLin, L., & Xu, C. (2020). Arcsine-based transformations for\nmeta-analysis of proportions: Pros, cons, and alternatives. Health\nScience Reports, 3(3), e178. https://doi.org/10.1002/hsr2.178\n\n\nLu, D., & Weng, Q. (2007). A survey of image classification methods\nand techniques for improving classification performance.\nInternational Journal of Remote Sensing, 28(5),\n823–870. https://doi.org/10.1080/01431160600746456\n\n\nMahuli, S. A., Rai, A., Mahuli, A. V., & Kumar, A. (2023).\nApplication ChatGPT in conducting systematic reviews and meta-analyses.\nBritish Dental Journal, 235(2), 90–92. https://doi.org/10.1038/s41415-023-6132-y\n\n\nMaso, J., Zabala, A., & Serral, I. (2023). Earth Observations for\nSustainable Development Goals. Remote Sensing, 15(10),\n2570. https://doi.org/10.3390/rs15102570\n\n\nMcCulloch, C. E., & Neuhaus, J. M. (2011). Misspecifying the shape\nof a random effects distribution: Why getting it wrong may not matter.\nStatistical Science, 26(3), 388–402. https://doi.org/10.1214/11-STS361\n\n\nNASA. (2019). What is Remote Sensing? https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing\n\n\nOwers, C. J., Lucas, R. M., Clewley, D., Tissott, B., Chua, S. M. T.,\nHunt, G., Mueller, N., Planque, C., Punalekar, S. M., Bunting, P., Tan,\nP., & Metternicht, G. (2022). Operational continental-scale land\ncover mapping of Australia using the Open Data Cube. International\nJournal of Digital Earth, 15(1), 1715–1737. https://doi.org/10.1080/17538947.2022.2130461\n\n\nPage, M. J., McKenzie, J. E., Bossuyt, P. M., Boutron, I., Hoffmann, T.\nC., Mulrow, C. D., Shamseer, L., Tetzlaff, J. M., Akl, E. A., Brennan,\nS. E., Chou, R., Glanville, J., Grimshaw, J. M., Hróbjartsson, A., Lalu,\nM. M., Li, T., Loder, E. W., Mayo-Wilson, E., McDonald, S., … Moher, D.\n(2021). The PRISMA 2020 statement: an updated guideline for reporting\nsystematic reviews. BMJ, n71. https://doi.org/10.1136/bmj.n71\n\n\nPolanin, J. R. (2014). An introduction to multilevel\nmeta-analysis,. https://www.youtube.com/watch?v=rJjeRRf23L8&t=1358s;\nCampbell Colloquium.\n\n\nPriem, J., Piwowar, H., & Orr, R. (2022). OpenAlex: A fully-open\nindex of scholarly works, authors, venues, institutions, and\nconcepts. https://arxiv.org/abs/2205.01833\n\n\nPustejovsky, J. E. (2020). Weighting in multivariate\nmeta-analysis. https://jepusto.com/posts/weighting-in-multivariate-meta-analysis/.\n\n\nRöver, C., & Friede, T. (2022). Double arcsine transform not\nappropriate for meta-analysis. Research Synthesis Methods,\n13(5), 645–648. https://doi.org/10.1002/jrsm.1591\n\n\nSchwarzer, G., Carpenter, J. R., & Rücker, G. (2015).\nMeta-Analysis with R. Springer\nInternational Publishing. https://doi.org/10.1007/978-3-319-21416-0\n\n\nSchwarzer, G., Chemaitelly, H., Abu-Raddad, L. J., & Rücker, G.\n(2019). Seriously misleading results using inverse of freeman-tukey\ndouble arcsine transformation in meta-analysis of single proportions.\nResearch Synthesis Methods, 10, 476–483. https://doi.org/10.1002/jrsm.1348\n\n\nSEOS. (2014). Introduction to remote sensing. https://seos-project.eu/remotesensing/remotesensing-c01-p06.html\n\n\nStehman, S. V., & Foody, G. M. (2019). Key issues in rigorous\naccuracy assessment of land cover products. Remote Sensing of\nEnvironment, 231, 111199. https://doi.org/10.1016/j.rse.2019.05.018\n\n\nTawfik, G. M., Dila, K. A. S., Mohamed, M. Y. F., Tam, D. N. H., Kien,\nN. D., Ahmed, A. M., & Huy, N. T. (2019). A step by step guide for\nconducting a systematic review and meta-analysis with simulation data.\nTropical Medicine and Health, 47(1), 46. https://doi.org/10.1186/s41182-019-0165-6\n\n\nThapa, A., Horanont, T., Neupane, B., & Aryal, J. (2023). Deep\nLearning for Remote Sensing\nImage Scene Classification:\nA Review and\nMeta-Analysis. Remote Sensing,\n15(19), 4804. https://doi.org/10.3390/rs15194804\n\n\nUCS. (2021). Union of Concerned Scientists (UCS) Satellite\nDatabase. https://www.ucsusa.org/resources/satellite-database\n\n\nUN DESA. (2023). The Sustainable\nDevelopment Goals Report 2023:\nSpecial Edition. United Nations. https://doi.org/10.18356/9789210024914\n\n\nUN-GGIM:Europe. (2019). The territorial dimension in SDG indicators:\nGeospatial data analysis and its integration with statistical data.\nInstituto Nacional de Estatística. https://un-ggim-europe.org/wp-content/uploads/2019/05/UN_GGIM_08_05_2019-The-territorial-dimension-in-SDG-indicators-Final.pdf\n\n\nUnited Nations. (2017). Earth observations for official statistics:\nSatellite imagery and geospatial data task team report. https://unstats.un.org/bigdata/task-teams/earth-observation/UNGWG_Satellite_Task_Team_Report_WhiteCover.pdf\n\n\nUnited Nations. (2024). The sustainable development goals report\n2024. https://unstats.un.org/sdgs/report/2024/The-Sustainable-Development-Goals-Report-2024.pdf\n\n\nVeroniki, A. A., Jackson, D., Viechtbauer, W., Bender, R., Bowden, J.,\nKnapp, G., Kuss, O., Higgins, J. P., Langan, D., & Salanti, G.\n(2015). Methods to estimate the between-study variance and its\nuncertainty in meta-analysis. Research Synthesis Methods,\n7(1), 55–79. https://doi.org/10.1002/jrsm.1164\n\n\nViechtbauer, W. (2010). Conducting Meta-Analyses in R with the metafor\nPackage. Journal of Statistical Software, 36, 1–48. https://doi.org/10.18637/jss.v036.i03\n\n\nViechtbauer, W. (2020). Weights in models fitted with the rma.mv()\nfunction. https://www.metafor-project.org/doku.php/tips:weights_in_rma.mv_models.\n\n\nViechtbauer, W. (2022). Metafor: Model selection using the glmulti\nand MuMIn packages. https://www.metafor-project.org/doku.php/tips:model_selection_with_glmulti_and_mumin#variable_importance.\n\n\nViechtbauer, W. (2024a). Frequently asked questions [the metafor\npackage]: Freeman-tukey transformation of proportions. https://www.metafor-project.org/doku.php/faq#how_is_the_freeman-tukey_trans.\n\n\nViechtbauer, W. (2024b). metafor: Meta-Analysis Package for R.\nhttps://doi.org/10.32614/CRAN.package.metafor\n\n\nWang, N. (2023). Conducting Meta-analyses of Proportions in R.\nJournal of Behavioral Data Science, 3(2), 64–126. https://doi.org/10.35566/jbds/v3n2/wang\n\n\nYin, C., Peng, N., Li, Y., Shi, Y., Yang, S., & Jia, P. (2023). A\nreview on street view observations in support of the sustainable\ndevelopment goals. International Journal of Applied Earth\nObservation and Geoinformation, 117, 103205. https://doi.org/10.1016/j.jag.2023.103205\n\n\nZhang, C., & Li, X. (2022). Land Use and Land Cover Mapping in the\nEra of Big Data. Land, 11(10), 1692. https://doi.org/10.3390/land11101692\n\n\nZhang, Y., Liu, J., & Shen, W. (2022). A Review of\nEnsemble Learning Algorithms\nUsed in Remote Sensing\nApplications. Applied Sciences, 12(17),\n8654. https://doi.org/10.3390/app12178654\n\n\nZhao, Q., Yu, L., Du, Z., Peng, D., Hao, P., Zhang, Y., & Gong, P.\n(2022). An Overview of the Applications of\nEarth Observation Satellite\nData: Impacts and Future\nTrends. Remote Sensing, 14(8), 1863. https://doi.org/10.3390/rs14081863",
    "crumbs": [
      "References"
    ]
  }
]