---
execute:
  echo: false
  message: false
  warning: false
  enabled: true
---

# Results

## Descriptive Statistics

```{r set_up}
source("../appendix/packages.R")
source("../appendix/feature_labels.R")
# final dataset, for data wrangling see appendix
my_data <- read.csv("../data/analysis_df.csv")

```

```{r weights_trans}
# weights and transformation 
# Function to calculate various effect sizes and to estimate the variance 
## for the traditional meta analysis

ies.da  <- escalc(xi = event , ni = total , data = my_data ,
               measure = "PFT",  # FT double  arcsine  transformation
               slab=paste(AuthorYear, " Estimate ", esid)
               ) 
#the ies: individual effect size, new variables:
## yi: FTT effect sizes
## vi: calculated variances 
ies.da$ancillary <- relevel(factor(ies.da$ancillary), ref = "Remote Sensing Only")
ies.da$indices <- relevel(factor(ies.da$indices), ref = "Not Used")
```

::: {style="text-align: justify"}
A total of $n = 20$ studies with $k = 86$ effect sizes were included in this analysis, with each primary study reported between one and 27 results ($1 \leq k_j \leq 27$). The research area of these studies span 18 countries, @fig-subfig-map shows a map indicating the location of each effect size. These primary studies were grouped into three different SDG goals: SDG 2 (Zero Hunger), SDG 11 (Sustainable Cities), and SDG 15 (Life on Land).
:::

::: {#fig-charts layout-nrow="2"}
```{r}
#| fig-height: 2.8
#| fig-width: 5
#| label: fig-subfig-map
#| fig-cap-location: top
#| fig-cap: "Map of researched locations"
sf_world <- ne_countries(returnclass = "sf")

# Get the world map data with country names, latitude, and longitude
world_map <- map_data("world") 
 
SDG_colours <- 
scale_colour_manual(values = c(
    "SDG2: Zero Hunger" = "#E8B700",  
    "SDG11: Sustainable Cities" = "#FF7518",  
    "SDG15: Life on Land" = "#2C9F2C"  
  ))  

study_map <- world_map |>
  group_by(region) |>
  summarize(lat = mean(lat), long = mean(long), .groups = 'drop')|>
  right_join(my_data, join_by("region"=="location"))
 
ggplot(sf_world) + 
  geom_sf(fill = "#f4f3f1")+
  coord_sf(ylim = c(-55,80)) +
  geom_jitter(data = study_map, aes(x = long, y = lat, colour = SDG_theme), 
              #color = "#001158",
              alpha = 0.4,
              size = 1.2, 
              width = 3, 
              height = 1, 
             show.legend = FALSE) +
 SDG_colours +
  theme_void()

```

```{r}
#| fig-height: 4.5
#| fig-width: 7
#| label: fig-subfig-oa
#| fig-cap-location: top
#| fig-cap:  "Reported overall accuracy by study" 
ggplot(ies.da ,aes(x = reorder(AuthorYear, OA_reported), y = OA_reported, 
               colour = SDG_theme
               ))+
  geom_point(alpha = 0.2, size = 1.5)+
  stat_summary(geom = "point", fun = "mean", 
                 size = 2, shape = 17)+
  labs(x = NULL, 
       y = "Observed Overall Accuracy", 
       title = " ") +
  SDG_colours+
  guides(colour=guide_legend(title= NULL, nrow=1, byrow=TRUE, 
                             override.aes = list(shape = 16, size = 2.5)
                             )
         )+
  coord_flip() +
  common_theme+
  theme(# grid lines 
        panel.grid.major.y = element_line(linewidth = 0.1, colour = "grey80"),
       # legend specs
        legend.position = "bottom",
        legend.key.size = unit(0, 'lines'), 
        legend.key.spacing.x = unit(-2.5, "lines"),
        legend.text= element_text(size=8, margin = margin(l = -0.3, r = 1.5, unit = "cm")), 
       plot.margin = margin(t= 2, b = 0, r= 0.1, l =0, "cm"), 
       legend.box.margin = margin(t= -0.4, b = 0, r= 0, l =0, "cm")
       )

```

Study location and range of reported overall accuracy, colour-coded by SDG goal. Individual outcomes shown as points and mean overall accuracy represented by triangles.
:::

```{r normal_test}
shapiro_test_result <- round(shapiro_test(ies.da$yi)[, 2:3], 2)
```

::: {style="text-align: justify"}
@fig-subfig-oa and @tbl-sum (bellow) show, the reported overall accuracies are not centered around 0.5. Therefore, a transformation is required. @fig-transfrom shows the distribution of observed overall accuracy as well as the logit and FT transformation values. FT visually performs better than the Logit transformation. However the Shapiro-Wilk Normality Test shows that the distribution of the FT transformed overall accuracy still departed significantly from normality ($W =$ `r shapiro_test_result$statistic`, p-value \< 0.01). Nevertheless, conducting a meta-analysis remains justified, as these statistical models are generally robust against violations of normality [@mcculloch2012].
:::

::: {#fig-transfrom}
```{r fig_transform}
#| fig-width: 8
#| fig-height: 4

raw_propotions <- ggplot(ies.da, aes(x = OA_reported))+
  geom_histogram(bins = 30, fill = "#001158", alpha = .8) +
  labs(x = "Observed Overall Accuracy", y = "Count") +
  common_theme
  

logit_transformation <- ggplot(ies.da, aes(x = log(OA_reported/(1-OA_reported))))+
  geom_histogram(bins = 30, fill = "#001158", alpha = .8)+
  labs(x = "Logit-transformed Overall Accuracy", y = NULL) +
  common_theme

arcsin_transformation<- ggplot(ies.da, aes(x = yi))+
  geom_histogram(bins = 30, fill = "#001158", alpha = .8)+
  labs(x = "FT-transformed Overall Accuracy", y = NULL)+
  common_theme

raw_propotions + logit_transformation+ arcsin_transformation +
  plot_annotation(title = "Density Plots of Observed and Transformed Overall Accuracy", 
                  theme = theme(plot.title = element_text(size = 10)))



```

Distribution of the observed overall accuracy and transformed by logit and FT transformation.
:::

::: {style="text-align: justify"}
@tbl-sum summarises the overall accuracy (effect size of interest), study sample size and the collected study features, including the study features such as sample size, overall accuracy, types of machine learning models used and SDG goal targeted. For the meta-analysis the range of the sample size `r paste("(", min(ies.da$total), " - ", max(ies.da$total), sep= "", ")" )` and overall accuracy `r paste("(", min(ies.da$OA_reported), " - ", max(ies.da$OA_reported), sep= "", ")" )` are of importance. Most studies used Neural Networks (48%), followed by Tree-Based Models (45%), and a small portion used other types of models (7%). Regarding SDGs, 44% of the studies aimed at SDG 11 (Sustainable Cities), 43% targeted SDG 15 (Life on Land), and 13% focused on SDG 2 (Zero Hunger).
:::

\newpage

::: {#tbl-sum}
\renewcommand{\arraystretch}{0.68}

```{r}
ies.da |>
  select(
    OA_reported,
    total,
    globalCitationsCount,
    number_classes,
    fraction_majority_class,
    Publication.Year,
    SDG_theme,
    classification_type, 
    model_group, 
    ancillary,
    indices,
    RS_device_type,
    RS_device_group,
    no_band_group,
    RS_spatital_res_grouped, 
    Confusion_matrix
  ) |> 
  tbl_summary(
    statistic = list(
      all_continuous() ~ "{mean} ({min} - {max})",
      all_categorical() ~ "{n} ({p}%)"
    ),
    label = feature_labels,
    digits = all_continuous() ~ 2,
    missing = "no"
  ) |>
  modify_header(label = "Feature") |>
  modify_header(all_stat_cols() ~ "Statistic") |>
  as_kable(
    booktabs = TRUE,
    longtable = TRUE, 
    linesep = ""
  )|>
  kable_styling(font_size = 9, 
                full_width = FALSE)|>
  pack_rows("Study Features", 2, 24, latex_gap_space = "0.1em" #, hline_before = T
            )|>
  pack_rows("Numeric b", 2, 5, latex_gap_space = "0em")|>
  pack_rows("Categorical c", 6,46, latex_gap_space = "0em")|>
   row_spec(c(0),  bold = TRUE)|>
  column_spec(1, width = "7cm")|>
  column_spec(2, width = "6cm")|>
  add_indent(c(7:12, 14:16, 18:20, 22:24, 26:27, 
               29:30, 32:35,37:40,42:44,
               46:48), 
               level_of_indent = 1.5)|>
  footnote(alphabet = c("Effect size of interest, b. Numeric: mean (min - max), c. Categorical variables: number of effect sizes (%)"), 
               threeparttable=TRUE)

```

Summary table
:::

\blandscape

::: {#fig-features}
```{r fig_study_features_plots}
# common aesthetics 
box_theme <- common_theme + theme(axis.title.y = element_blank())

point_color <- "#001158"
ylab <- c("FT Transformed Overall Accuracy")

# Scatter plots for continuous variables

sample_size <- ggplot(ies.da, aes(x = log10(total), y = yi)) +
  geom_point(size = 1, alpha = 0.8, colour = point_color) +
  labs(title = feature_labels$total, 
       y = ylab, 
        x = bquote("Sample Size:"~log[10](m[ij])), 
       #caption = stringr::str_wrap("Note: Log transformation was applied to handle large variation in sample sizes", width = 42)
       ) +
   #expand_limits(x = 0)+

  common_theme  #+
  #theme(plot.caption = element_text(size = 7.5, hjust = 0, color = "gray30", 
   #                                 margin = margin(t = 5, r = 0, b = 20, l = 0))) 

number_class <- ggplot(ies.da, aes(x = number_classes, y = yi)) +
  geom_point(size = 1, alpha = 0.8, colour = point_color) +
  labs(title = feature_labels$number_classes, y = ylab, x = "Classes") + 
  scale_x_continuous(breaks=seq(0, 13, 2))+
   expand_limits(x = 0)+
  common_theme 

fraction_majority_class <- ggplot(ies.da, aes(x = fraction_majority_class, y = yi)) +
  geom_point(size = 1, alpha = 0.8, colour = point_color) +
  labs(title = feature_labels$fraction_majority_class, 
       y = ylab, 
       x = "Proportion") +
   expand_limits(x = 0)+
  common_theme 

cits <- ggplot(ies.da, aes(x = globalCitationsCount , y = yi)) +
  geom_point(size = 1, alpha = 0.8, colour = point_color) +
  labs(title = feature_labels$globalCitationsCount, 
       y = ylab, 
       x = "Citation") + 
   expand_limits(x = 0)+
  common_theme 

#######################
# Boxplots for categorical variables
modelgroup_box <- ggplot(ies.da, aes(y = yi, 
                                     x = factor(model_group, 
                                                levels = c("Other", 
                                                           "Neural Networks", 
                                                           "Tree-Based Models")))) +
  geom_boxplot() +
  geom_jitter(colour = point_color, alpha = 0.7) +
  scale_x_discrete_wrap() +
  labs(title = feature_labels$model_group, 
       y = NULL) +  # patchwork collect x.axis not being agreeable manual selection 
  coord_flip() + box_theme

classtype_box <- ggplot(ies.da, aes(y = yi, x = fct_rev(classification_type))) +
  geom_boxplot() +
  geom_jitter(colour = point_color, alpha = 0.7) +
  labs(title = feature_labels$classification_type, 
       y = ylab) +
  coord_flip() + box_theme

indexes_box <- ggplot(ies.da, aes(y = yi, x = indices)) +
  geom_boxplot() +
  geom_jitter(colour = point_color, alpha = 0.7) +
  labs(title = feature_labels$indices, y = ylab) +
  coord_flip() + box_theme

ancillary_box <- ggplot(ies.da, aes(y = yi, x = ancillary)) +
  geom_boxplot() +
  geom_jitter(colour = point_color, alpha = 0.7) +
  scale_x_discrete_wrap() +
  labs(title = feature_labels$ancillary, y = NULL) +
  coord_flip() + box_theme

# Data about RS devices and features
RStype_box <- ggplot(ies.da, aes(y = yi, x = factor(RS_device_type,
                                                     levels = c("Not Reported", "Combined", "Passive","Active")))) +
  geom_boxplot() +
  geom_jitter(colour = point_color, alpha = 0.7) +
  scale_x_discrete_wrap() +
  labs(title = feature_labels$RS_device_type, y = NULL) +
  coord_flip() + box_theme

RSgroup_box <- ggplot(ies.da, aes(y = yi, 
                                  x = factor(RS_device_group,
                                          levels = c("Not Reported", "Other", "Sentinel","Landsat")))) +
  geom_boxplot() +
  geom_jitter(colour = point_color, alpha = 0.7) +
  labs(title = feature_labels$RS_device_group, y = NULL) +
  coord_flip() + box_theme

RS_resolution_box <- ggplot(ies.da, aes(y = yi, x = fct_rev(RS_spatital_res_grouped))) +
  geom_boxplot() +
  geom_jitter(colour = point_color, alpha = 0.7) +
  labs(title = feature_labels$RS_spatital_res_grouped, y = ylab) +
  coord_flip() + box_theme

SDG_theme_box <- ggplot(ies.da, 
                        aes(y = yi, x = factor(SDG_theme, 
                                               levels = c("SDG2: Zero Hunger",
                                                          "SDG11: Sustainable Cities", 
                                                          "SDG15: Life on Land")))) +
  geom_boxplot() +
  geom_jitter(colour = point_color, alpha = 0.7) +
  scale_x_discrete_wrap() +
  labs(title = feature_labels$SDG_theme, y = NULL) +
  coord_flip() + box_theme


publication_year_box <- ggplot(ies.da, aes(x = as.factor(Publication.Year), y = yi)) +
  geom_boxplot() +
  geom_jitter(colour = point_color, alpha = 0.7) +
  labs(title = feature_labels$Publication.Year, y = ylab) +
  coord_flip() + box_theme


no_band_group_box <- ggplot(ies.da, aes(y = yi, x = fct_rev(no_band_group))) +
  geom_boxplot() +
  geom_jitter(colour = point_color, alpha = 0.7) +
  labs(title = feature_labels$no_band_group, y = ylab) +
  coord_flip() + box_theme

confusion_matrix_box <- ggplot(ies.da, aes(y = yi, x = Confusion_matrix)) +
  geom_boxplot() +
  geom_jitter(colour = point_color, alpha = 0.7) +
  labs(title = feature_labels$Confusion_matrix, y = NULL) +
  coord_flip() + box_theme


```

```{r}
#| fig-width: 11
#| fig-height: 7

# layout areas 
design <- c(
  area(1, 1, 2, 1),
  area(1, 2),
  area(1, 3),
  area(1, 4),
  area(2, 2),
  area(2, 3),
  area(2, 4),
  area(3, 1),
  area(3, 2),
  area(3, 3),
  area(3, 4)
)

publication_year_box + SDG_theme_box + classtype_box +  modelgroup_box + 
ancillary_box + indexes_box + RStype_box + no_band_group_box +
RSgroup_box + RS_resolution_box + confusion_matrix_box +
  plot_layout(design = design)


```

Categorical study features
:::

\elandscape

::: {#fig-features2}
```{r}
#| fig-width: 6
#| fig-height: 5

fraction_majority_class + cits +
number_class + sample_size +
plot_layout(axis_titles = 'collect_y', ncol = 2)

```

Numeric study features
:::

## Meta-analysis

```{r lvl2_mod}
# METHOD: Weighted: level 2 model
## pes: pooled effect size
pes.da  <- rma(yi,
               vi,
               data = ies.da,
               method = "REML",
               test = "t")
```

```{r lvl3_mod}
# METHOD: Weighted: Nested level 3model
pes.da.lvl3 <- rma.mv(yi,
                      vi,
                      data = ies.da ,
                      tdist = TRUE,
        # adding random effects at the study level and effect size 
                      random = ~ 1 | AuthorYear / esid,
                      method = "REML",
                      # recommendations from the function documentation:
                      test="t",  
                      dfs="contain"
                    )

#summary(pes.da.lvl3)

```

```{r lvl3_mod_backtras}
pes <- predict(pes.da.lvl3, transf = transf.ipft.hm, targ = list(ni=1/(pes.da.lvl3$se)^2))

pes.da.lvl3_I_squared <- dmetar::var.comp(pes.da.lvl3)

pes.da.lvl3_CI <- confint(pes.da.lvl3)

results <- data.frame(
  theta = pes$pred, 
  ci_l = pes$ci.lb,
  ci_u = pes$ci.ub,
  sigma2.1 = pes.da.lvl3$sigma2[1],
  sigma2.2 = pes.da.lvl3$sigma2[2],
  df = pes.da.lvl3$QEdf, 
  Q = pes.da.lvl3$QE, 
  p = pes.da.lvl3$QEp,
  I_L2 = pes.da.lvl3_I_squared$results$I2[2],  
  I_L3 = pes.da.lvl3_I_squared$results$I2[3] 
)
row.names(results) <-"RE_lvl3"

# results$theta
# # same as
# v_bar <- (pes.da.lvl3$se)^2
# t_bar <- pes.da.lvl3$b[1]
# (1/2 * (1 - sign(cos(2*t_bar)) *
#           sqrt(1 - (sin(2*t_bar)+(sin(2*t_bar)-1/sin(2*t_bar))/(1/v_bar))^2)))
# 

```

```{r lvl3_mod_test}
#profile likelihood plots of the variance components of the model
#par(mfrow=c(2,1))
#profile(pes.da.lvl3, sigma2=1)
#profile(pes.da.lvl3, sigma2=2)

# METHOD 1.2:  
# multivariate parameterization model 
# this is a multilvl rather than a nested model (if since the samples could be different data or countries this might be the better approch?)
pes.da.lvl3.mv <- rma.mv(yi,
                      vi,
                      data = ies.da,
                      random = ~ esid|AuthorYear,
                      method = "REML",
                      # recomendations from the function detains +Assink et.al:
                      tdist = TRUE,
                      test="t",  
                      dfs="contain"
                    )

# should be exactly the same
#logLik(pes.da.lvl3)
#logLik(pes.da.lvl3.mv)


# future research look into different variance-covariance matrix structures

```

```{r weights_pixel_v_object, eval=FALSE}
# 
W <- weights(pes.da.lvl3, type="matrix")
X <- model.matrix(pes.da.lvl3)
y <- cbind(ies.da$yi)
solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y


# weights of pixel- vs weights of object- based studies 
ies.da$weights <- weights(pes.da.lvl3, type = "rowsum")

hist(ies.da$weights)

ggplot(ies.da, mapping = aes(classification_type, weights))+
  geom_boxplot()+
  geom_jitter(aes(colour = AuthorYear), alpha = 0.7)

t.test(weights~classification_type, 
       # removing unclear
       data = subset(ies.da, ies.da$classification_type != "Unclear"))

ggplot( subset(ies.da, ies.da$classification_type != "Unclear"),
        mapping = aes(classification_type, weights))+
  geom_boxplot()

```

```{r fig_forrest, eval=FALSE}
#| fig-width: 10
#| fig-height: 8 

# To visualize results aggregate model 

### assume that the effect sizes within studies are correlated with 
V <- vcalc(vi, cluster=AuthorYear, obs=esid, data=ies.da, rho=pes.da.lvl3.mv$rho)
 
### fit multilevel model using this approximate V matrix
pes_agg.da.lvl3 <- rma.mv(yi, V, random = ~ 1 | AuthorYear/esid, data=ies.da)

agg <- aggregate(ies.da, cluster=AuthorYear, V=vcov(pes_agg.da.lvl3, type="obs"), addk=TRUE)

agg.pes.da <- rma(yi, vi, method="EE", data=agg, digits=3)
agg.pes <- predict(agg.pes.da , transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3$se)^2))

# forest plot:
model_summary <- bquote(paste("RE (3-Level) Model ",
                 "(Q = ",
                 .(fmtx(results$Q[1], digits=0)),
                 ", df = ", 
                 .(fmtx(results$df[1], digits=0)),
                 ", p <.001; ", 
                 I^2, " = 100% ",
                       sep = ""))

png(file='forestplot_weighted.png', width = 1000, height = 800)
forest(agg.pes.da,addpred=TRUE, 
       xlim=c(-2,2.5),
       alim =c(0, 1),
       transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3$se)^2), 
       header=TRUE,
       slab=AuthorYear,
       order="obs",
       cex=1.5, 
       refline=NA,
       digits = 3,
       ilab=ki, 
       mlab= model_summary,
       ilab.xpos= -.3,
       ilab.pos = 2, 
       #showweights = TRUE, 
       shade="zebra"
       )

text(-.5, agg.pes.da$k+2, "OA Count", cex=1.5, font=2)
text(2.1, agg.pes.da$k+3, "Weighted", cex=1.5, font=2)


```

```{r unweighted_mod}
########################
# METHOD 2: UnWeighted
# the mean of the yi should be the estimate of effect 
pes.da.lvl3_unweighted  <- rma.mv(yi = yi, 
                                  V = 1,
                                  random = ~ 1 | AuthorYear / esid, # Nested random effects 
                                  data = ies.da, 
                                  method = "REML", 
                                  tdist = TRUE,
                                  test="t", 
                                  dfs="contain")
#summary(pes.da.lvl3_unweighted)
#mean(ies.da$yi)

pes_u <- predict(pes.da.lvl3_unweighted, transf = transf.ipft.hm, targ = list(ni=1/(pes.da.lvl3_unweighted$se)^2))

theta_u <- mean(ies.da$OA_reported)
t_critical <- qt(0.975, 19)
se <- 1 / sqrt(86)

results["Unwei_rma.mv"] <- NA
results["Unwei_rma.mv", 1:3 ]<- c(pes_u$pred, pes_u$ci.lb, pes_u$ci.ub)

results<- results|> mutate_if(is.numeric, round, digits=3)

```

```{r fig_forrest_unweighted, eval=FALSE}
# To visualize results aggregate model 

### assume that the effect sizes within studies are correlated with 
V <- vcalc(vi, cluster=AuthorYear, obs=esid, data=ies.da, rho=0.6)
 
### fit multilevel model using this approximate V matrix
pes_agg.unweighted <- rma.mv(yi, V, random = ~ 1 | AuthorYear/esid, data=ies.da)

agg <- aggregate(ies.da, cluster=AuthorYear, V=vcov(pes_agg.unweighted, type="obs"), addk=TRUE)

agg.pes.da <- rma(yi, vi, method="EE", data=agg, digits=3)
agg.pes <- predict(agg.pes.da , transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3_unweighted$se)^2))

# forest plot:
png(file='forestplot_unweighted.png', width = 1000, height = 800)
forest(agg.pes.da,addpred=TRUE, 
       xlim=c(-2,2.5),
       alim =c(0, 1),
       transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3_unweighted$se)^2), 
       header=TRUE,
       slab=AuthorYear,
       order="obs",
       refline=NA,
       col =  "#001158",
       cex=1.5, 
       digits = 3,
       ilab=ki, 
       mlab= "Unweighted Model",
       ilab.xpos= -.3,
       ilab.pos = 2, 
       #showweights = TRUE, 
       shade="zebra"
       )

text(-.5, agg.pes.da$k+2, "OA Count", cex=1.5, font=2)
text(2.1, agg.pes.da$k+3, "Unweighted", cex=1.5, font=2)


```

::: {style="text-align: justify"}
The forest plot below (@fig-forest) compares the overall accuracy effect size across studies using both weighted and unweighted models, with error bars which correspond to the weighted model --- at this scale there is no discernible difference between the error bars of the two models. Each study is given with the number of estimates per study $k_j$, and study average effect size ($\kappa_j$), with 95% confidence intervals (CI), both for the weighted and unweighted model. Of the 20 primary studies included, six reported only one effect. Based on the unweighted model, the average accuracy of machine learning methods applied to remote sensing data is 0.90 (95% CI\[0.85; 0.94\]). While the three-level meta-analytic model produced an average accuracy of 0.89 (95% CI\[0.85; 0.93\]). This implies, that on average, the machine learning methods correctly classify around 90% of the time when applied to remote sensing data.
:::

::: {style="text-align: justify"}
![Forest plot for both the weighted and unweighted model. $k_j$ is number of reported overall accuracy estimates per study, the corresponding average effect size($\kappa_j$) and confidence interval per study for both models is given on the right. The pooled summary effect size based on the three-level RE meta-analytic and unweighted model are given on the bottom.](figures/forestplot_comb.png){#fig-forest width="18cm" height="17cm"}
:::

::: {style="text-align: justify"}
The heterogeneity metrics Cochran's Q indicate significant heterogeneity of the reported overall acccuracies. The percentage of the variance attribution is $I^2_{\text{level3}}$ = `r results$I_L3[1]`% which is the fraction of the variation that can be attributed to between-study, and $I^2_{\text{level2}}$ = `r results$I_L2[1]`% which is within-study heterogeneity, with negligible fixed effect variance (variance due to sampling error). The $I^2$ value of 100% indicates that all the observed variability in effect sizes across studies is due to heterogeneity rather than sampling error, suggesting substantial differences between the studies and a high degree of variation in their results.

#### Model Selection

Using the multi-model inference function, a total of 31,298 models were fitted. @fig-best, illustrates the predictor importance after evaluating all possible combinations of predictors to identify which combination provides the best fit and which predictors are most influential. Higher importance values indicate more consistent inclusion in high-weight models. The majority class proportion is the most important predictor, followed by the inclusion of ancillary data. Less influential predictors include used of indices, sample size, publication year, and the number of classes in the study. Meanwhile, factors such as classification type, SDG goal, machine learning group, spatial resolution, and citation count have minimal importance in the overall model performance (i.e., where not included in the models top performing models according to AIC).
:::

```{r eval=FALSE}
study_features <- c("model_group", "indices", "SDG_theme", "classification_type",
  "Confusion_matrix", "RS_device_type", "RS_device_group", "RS_spatital_res_grouped",
  "ancillary", "no_band_group", "Publication.Year", "total",
  "globalCitationsCount", "number_classes", "fraction_majority_class"
)

ies.da$se <- sqrt(ies.da$vi)
multi_inf <- multimodel.inference(TE = "yi", 
                     seTE = "se",
                     method='REML', 
                     test='t', 
                     data = ies.da,
                     predictors = study_features,
                     interaction = FALSE, 
                     seed=357)

multi_inf$predictor.importance.plot+
  scale_x_discrete(labels = feature_labels)+
  labs(x = NULL, 
       title = "Model-averaged predictor importance plot" )+
  
  theme_apa()+
  common_theme
ggsave("figures/fig-best_mod.png", height =4, width = 7)

#saveRDS(multi_inf, "../appendix/multimodel_inference_out.rda")


```

::: {style="text-align: justify"}
![Model-averaged predictor importance plot with a reference line at 0.8 a commonly used as a threshold to indicate important predictors.](figures/fig-best_mod.png){#fig-best width="697"}
:::

@tbl-multi_coef shows the results of the multi-model inference. The significant study features are the Majority-class Proportion and the inclusion of ancillary data. Interestingly, the use of ancillary data has a negative effect on overall accuracy.

::: {#tbl-multi_coef}
\renewcommand{\arraystretch}{0.68}

```{r multi_coef}
multi_coef<- read.csv("multimodel_inference_results.csv")
multi_coef[is.na(multi_coef)] <- ""
  
kable(
  multi_coef,
    booktabs = TRUE,
    #longtable = TRUE, 
    linesep = ""
  )|>
  kable_styling(font_size = 9, 
                full_width = FALSE)|>
 row_spec(c(0),  bold = TRUE)
  
```

Multimodel inference coefficients and feature importance. The estimated coefficients (b) and standard error (SE) are in FT transformed scale.
:::

<div>

Multimodel inference something about best 5 models and comparing AIC

</div>

::: {#tbl-compare}
```{r}
multi_inf<- readRDS("../appendix/multimodel_inference_out.rda")

models <- data.frame(
  #Model.no = c(42, 138, 10, 142, 16522, 1),
  "Candidate models" = c("Ancillary Data + Majority-class Proportion + Indices", 
                          "Ancillary Data + Majority-class Proportion + Number of Spectral Bands", 
                          "Ancillary Data + Majority-class Proportion", 
                          "Ancillary Data + Confusion Matrix + Majority-class Proportion + Number of Spectral Bands", 
                          "Ancillary Data + Majority-class Proportion + Number of Spectral Bands + Sample Size", 
                         "Intercept-Only"), 
  "df" =  c(multi_inf$top5.models$df, 2), 
  AICc = c(multi_inf$top5.models$AICc, -41.93257), 
  "Akaike weights" = c(multi_inf$top5.models$weight, 2.444007e-17)
)

kable(
  models,
  booktabs = TRUE,
  linesep = c("\\addlinespace \\addlinespace"),
  col.names = c("Candidate models", "df", "AIC_c", "Akaike weights"),
  digits = c(0,0, 2, 2,2)
  )|>
  kable_styling(font_size = 10, 
                full_width = FALSE)|>
 row_spec(c(0),  bold = TRUE)|>
  column_spec(1, width = "8.5cm")

```

Set of 5 best-ranked models and intercept only model ordered by AIC$_c$
:::

```{r best_mod}
meta_reg_42 <- rma.mv(yi, vi,
  data = ies.da ,
  random = ~ 1 | AuthorYear / esid,
  tdist = TRUE,
  method = "REML",
  test = "t", 
  dfs="contain",
  mods = ~ fraction_majority_class+ancillary+ indices
)
```

::: {style="text-align: justify"}
@tbl-best_mod_coef shows the estimated coefficients for the best fit model (lowest AIC), both in the FT transformed scale (b) and on the natural scale (b back-transformed). This shows that the proportion of majority class has the largest positive impact on the model's outcome (b = 0.39, p \< .001), while the inclusion of ancillary data has a small negative effect (b = -0.11, p = 0.029) but a small but positive effect when back-transfored . The use of indices has a minimal and non-significant effect (b = 0.06, p = 0.131).
:::

::: {#tbl-best_mod_coef}
```{r}
backtras.function<- function(t_bar, se){
  v_bar <- (se)^2
  (1/2 * (1 - sign(cos(2*t_bar)) *
           sqrt(1 - (sin(2*t_bar)+(sin(2*t_bar)-1/sin(2*t_bar))/(1/v_bar))^2)))
}
ci.function <- function(ci_l, ci_u, se){
  back_ci_l = backtras.function(ci_l, se)
  back_ci_u = backtras.function(ci_u, se)
  out<- paste0("[", round(back_ci_l, 2), ", ", round(back_ci_u, 2), "]")
  return(out)
}

meta_reg_42|>
  tidy() |>
  mutate(estimate_back_transfromed = mapply(backtras.function, meta_reg_42$b, meta_reg_42$se))|>
  mutate(CI = mapply(ci.function, meta_reg_42$ci.lb,meta_reg_42$ci.ub, meta_reg_42$se))|>
  select(term, estimate, std.error, statistic,	p.value, estimate_back_transfromed, CI)|>
  kable(booktabs = TRUE, linesep = "", 
        col.names = c("Predictor", "b", "SE", "t", "p", "b_BT", "CI"), 
        digits = c(0, 2, 2, 2, 3, 2), 
        align = c("l", "r", "r", "r", "r", "r")
  ) |>
  kable_styling(font_size = 10, 
                full_width = FALSE)|>
  add_header_above(c(" ","","","","", "back-transfromed scale" = 2)) |>
  row_spec(c(0),  bold = TRUE)|>
  column_spec(1, width = "6cm")|>
  footnote(c("The estimated coefficients (b), standard errors (SE) on the FT transformed scale, t-statistics, and p-values. Additionally, the coefficients (b) and their confidence intervals (CI) are shown on the back-transformed scale."), threeparttable=T)
```

Results of the best fit model.
:::

::: {#tbl-hetro}
```{r multi-level_meta-regression}

meta_reg_anc <- rma.mv(yi, vi,
                       data = ies.da,
                       random = ~ 1 | AuthorYear / esid,
                       tdist = TRUE,
                       method = "REML",
                       test = "t", 
                       dfs="contain", 
                       mods = ~ ancillary)


meta_reg_frac <- rma.mv(yi, vi,
                       data = ies.da,
                       random = ~ 1 | AuthorYear / esid,
                       tdist = TRUE,
                       method = "REML",
                       test = "t", 
                       dfs="contain", 
                       mods = ~ fraction_majority_class)

extract_parameters <- function(model) {
  out <- data.frame(model = if (!is.null(model$formula.mods)) paste(model$formula.mods)[2] else "Intercept Only",  
                    sig_lvl2 = round(model$sigma2[2], 3), 
                    sig_lvl3 = round(model$sigma2[1], 3), 
                    QE = round(model$QE,0), 
                    df_Q = model$QEdf,
                    p_Q = round(model$QEp,3), 
  #for F-test
  "F" = if (!is.null(model$formula.mods)) round(model$QM,0) else NA,
  df_F = if (!is.null(model$formula.mods)) model$QMdf[1] else NA,
  p_F = if (!is.null(model$formula.mods)) round(model$QMp,3) else NA, 
  I2_lvl2 = dmetar::var.comp(model)$results[2,2], 
  I2_lvl3 = dmetar::var.comp(model)$results[3,2],
  R2_lvl2 = if (!is.null(model$formula.mods)) (round(1-(model$sigma2[2]/pes.da.lvl3$sigma2[2]),3)*100) else NA, 
  R2_lvl3 = if (!is.null(model$formula.mods)) (round(1-(model$sigma2[1]/pes.da.lvl3$sigma2[1]),3)*100) else NA
  
  )
    return(out)
}

heterogeneity_tbl <- rbind(extract_parameters(pes.da.lvl3), 
                           extract_parameters(meta_reg_frac), 
                           extract_parameters(meta_reg_anc), 
                           extract_parameters(meta_reg_42))
heterogeneity_tbl[] <- lapply(heterogeneity_tbl, function(x) {
  if (is.numeric(x)) {
    return(as.character(x))
  }
  return(x)
})
transposed <- heterogeneity_tbl %>%
  pivot_longer(cols = -model, names_to = "Parameter", values_to = "Value")|>
  pivot_wider(names_from = model, values_from = Value)



kable(
  transposed,
    booktabs = TRUE,
    linesep = "", 
  align = c("l", "r", "r", "r", "r", "r"),
  col.names = c("Paramter", "Intercept Only", 
                "Majority-class Proportion", 
                "Ancillary Data", 
                "Ancillary Data + Majority-class Proportion + Indices")
  )|>
  kable_styling(font_size = 9.5, 
                full_width = FALSE)|>
 row_spec(c(0),  bold = TRUE)|>
  column_spec(1, width = "1.5cm")|>
  column_spec(2, width = "2cm")|>
  column_spec(3, width = "2.5cm")|>
  column_spec(4, width = "2cm")|>
  column_spec(5, width = "5.5cm")|>
  add_header_above(c(" ","Model" = 4), bold = TRUE) |>
  footnote(c("Test statistic, degrees of freedom and respective p values are provide. This table allows heterogeneity at level 2 and 3 can be compared between the incetept only model, Majority-class Proportion and Adncillary Data only models, as well as the combinded model"), threeparttable=TRUE)

```

Results for heterogeneity and covariates tests for intercept only model, individual covariates as well as the best model.
:::

@tbl-hetro shows the parameter estimates of the meta-analysis comparing the intercept only and three mixed effects models: (1) with the Majority-class Proportion as the only covariate, (2) use Ancillary Data only, and (3) the best fit model (from @tbl-best_mod_coef). Majority-class Proportion explains more of the between study heterogeneity, as shown by the difference in $\sigma^2_{\text{level2}}$ between the intercept only and the Majority-class Proportion. The use of Ancillary Data explains relatively little between study heterogeneity and negligible within study heterogeneity. The combined model explains the most heterogeneity. This shift is also reflect in the $I^2$. The total $I^2$ consistently being 100% in both models indicates that almost none of the variation between effect sizes can be attributed to sampling error, this might suggest that the included studies are too different from each to compare (see discussion for apples and oranges problem). All models show significant heterogeneity (Cochran's Q, p \< 0.001) results. The $R^2$values show that the covariates in the combined mixed effects model explain 69.9% of the variance at level 3 and 8.6% at level 2.

::: {style="text-align: justify"}
@fig-bubble illustrates the relationship between the proportion of the majority class and overall accuracy of the individual studies included in the meta-analysis. The plot is based on combined mixed effects model, with the solid black line representing the fitted regression line and the shaded area indicating the 95% confidence interval. Each point (bubble) represents a study, with its size proportional to the weight it received in the analysis (larger points indicate studies with more influence). The plot shows that as the proportion of the majority class increases, overall accuracy tends to improve.
:::

```{r, eval=FALSE}
library(RColorBrewer)
colors <- c(brewer.pal(9, "Pastel1"), brewer.pal(12, "Set3"))
authors <- unique(ies.da$AuthorYear)
author_colors <- setNames(colors[1:length(authors)], authors)
ies.da$color <- author_colors[ies.da$AuthorYear]

# labels 
lowest_yi_per_author <- ies.da |>
  group_by(AuthorYear) |>
  filter(yi == min(yi)) |>
  select(AuthorYear, effectsizeID)
png(file="fig-bubble.png", width=700, height=700)
bubble_plot <- regplot(meta_reg_22,
        mod = "fraction_majority_class", 
        transf = transf.ipft.hm, targ =list(ni=1/(meta_reg_22$se)^2), 
        xlab = "Proportion of majority class", 
        ylab = "Overall Accuracy",
        slab = ies.da$AuthorYear, 
        label = lowest_yi_per_author$effectsizeID,
        labsize = 1, 
        bg=ies.da$color, 
        xlim=c(0.05, 1.05), 
        ylim=c(0.64, 1))

dev.off()
```

![Bubble plot showing the observed effect size, overall accuracy of the individual studies plotted against a the proportion of the majority class. Based on the mixed-effects meta-regression model, the overall accuracy as a function of proportion of the majority with corresponding 95% confidence interval bounds. The size of the points are proportional to the weight that the observation received in the analysis, while the color of the points is unique to each study, with the lowest overall actuary from each study labeled with the first author and publication year.](figures/fig-bubble.png){#fig-bubble width="673"}

::: {style="text-align: justify"}
@fig-preds shows the observed overall accuracy against the predicted overall accuracy's made by combined mixed effects model. The points are coloured by the addition of ancillary information in the primary study. It appears that the addition of ancillary information leads to a lower overall accuracy, however, this could be due to a number of unmeasured factors, such as study's with more complicated classifications (more similar classes) adding accuracy data. As @fig-preds shows Model 2 over estimates the overall accuracy --- the fit regression line (in grey) is above the line of perfect agreement ($y = x$, in black).
:::

::: {#fig-preds}
```{r preds}
ies.da$preds <- predict(meta_reg_42, transf = transf.ipft.hm, targ =list(ni=1/(meta_reg_42$se)^2))$pred

ggplot(ies.da, aes(x = OA_reported, y = preds))+
  geom_point(size = 5, aes(colour = ancillary), alpha = 0.7)+
  geom_smooth(method = "lm",formula = 'y ~ x',  se = FALSE, colour = "grey", linetype= "dashed", linewidth = 0.8)+
  # y = x line 
  geom_textabline(label = "y = x", intercept = 0, slope = 1,hjust = 0.2,  linetype= "longdash")+
  scale_colour_brewer(palette = "Dark2",
                      labels = c("Only Remote Sensing Data","Addition of Ancillary Data"))+
  xlim(c(0.64, 1))+
  ylim(c(0.64,1))+
  labs(x = "Reported Overall Accuracy", 
       y = "Predicted Overall Accuracy", 
       title = "Plot of Observed Agaist Predicted Accuracy Based on the Meta-regression Model.")+
    theme(legend.text = element_text(size = 8), 
          legend.position = "top") +
  common_theme
```

Observed and predicted overall accuracy. The colour indicates whether the addition of ancillary data in the primary study's model. The line of perfect agreement $y = x$ is in black and fit regression of the points in grey.
:::
