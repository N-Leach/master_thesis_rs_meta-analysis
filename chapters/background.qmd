# Background {#sec-background style="text-align: justify"}

```{r set-up}
source("../appendix/packages.R")
source("../appendix/feature_labels.R")
```

## Remote Sensing

```{r}
# data for the number of satellites
UCS_Satellite_Database <- read.csv("figures/UCS-Satellite-Database_5-1-2023.csv")

UCS_Satellite_Database_EarthObs <- 
  UCS_Satellite_Database |> 
  mutate(Date_Launch = as.Date(Date_Launch, format = "%Y-%m-%d"))|>
  filter(!grepl("Military", Users))|>
  filter(grepl("Earth",Purpose ))

prop_after_2020<- sum(year(UCS_Satellite_Database_EarthObs$Date_Launch)>=2020,
                      na.rm = TRUE)/ nrow(UCS_Satellite_Database_EarthObs)

```

::: {style="text-align: justify"}
In the broadest sense, remote sensing involves acquiring information about an object or phenomenon without direct contact [@campbell2011]. More specifically, remote sensing refers to gathering data about land or water surfaces using sensors mounted on aerial or satellite platforms that record electromagnetic radiation reflected or emitted from the Earth's surface [@campbell2011, pp.6]. The origins of remote sensing lie with the development of photography in the 19th century, with the earliest aerial or Earth Observation photographs taken with cameras mounted on balloons, kites, pigeons, and aeroplanes. [@campbell2011, pp.7; @burke2021]. The first mass use of remote sensing was during World War I with aerial photography. The modern era of satellite-based remote sensing started with the launch of Landsat 1 in 1972, the first satellite specifically designed for Earth Observation [@campbell2011, pp.15]. Today, remote sensing technology enables frequent and systematic collection of data about the Earth's surface with global coverage, revolutionizing our ability to monitor and analyze the Earth's surface [@burke2021; @nasa2019]. As of May 2023, roughly 1039 active nonmilitary Earth Observation satellites are in orbit; 51% were launched in 2020 [@unionofconcernedscientists].
:::

::: {#fig-UCS}
```{r}
#| fig-width: 5
#| fig-height: 3

ggplot(data = UCS_Satellite_Database_EarthObs|> 
         group_by(week = lubridate::floor_date(Date_Launch, 'week'))|>
         summarise(n = n()),
       aes(x = week, y = n)) +
  geom_line(aes(y = cumsum(n)), na.rm = TRUE) +
  
  scale_x_date(date_breaks = "5 years", date_labels = "%Y") +
  
  labs(y = "Number of Satellite", x = "Date", 
        title = "Number of Satellites Launched Over Time") +
  common_theme

```

Number of active satellites by date of launch. Data acquired from @unionofconcernedscientists.
:::

::: {style="text-align: justify"}
Sensors on remote sensing devices such as satellites measure electromagnetic radiation reflected by objects on the Earth's surface. This is done in two different ways: passive and active. Passive sensors rely on natural energy sources, like sunlight, to record incident energy reflected off the Earth's surface. While active sensors generate their own energy, which is emitted and then measured as it reflects back from with the Earth's surface [@nasa2019].
:::

![Illustration of a passive sensor and an active sensor. Source: @nasa2019 Applied Passive Sciences Remote Sensing Training Program.](figures/activepassive_rs.png){#fig-sensors width="14.1cm" height="4.4cm"}

::: {style="text-align: justify"}
Components of the Earth's surface have different spectral signatures --- i.e., reflect, absorb, or transmit energy in different amounts and wavelengths [@campbell2011]. Remote sensing devices have several sensors that measure specific ranges of wavelengths in the electromagnetic spectrum; these are referred to as spectral bands (e.g. visible light, infrared, or ultraviolet radiation) [@seos2014; @nasa2019]. By capturing information from particular bands the spectral signatures of surfaces can be used to identify objects on the ground. @fig-spectralsig illustrates the differences between the spectral signatures of soil, green vegetation, and water across various wavelengths. The grey bands in the figure represent the specific spectral bands on the Landsat TM satellite [@seos2014]. The distinct reflectance properties of each material within these bands enable the differentiation of surface materials, making it possible to identify different land cover types. This information can be used directly for classification, or it can be combined into indices---such as the Normalized Difference Vegetation Index (NDVI)---to enhance the detection of specific features like vegetation health and coverage [@nasa2019; @campbell2011]. The $NDVI$ uses red light and near-infrared (NIR) ---given by $\frac{NIR - Red}{NIR + Red}$ --- to distinguish green vegetation. Higher $NDVI$ values indicate green vegetation as more red light is absorbed, whereas lower values correspond to non-vegetated areas where more red light is reflected.
:::

![Spectral signatures of soil, green vegetation, and water across different wavelengths, representing the portion of incident radiation that is reflected by each material as a function of wavelength. The grey bands indicate the spectral ranges (channels) of Landsat TM satellite. Bands 1-3 capture visible light (Blue, Green, Red), while Band 4 captures near-infrared (NIR), and Bands 5 and 7 cover parts of the intermediate infrared spectrum. These spectral bands allow for the differentiation of various surface materials based on their unique reflectance properties. Source: Siegmund and Menz (2005) as cited and modified by @seos2014.](figures/spectral_signatures_landsat.jpg){#fig-spectralsig width="12.5cm"}

## Machine Learning

::: {style="text-align: justify"}
Machine learning techniques such as neural networks, random forests, and support vector machines have long been applied for spatial data analysis and geographic modeling [@lavallin_machine_2021; @haddaway_evidence_2022]. Compared to using indices alone, machine learning techniques enhance the accuracy and efficiency of data analysis and interpretation processes making it possible to analyze large volumes of data effectively. Which is particularly useful for handling the high complexity and dimensionality of remote sencing data. In recent years, the application of machine learning techniques in remote sensing has surged, driven by the increasing availability of large datasets and advancements in computational power [@un-ggim2019; @zhang_review_2022]. These machine learning models can be grouped into four main types according to the aims of analyses: classification, clustering, regression, and dimension reduction. @tbl-models describes this grouping as well as giving examples. It is important to note that recent trends in machine learning and remote sensing analyses use hybrid or ensemble approaches using a combination of these groups [@un-ggim2019]. For a thorough review of these methods see @un-ggim2019.
:::

::: {#tbl-models style="text-align: justify"}
```{r}
read_excel("figures/summary_tables.xlsx", sheet = "ML_cat")|>
  kable(booktabs = TRUE, linesep = "") |>
  kable_styling(font_size = 10, 
                full_width = FALSE)|>
  column_spec(1, width = "2cm")|>
  column_spec(2, width = "14cm")|>
  
  footnote(c("Adapted from UN-GGIM:Europe (2019) and Haddaway et al.(2022)."), 
           threeparttable=TRUE)
```

Categories of machine learning methods grouped according to the analytic aim
:::

::: {style="text-align: justify"}
To verify these analyses performance metrics are used. For classification tasks, this involves creating a confusion matrix --- a cross-tabulation of class labels assigned to model predictions and reference data (ground truth). In a confusion matrix the correctly classified instances are on the diagonal, and the off-diagonal cells indicate which classes are confused (i.e., are incorrectly classified). In remote sensing applications, accuracy assessments are undertaken on a pixel, group of pixels (e.g. block), or an object level [@stehman2019].
:::

::: {#tbl-confusion style="text-align: justify"}
```{r}
confusion <- data.frame(
  Reference = c("Class 1","Class 2","Class 3","Class 4", "Total", "User's accuracy"), 
  #Predictions 
  c1 = c("$m_{11}$", "$m_{21}$", "$m_{31}$", "$m_{41}$", "$m_{.1}$", "$m_{11}/m_{.1}$"),
  c2 = c("$m_{12}$", "$m_{22}$", "$m_{32}$", "$m_{42}$", "$m_{.2}$", "$m_{22}/m_{.2}$"),
  c3 = c("$m_{13}$", "$m_{23}$", "$m_{33}$", "$m_{43}$", "$m_{.3}$", "$m_{33}/m_{.3}$"),
  c4 = c("$m_{14}$", "$m_{24}$", "$m_{33}$", "$m_{44}$", "$m_{.4}$", "$m_{44}/m_{.4}$"),
  total = c("$m_{1.}$", "$m_{2.}$", "$m_{3.}$", "$m_{4.}$", "$m$", ""),
  pa = c("$m_{11}/m_{1.}$", "$m_{22}/m_{2.}$",
         "$m_{33}/m_{3.}$", "$m_{44}/m_{4.}$", "", "")
)
confusion|>
  kable(booktabs = TRUE, linesep = "", escape = FALSE, 
        col.names = c("Reference","Class 1","Class 2","Class 3","Class 4", "Total", "Producer's accuracy")
        ) |>
  kable_styling(font_size = 10, 
                full_width = FALSE)|>
  add_header_above(c("", "Predictions" = 6), bold = TRUE) |>
  row_spec(0, bold = TRUE)|>
  column_spec(1, bold = TRUE)|>
  footnote(c("Confusion matrix for a classification with four classes, where the rows ($r$) represent the reference (observed) classification and the columns ($c$) represent the predicted classes. $m_{rc}$ is the number of instances predicted in reference class $r$ and predicted class $c$, and $m$ is the total number of instances (i.e., the number of pixels/objects classified)."), 
           threeparttable=TRUE, escape=FALSE)
```

Confusion matrix of four classes
:::

::: {style="text-align: justify"}
From this matrix, performance measures such as overall accuracy are derived [@fao2016; @un-ggim2019; @stehman2019]. Where the overall accuracy is the total number of successful classifications, $s$ over total number of instances, $m$.

$$
\text{Overall Accuracy (OA)} = \frac{\sum^q_{r=1}m_{rr}}{m}= \frac{s}{m}
$$ {#eq-OA}

If the unit of accuracy assessment is a pixel, then overall accuracy is the proportion of pixels classified correctly. Other metrics include the reliability (User's accuracy) and sensitivity (recall or Producer's accuracy). Reliability is the correct classifications for a particular class divided by the column total ($m_{.c}$) and sensitivity is correct classifications over the row total ($m_{r.}$). It is important to consider the purpose of the map when evaluating its accuracy, as overall accuracy may not reflect the accuracy of specific classes. Factors such as sample size, class stability, class proportions, and landscape variability influence the overall accuracy [see @un-ggim2019; @fao2016 ].

## Australia Land Cover Mapping

To illustrate how remote sensing data and machine leaning can be used to support ecological sustainable development, @owers2022 developed an approach to monitor and map land cover across Australia using techniques. Their study utilized Landsat sensor data archive through Digital Earth Australia to generate annual land cover maps from 1988 to 2020 at a 25-meter resolution. The study used random forest and artificial neural networks to classify individual pixels according to the FAO's Land Cover Classification System (LCCS) framework.
:::

![Land cover mapping created by @owers2022 using Landstat data to make continent-wide classifications using the LCCS frame work which differentiates six (classes) land cover types: cultivated terrestrial vegetation (CTV), natural terrestrial vegetation (NTV), natural aquatic vegetation (NAV), artificial surfaces (AS), bare surfaces (BS), and water bodies (W).](figures/owers_etal22.jpg){#fig-owers2022 alt="Spectral signatures of soil, green vegetation and water a cross wavelengths, i.e. the portion of the incident radiation that is reflected by the material as a function of wavelength. The grey bands are the spectral ranges (channels) of NASA Landsat TM satalite. Bands 1-3 collect visble light… TO ADD: reference." width="10.2cm"}

::: {style="text-align: justify"}
To produce such maps using a topographical field survey is impractical, given Australia's size ($7,688,287 \text{ km}^2$). While field surveys are the most accurate method of generating training sample data, they are labor-intensive, time-consuming, and expensive [@zhang2022]. A topographical survey of just 20 hectares ($0.2 \text{ km}^2$) takes a team of four people approximately five days to complete, even though the resulting topographical map would have a high resolution of 0.5 meters (L.A. Mbila, personal communication, January 26, 2024). In @owers2022, experts visually inspected the satellite imagery to validate the training and test data. While this is a less labor-intensive, costly and time-consuming than field surveys it still requires significant effort and expertise.

In contrast to the challenges associated with field surveys, remote sensing provides an efficient method for the continuous monitoring of large areas that would otherwise be inaccessible [@zhang2022; @owers2022]. Thefore, [the potential applications are numerous. Examples include monitoring of land use and degradation, forestry, biodiversity, agriculture, disaster prediction, water resources, public health, urban planning, poverty, and the management and preservation of world heritage sites]{style="text-align: justify"} [@anshuka_drought_2019; @campbell2011; @ekmen2024; @hall2023; @lavallin_machine_2021; @maso2023].
:::

## Previous Reviews

::: {style="text-align: justify"}
Numerous studies have previously examined the application of remote sensing for SDG monitoring. However, existing reviews are typically either limited to specific contexts, such as the use of satellite data for poverty estimation [@hall2023] or focus on descriptive results [see @yin_review_2023]. The existing reviews either apply methodology that aligns more closely with Synthesis Without Meta-Analysis [@campbell2020] ---for example, @thapa_deep_2023 and @ekmen2024 --- or apply unweighted meta-analysis techniques, such as @khatami2016 and @hall2023 ). In unweighted meta-analysis all studies are treated equally regardless of their sample size, quality, or variance [@hall2018]. However, it is more common in traditional applications of meta-analysis, to use the sample sizes when aggregating individual studies [@hall2018]. However, to my knowledge, no examples of a weighted meta-analysis applied to predictive performance in remote sensing data have been conduced, highlighting a gap that this study aims to address.
:::
