---
execute:
  echo: false
  message: false
  warning: false
  enabled: true
---

# Results

```{r set_up}
library(tidyverse)
library(readxl)
library(metafor)
library(rstatix) # normally test
# plot and table packages 
library(patchwork)
library(jtools)
theme_set(theme_apa()) 
# common theme specification  
common_theme <- theme(
  plot.title = element_text(size = 10),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8),
  axis.text = element_text(size = 8)
)
library(geomtextpath)
library(maps)
library(gtsummary)
library(kableExtra)
library(gt)
```

```{r data_import_reaganging}
## Features extracted from the relevant papers
extracted_features <- read_excel("../data/extracted_features.xlsx", sheet = "Include",  na = "NA")
extracted_features <- extracted_features|>
  mutate(DOI = tolower(DOI)) 

## total set of papers after duplicates removed
citations<- read.csv("../appendix/data/citations.csv")
citations <- citations|>
  mutate(DOI = tolower(DOI))
# citation number data
open_alex <- read.csv("../data/OpenAlex.csv")
open_alex <- open_alex[, c("DOI", "globalCitationsCount")]
open_alex <- open_alex|>
  mutate(DOI = tolower(DOI)) 

## murging first author and year for study label
citations$AuthorYear <- paste(sapply(strsplit(citations$Author, ", "), 
                                     `[`, 1), "et al.", 
                              citations$Publication.Year)

## Joining the reference data with the extracted feature data
citations_needed<- subset(citations, DOI %in% extracted_features$DOI, 
                          select = c(DOI, AuthorYear, Publication.Title, Publication.Year))

dat <- inner_join(citations_needed, extracted_features, by = "DOI")


### columns that are factors 
cols_to_factor<- c("AuthorYear", "SDG_theme", "ancillary", "classification_type", 
                   "indices", "RS_spatital_resolution_m", "model_group", "Confusion_matrix", 
                   "RS_device_group", "RS_device_type")


my_data <- subset(dat, !(is.na(total)), # omitting studies without totals 
               # needed columns 
               select = c(cols_to_factor, "DOI","Publication.Year","number_classes", 
                                 "fraction_majority_class",
                                 "total", "OA_reported"))

# adding global citation number 
my_data <- left_join(my_data,open_alex, by = "DOI")

## Regrouping the extracted features: at least 5 for each group  
### ML models
my_data$model_group <- ifelse((my_data$model_group == "Neural Networks" | 
                                 my_data$model_group == "Tree-Based Models"), 
                          my_data$model_group,"Other") # other is SVM and KNN 
my_data$model_group <- factor(my_data$model_group, 
                             levels = c("Neural Networks", "Tree-Based Models",
                                        "Other"))

## Remote Sensing Spatial resolution
my_data$RS_spatital_res_grouped <- ifelse(my_data$RS_spatital_resolution_m<1, ">1", 
                                      ifelse(my_data$RS_spatital_resolution_m>10 &
                                               my_data$RS_spatital_resolution_m<30,
                                             "15-25", my_data$RS_spatital_resolution_m))
## reorder RS_device group
my_data$RS_device_group <- factor(my_data$RS_device_group, 
                             levels = c("Sentinel", "Landsat", "Other", "Not Reported"))

my_data[cols_to_factor] <- lapply(my_data[cols_to_factor], as.factor)


# making by paper estimate id 
my_data <- my_data |> group_by(AuthorYear) |> mutate(esid = row_number())


my_data <-tibble::rowid_to_column(my_data, "effectsizeID")

my_data$studyID <- as.numeric(factor(my_data$AuthorYear))

### analysis of proportions done as event/sample size so making an event variable
my_data$event <- my_data$total*my_data$OA_reported

### saving data for dashboard 
#write.csv(my_data, "analysis_df.csv")

# weights and transformation 
# Function to calculate various effect sizes and to estimate the variance 
## for the traditional meta analysis

ies.da  <- escalc(xi = event , ni = total , data = my_data ,
               measure = "PFT",  # FT double  arcsine  transformation
               slab=paste(AuthorYear, " Estimate ", esid)
               ) 
#the ies: individual effect size, new variables:
## yi: FTT effect sizes
## vi: calculated variances 


```

::: {style="text-align: justify"}
## Descriptive Statistics

A total of 20 studies with 86 effect sizes were included in this analysis, with each primary study reported between one and 27 results. These studies span 18 countries, @fig-map shows a map of distribution and number of results per study.
:::

::: {#fig-map}
```{r}
library(sf)
library(rnaturalearth)
library(ggrepel)
options(ggrepel.max.overlaps = Inf)
sf_world <- ne_countries(returnclass = "sf")

# Get the world map data with country names, latitude, and longitude
world_map <- map_data("world") 
  

study_map <- world_map |>
  group_by(region) |>
  summarize(lat = mean(lat), long = mean(long), .groups = 'drop')|>
  right_join(dat, join_by("region"=="location"))
 


ggplot(sf_world) + 
  geom_sf(fill = "#f6e8c3", alpha = 0.4)+
  coord_sf(ylim = c(-55,80)) +
  geom_jitter(data = study_map, aes(x = long, y = lat), 
              color = "#001158",
              alpha = 0.8,
              size = 1, 
              width = 4, 
              height = 4) +
  
  # geom_label_repel(data = (study_map|> 
  #                    group_by(region)|>
  #                    summarize(AuthorYear = first(AuthorYear), 
  #                              lat = mean(lat), 
  #                              long = mean(long))), 
  #                  aes(x = long, y = lat, label = AuthorYear), 
  #                  box.padding = 0.5)
  theme_void() 
  
```

map of study location
:::

::: {style="text-align: justify"}
@tbl-sum summarises the overall accuracy (effect size of interest), study sample size and the collected study features, including the study features such as sample size, overall accuracy, types of machine learning models used and SDG goal targeted. For the meta-analysis the range of the sample size `r paste("(", min(ies.da$total), " - ", max(ies.da$total), sep= "", ")" )` and overall accuracy `r paste("(", min(ies.da$OA_reported), " - ", max(ies.da$OA_reported), sep= "", ")" )` are of importance. Most studies used Neural Networks (48%), followed by Tree-Based Models (45%), and a small portion used other types of models (7%). Regarding SDGs, 44% of the studies aimed at SDG 11 (Sustainable Cities and Communities), 43% targeted SDG 15 (Life on Land), and 13% focused on SDG 2 (Zero Hunger).
:::

::: {#tbl-sum}
```{r sum_table}
# important study feature labels 
feature_labels <- list(OA_reported = "Overall Accuracy", total = "Sample Size", 
                       model_group = "Machine Learning Model Group", 
                       indices = "Indices Used", 
                       classification_type = "Classification Type",  
                       ancillary = "Ancillary Data", SDG_theme = "SDG Goal", 
                       fraction_majority_class = "Majority Class Propotion", 
                       RS_device_group = "RS Device Group", 
                       RS_spatital_res_grouped = "RS Spatial resolution (m)", 
                       globalCitationsCount = "Citation Number" )
# summary table 

summary_table <- ies.da |>
  select(OA_reported, total,fraction_majority_class, 
         model_group, SDG_theme, classification_type, 
         indices, ancillary, RS_device_group, RS_spatital_res_grouped, 
         globalCitationsCount
         )|>
  tbl_summary(
  #  # by = model_group,
    statistic = list(
      all_continuous() ~ "{mean} ({min} - {max})",
      all_categorical() ~ "{n} ({p}%)"
    ),
    label = feature_labels,
    digits = all_continuous() ~ 2,
    missing = "no"
  ) |>
 # add_overall() |>
  modify_header(label = "Feature") |>
  modify_header(all_stat_cols() ~ "{level}")|>
  as_kable(booktabs = TRUE, linesep = "")


summary_table|>
  kable_styling(font_size = 10, 
                full_width = FALSE)|>
  column_spec(1, width = "7cm")|>
  column_spec(2, width = "6cm")|>
  row_spec(c(0, 4, 8, 12, 16, 19, 22, 27),  bold = TRUE)|>
  add_indent(c(5:7,9:11,13:15, 17:18, 20:21, 23:26, 28:32))|>
  footnote(threeparttable=T)
  
  

```

Note*:* RS: remote sensing, SDG: Sustainable development goals

Summary Statistics of Study Features and Performance Metrics
:::

::: {#fig-oa}
```{r fig_oa}
#| fig-width: 8
#| fig-height: 4.5
ggplot(ies.da ,aes(x = reorder(AuthorYear, OA_reported), y = OA_reported, 
               colour = SDG_theme
               ))+
    geom_point(alpha = 0.2, size = 1)+
    stat_summary(geom = "point", fun = "mean", size = 2, shape = 17)+
      labs(x = NULL, 
       y = "Observed Overall Accuracy", 
       title = "Range of reported Overall Accuracy") +
    scale_colour_brewer(palette = "Set1")+
    guides(colour=guide_legend(title= NULL, nrow=3, byrow=TRUE))+
    coord_flip() +
    theme(# grid lines 
        panel.grid.major.y = element_line(linewidth = 0.1, colour = "grey80"),
       # legend specs
        legend.position = "bottom",
        legend.key.size = unit(0, 'lines'), 
        legend.key.spacing.x = unit(-2.5, "lines"),
        legend.text= element_text(size=8, margin = margin(r = 15, unit = "cm")),
       plot.title = element_text(size=11),
        axis.title.x = element_text(size=9),
        axis.title.y = element_text(size=9), 
        axis.text = element_text(size=9)
       )
```

Reported overall accuracy from each of the included studies. Individual results are depicted with a circle point and the mean overall accuracy with triangles. The studies are colour-coded according to the SDG goal that the study aims fell under.
:::

```{r normal_test}
shapiro_test_result <- round(shapiro_test(ies.da$yi)[, 2:3], 2)
```

As @fig-oa and @tbl-sum shows that overall accuracies are not centered around 0.5. Therefore, a transformation is required, @fig-transfrom shows the distribution of observed overall accuracy as well as the logit and FT transformation values. FT visually performs better than the Logit transformation. However the Shapiro-Wilk Normality Test shows that the distribution of the FT transformed OA still departed significantly from normality ($W =$ `r shapiro_test_result$statistic`, p-value \< 0.01)[^results-1].

[^results-1]: The limitations of the FT as well as the departure from the normal distribution are discussed in the next chapter Q: However continued anyway? how should I justify that?

::: {#fig-transfrom}
```{r fig_transform}
#| fig-width: 8
#| fig-height: 3

# n_pdf<- stat_function(fun = dnorm, args = list(mean = mean(ies.da$OA_reported), 
#                                                sd = sd(ies.da$OA_reported)))

raw_propotions <- ggplot(ies.da, aes(x = OA_reported))+
  geom_histogram(bins = 30, fill = "#001158", alpha = .8) +
  labs(x = "Observed Overall Accuracy", y = "Count") +
  common_theme
  

logit_transformation <- ggplot(ies.da, aes(x = log(OA_reported/(1-OA_reported))))+
  geom_histogram(bins = 30, fill = "#001158", alpha = .8)+
  labs(x = "Logit-transformed Overall Accuracy", y = NULL) +
  common_theme

arcsin_transformation<- ggplot(ies.da, aes(x = yi))+
  geom_histogram(bins = 30, fill = "#001158", alpha = .8)+
  labs(x = "FT-transformed Overall Accuracy", y = NULL)+
  common_theme

raw_propotions + logit_transformation+ arcsin_transformation +
  plot_annotation(title = "Comparison of Density Plots of Observed and Transformed Overall Accuracy", 
                  theme = theme(plot.title = element_text(size = 10)))



```

Distribution of the observed overall accuracy and transformed by logit and FT transformation.
:::

\blandscape

::: {#fig-features}
```{r fig_study_features}
#| fig-width: 16
#| fig-height: 10

### About the Model 

sample_size<- ggplot(ies.da, aes(x = total, y= yi
                ))+
  geom_point(size = 3, alpha = 0.8, colour = "#001158")+
  labs(title = "Sample Size", 
        y = "Transformed Overall Accuracy",
       x = "Number of instances")


number_class<- ggplot(ies.da, aes(x = number_classes, y= yi
                ))+
  geom_point(size = 3, alpha = 0.8, colour = "#001158")+
  labs(title = "Number of Classes", 
        y = "Transformed Overall Accuracy",
       x = "Number of Class")



fraction_majority_class<- ggplot(ies.da, aes(x = fraction_majority_class, y= yi
                ))+
  geom_point(size = 3, alpha = 0.8, colour = "#001158")+
  labs(title = "Majority Class Proportion", 
        y = "Transformed Overall Accuracy",
       x = "Proportion")

cits<- ggplot(ies.da, aes(x = globalCitationsCount , y= yi
                ))+
  geom_point(size = 3, alpha = 0.8, colour = "#001158")+
  labs(title = "Number of times cited", 
        y = "Transformed Overall Accuracy",
       x = "Citations")

#num_features <- sample_size + number_class + fraction_majority_class + cits + 
 # plot_layout(axis_titles = "collect", ncol = 1) 

# model used 
modelgroup_box <- ggplot(ies.da, aes(y= yi, x = model_group)) + 
  geom_boxplot()+
  #stat_summary(fun.data = give.n, geom = "text", fun.y = median)+
  geom_jitter(colour = "#001158", alpha = 0.7)+
  labs(title =  "Machine Learning type", 
        y = "Transformed Overall Accuracy",
       x = NULL)+
  coord_flip()
  
# classfication type 
classtype_box <- ggplot(ies.da, aes(y= yi, x = classification_type)) + 
  geom_boxplot()+
  #stat_summary(fun.data = give.n, geom = "text", fun.y = median)+
  geom_jitter(colour = "#001158", alpha = 0.7)+
  labs(title = "Type of clasification", 
       y = "Transformed Overall Accuracy",
       x = NULL)+
  coord_flip()



# use of indexes 
indexes_box <- ggplot(ies.da, aes(y= yi, x = factor(indices, 
                                                          labels = c("Utilised","Unimplemented"))))+
  geom_boxplot()+
  #stat_summary(fun.data = give.n, geom = "text", fun.y = median)+
  geom_jitter(colour = "#001158", alpha = 0.7)+
  labs(title = "Use of RS Indices", 
        y = "Transformed Overall Accuracy",
       x = NULL)+
  coord_flip()

# use of indexes 
ancillary_box <- ggplot(ies.da, aes(y= yi, x = factor(ancillary, 
                                                          labels = c("Utilised","Unimplemented"))))+
  geom_boxplot()+
  #stat_summary(fun.data = give.n, geom = "text", fun.y = median)+
  geom_jitter(colour = "#001158", alpha = 0.7)+
  labs(title = "Use of RS Indices", 
        y = "Transformed Overall Accuracy",
       x = NULL)+
  coord_flip()
       

### About the data 
# type of data colection  
RStype_box <- ggplot(ies.da, aes(y= yi,x = RS_device_type))+
  geom_boxplot()+
  #stat_summary(fun.data = give.n, geom = "text", fun.y = median)+
  geom_jitter(colour = "#001158", alpha = 0.7)+
  labs(title = "Type of RS", 
        y = "Transformed Overall Accuracy",
       x = NULL)+
  coord_flip()

# type of RS group  
RSgroup_box <- ggplot(ies.da, aes(y= yi,x = RS_device_group))+
  geom_boxplot()+
  #stat_summary(fun.data = give.n, geom = "text", fun.y = median)+
  geom_jitter(colour = "#001158", alpha = 0.7)+
  labs(title = "RS device", 
        y = "Transformed Overall Accuracy",
       x = NULL)+
  coord_flip()


RS_resolution_box<- ggplot(ies.da, aes(y= yi,x = RS_spatital_res_grouped))+
   geom_boxplot()+
  #stat_summary(fun.data = give.n, geom = "text", fun.y = median)+
  geom_jitter(colour = "#001158", alpha = 0.7)+
  labs(title = "RS Spatial Resolution", 
         y = "Transformed Overall Accuracy",
       x = NULL)+
  coord_flip()

SDG_theme_box<- ggplot(ies.da, aes(y= yi, x = SDG_theme))+
  geom_boxplot()+
  #stat_summary(fun.data = give.n, geom = "text", fun.y = median)+
  geom_jitter(colour = "#001158", alpha = 0.7)+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 10))+
  labs(title = "SDG Goal", 
        y = "Transformed Overall Accuracy",
       x = NULL)+
  coord_flip()

number_class_box<- ggplot(ies.da, aes(x = as.factor(number_classes), y= yi))+
  geom_boxplot()+
  #stat_summary(fun.data = give.n, geom = "text", fun.y = median)+
  geom_jitter(colour = "#001158", alpha = 0.7)+
  labs(title = "Number of categories to group", 
        y = "Transformed Overall Accuracy",
       x = NULL)+
  coord_flip()




modelgroup_box+
  indexes_box+
  SDG_theme_box+
  classtype_box+
  RStype_box+
  RSgroup_box+
  RS_resolution_box+  ancillary_box +
  sample_size + number_class + fraction_majority_class + cits+ 
  plot_layout(axis_titles = "collect", ncol = 4)

```

Distribution of study features used to try and explain the variation
:::

\elandscape

```{r, eval=FALSE}
# most variables are between study 
data <- ies.da |>
  select(SDG_theme, AuthorYear, globalCitationsCount, classification_type,
         indices,  model_group, ancillary, RS_spatital_res_grouped,
         number_classes,  fraction_majority_class)|>
  group_by(SDG_theme, AuthorYear, classification_type, model_group)
data 

mpg_list <- split(ies.da$model_group, ies.da$AuthorYear )

inline_plot <- data.frame(data = unique(ies.da$AuthorYear), mpg_box = "")
inline_plot %>%
  kbl(booktabs = TRUE) %>%
  kable_paper(full_width = FALSE) %>%
  column_spec(2, image = spec_hist(mpg_list))


ggplot(ies.da, aes(x = RS_spatital_res_grouped, y = yi))+
  geom_boxplot(aes(fill = AuthorYear), show.legend = FALSE)+
   geom_jitter(aes(colour = AuthorYear), alpha = 0.7, show.legend = FALSE)+
  facet_grid(number_classes ~model_group)

```

## Meta-analysis

```{r lvl2_mod}
# METHOD: Weighted: level 2 model
## pes: pooled effect size
pes.da  <- rma(yi,
                  vi, 
                  data = ies.da,
                  method = "REML", 
                  test="t"  
                  )


```

```{r lvl3_mod}
# METHOD: Weighted: Nested level 3model
pes.da.lvl3 <- rma.mv(yi,
                      vi,
                      data = ies.da ,
                      tdist = TRUE,
        # adding random effects at the study level and effect size 
                      random = ~ 1 | AuthorYear / esid,
                      method = "REML",
                      # recommendations from the function documentation:
                      test="t",  
                      dfs="contain"
                    )

#summary(pes.da.lvl3)

```

```{r lvl3_mod_backtras}
pes <- predict(pes.da.lvl3, transf = transf.ipft.hm, targ = list(ni=1/(pes.da.lvl3$se)^2))

pes.da.lvl3_I_squared <- dmetar::var.comp(pes.da.lvl3)

pes.da.lvl3_CI <- confint(pes.da.lvl3)

results <- data.frame(
  theta = pes$pred, 
  ci_l = pes$ci.lb,
  ci_u = pes$ci.ub,
  sigma2.1 = pes.da.lvl3$sigma2[1],
  sigma2.2 = pes.da.lvl3$sigma2[2],
  df = pes.da.lvl3$QEdf, 
  Q = pes.da.lvl3$QE, 
  p = pes.da.lvl3$QEp,
  I_L2 = pes.da.lvl3_I_squared$results$I2[2],  
  I_L3 = pes.da.lvl3_I_squared$results$I2[3] 
)
row.names(results) <-"RE_lvl3"

# results$theta
# # same as
# v_bar <- (pes.da.lvl3$se)^2
# t_bar <- pes.da.lvl3$b[1]
# (1/2 * (1 - sign(cos(2*t_bar)) *
#           sqrt(1 - (sin(2*t_bar)+(sin(2*t_bar)-1/sin(2*t_bar))/(1/v_bar))^2)))
# 

```

```{r lvl3_mod_test}
# METHOD 1.2:  
# multivariate parameterization model 
# this is a multilvl rather than a nested model (if since the samples could be different data or countries this might be the better approch?)
pes.da.lvl3.mv <- rma.mv(yi,
                      vi,
                      data = ies.da,
                      random = ~ esid|AuthorYear,
                      method = "REML",
                      # recomendations from the function detains +Assink et.al:
                      tdist = TRUE,
                      test="t",  
                      dfs="contain"
                    )

# should be exactly the same
#logLik(pes.da.lvl3)
#logLik(pes.da.lvl3.mv)


# future research look into the variance-covariance matrix structures

```

```{r lvl3_mod_manual, eval=FALSE}
# model contains two variance components for the between-cluster (AuthorYear ) heterogeneity and the within-cluster (AuthorYear/ID) heterogeneity.
W <- diag(1/pes.da.lvl3$vi)

X <- model.matrix(pes.da.lvl3)
P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
100 * sum(pes.da.lvl3$sigma2) / (sum(pes.da.lvl3$sigma2) + (pes.da.lvl3$k-pes.da.lvl3$p)/sum(diag(P)))
100 * pes.da.lvl3$sigma2 / (sum(pes.da.lvl3$sigma2) + (pes.da.lvl3$k-pes.da.lvl3$p)/sum(diag(P)))
```

```{r fig_forrest, eval=FALSE}
#| fig-width: 10
#| fig-height: 8 

# To visualize results aggregate model 

### assume that the effect sizes within studies are correlated with 
V <- vcalc(vi, cluster=AuthorYear, obs=esid, data=ies.da, rho=pes.da.lvl3.mv$rho)
 
### fit multilevel model using this approximate V matrix
pes_agg.da.lvl3 <- rma.mv(yi, V, random = ~ 1 | AuthorYear/esid, data=ies.da)

agg <- aggregate(ies.da, cluster=AuthorYear, V=vcov(pes_agg.da.lvl3, type="obs"), addk=TRUE)

agg.pes.da <- rma(yi, vi, method="EE", data=agg, digits=3)
agg.pes <- predict(agg.pes.da , transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3$se)^2))

# forest plot:

model_summary <- bquote(paste("RE (3-Level) Model ",
                 "(Q = ",
                 .(fmtx(results$Q[1], digits=0)),
                 ", df = ", 
                 .(fmtx(results$df[1], digits=0)),
                 ", p <.001; ", 
                 I^2, " = 100% ",
                       sep = ""))

png(file='forestplot_weighted.png', width = 1000, height = 800)
forest(agg.pes.da,addpred=TRUE, 
       xlim=c(-2,2.5),
       alim =c(0, 1),
       transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3$se)^2), 
       header=TRUE,
       slab=AuthorYear,
       order="obs",
       cex=1.5, 
       refline=NA,
       digits = 3,
       ilab=ki, 
       mlab= model_summary,
       ilab.xpos= -.3,
       ilab.pos = 2, 
       #showweights = TRUE, 
       shade="zebra"
       )

text(-.5, agg.pes.da$k+2, "OA Count", cex=1.5, font=2)
text(2.1, agg.pes.da$k+3, "Weighted", cex=1.5, font=2)


```

```{r unweighted_mod}
########################
# METHOD 2: UnWeighted
# the mean of the yi should be the estimate of effect 
pes.da.lvl3_unweighted  <- rma.mv(yi = yi, 
                                  V = 1,
                                  random = ~ 1 | AuthorYear / esid, # Nested random effects 
                                  data = ies.da, 
                                  method = "REML", 
                                  tdist = TRUE,
                                  test="t", 
                                  dfs="contain")
#summary(pes.da.lvl3_unweighted)
#mean(ies.da$yi)

pes_u <- predict(pes.da.lvl3_unweighted, transf = transf.ipft.hm, targ = list(ni=1/(pes.da.lvl3_unweighted$se)^2))

theta_u <- mean(ies.da$OA_reported)
t_critical <- qt(0.975, 19)
se <- 1 / sqrt(86)

results["Unwei_rma.mv"] <- NA
results["Unwei_rma.mv", 1:3 ]<- c(pes_u$pred, pes_u$ci.lb, pes_u$ci.ub)

results<- results|> mutate_if(is.numeric, round, digits=3)

```

```{r fig_forrest_unweighted, eval=FALSE}


# To visualize results aggregate model 

### assume that the effect sizes within studies are correlated with 
V <- vcalc(vi, cluster=AuthorYear, obs=esid, data=ies.da, rho=0.6)
 
### fit multilevel model using this approximate V matrix
pes_agg.unweighted <- rma.mv(yi, V, random = ~ 1 | AuthorYear/esid, data=ies.da)

agg <- aggregate(ies.da, cluster=AuthorYear, V=vcov(pes_agg.unweighted, type="obs"), addk=TRUE)

agg.pes.da <- rma(yi, vi, method="EE", data=agg, digits=3)
agg.pes <- predict(agg.pes.da , transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3_unweighted$se)^2))

# forest plot:
png(file='forestplot_unweighted.png', width = 1000, height = 800)
forest(agg.pes.da,addpred=TRUE, 
       xlim=c(-2,2.5),
       alim =c(0, 1),
       transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3_unweighted$se)^2), 
       header=TRUE,
       slab=AuthorYear,
       order="obs",
       refline=NA,
       col =  "#001158",
       cex=1.5, 
       digits = 3,
       ilab=ki, 
       mlab= "Unweighted Model",
       ilab.xpos= -.3,
       ilab.pos = 2, 
       #showweights = TRUE, 
       shade="zebra"
       )

text(-.5, agg.pes.da$k+2, "OA Count", cex=1.5, font=2)
text(2.1, agg.pes.da$k+3, "Unweighted", cex=1.5, font=2)


```

::: {style="text-align: justify"}
The forest plot below (@fig-forest) compares the overall accuracy effect size across studies using both weighted and unweighted models. Each study is given with the number of estimates per study, and study average effect size ($\kappa_j$), with 95% confidence intervals (CI), both for the weighted and unweighted model. Of the 20 primary studies included, six reported only one effect. Based on the unweighted model, the average accuracy of machine learning methods applied to remote sensing data is 0.90 (95% CI\[0.85; 0.94\]). While the three-level meta-analytic model produced an average accuracy of 0.89 (95% CI\[0.85; 0.93\]). This implies, that on average, the machine learning methods correctly classify around 90% of the time when applied to remote sensing data.
:::

::: {style="text-align: justify"}
![Forest plot for both the weighted and unweighted model. OA Count is number of overall accuracy estimates per study, the corresponding average effect size($\kappa_j$) and confidence interval per study for both models is given on the right. The pooled summary effect size based on the three-level RE meta-analytic and unweighted model are given on the bottom. Note, the error bars here are only the ones corresponding to the weighted model but at this scale there is no discernible difference.](figures/forestplot_comb.png){#fig-forest}
:::

::: {style="text-align: justify"}
On the bottom left of @fig-forest, the heterogeneity metrics Cochran's Q indicate significant heterogeneity. The percentage of the variance attribution are $I^2$ Level 3 = `r results$I_L3[1]`% which is the total variation can be attributed to between-study, and $I^2$ at Level 2 = `r results$I_L3[1]`% which is within-study heterogeneity, with negligible fixed effect variance (variance do to sampling error). The $I^2$ value of 100% indicates that all the observed variability in effect sizes across studies is due to heterogeneity rather than sampling error, suggesting substantial differences between the studies and a high degree of inconsistency in their results. @tbl-results shows these results in more detail.

Some of the variance in the model accuracy can be explained by the collected study features. @fig-best, illustrates the predictor importance after evaluating all possible combinations of predictors to identify which combination provides the best fit and which predictors are most influential. Higher importance values indicate more consistent inclusion in high-weight models. The majority class proportion is the most important predictor, followed by the inclusion of ancillary data and the use of indices. Less influential predictors include sample size, publication year, and the number of classes in the study. Meanwhile, factors such as classification type, SDG goal, machine learning group, spatial resolution, and citation count have minimal importance in the overall model performance (i.e. where not included in the models top performing models according to AIC).
:::

```{r eval=FALSE}
set.seed(123)
library(dmetar)
ies.da$se <- sqrt(ies.da$vi)
x<-multimodel.inference(TE = "yi", 
                     seTE = "se",
                     data = ies.da,
                     predictors = c("fraction_majority_class", "number_classes", 
                              "globalCitationsCount", "classification_type", 
                              "Publication.Year", "total",
                              "model_group", "SDG_theme", "ancillary",
                              "RS_spatital_res_grouped", "indices"),
                     interaction = FALSE)

x$predictor.importance.plot+
  scale_x_discrete(labels = c("fraction_majority_class"= "Majority Class Proportion", 
                              "ancillary"= "Ancillary", 
                              "indices" = "Indices", 
                              "total" ="Sample Size", 
                              "Publication.Year" = "Publication Year", 
                              "number_classes" = "Number of Classess", 
                              "classification_type" = "Classfication type", 
                              "SDG_theme" = "SDG goal", 
                              "model_group" = "Machine Learning Group",
                              "RS_spatital_res_grouped" = "RS Spatial Resolution", 
                              "globalCitationsCount"= "Citation Count"))+
  
  labs(x = NULL, 
       title = "Model-averaged predictor importance plot" )+
  
  theme_apa()+
  common_theme
#ggsave("figures/fig-best_mod.png", height =4, width = 7)

```

::: {style="text-align: justify"}
![Model-averaged predictor importance plot. The averaged importance of each predictor across all models is displayed. The majority class proportion is the most important predictor, followed by the inclusion of ancillary data and the use of indices.](figures/fig-best_mod.png){#fig-best}
:::

::: {style="text-align: justify"}
@tbl-bestmod compares the intercept-only model (a model without covariates) and the full model (which includes all available features), along with the two best-performing models based on multi-model inference. The best-performing model was *Intercept + Majority class proportion + Ancillary+ Indices* (Model 22); followed closely by *Intercept + Majority class proportion* (Model 6). The table presents the degrees of freedom (df), AIC, BIC, and p-values from ANOVA tests comparing the models. All models that include covariates significantly improve upon the intercept-only model (p \< 0.001). Model 6 has the lowest AIC and BIC, indicating it is the best model overall. However, when comparing Model 6 to Model 22, the difference is not significant at the 0.05 level, meaning the addition of "ancillary data" does not offer substantial improvement (p = 0.045). Similarly, the full model does not provide further improvements over the simpler models (p = 0.540).
:::

::: {#tbl-bestmod}
```{r best_mod}

# Adding predictors 
#best model 
meta_reg_22 <- rma.mv(yi, vi,
  data = ies.da ,
  random = ~ 1 | AuthorYear / esid,
  tdist = TRUE,
  method = "REML",
  test="t", 
  dfs="contain", 
  mods = ~ ancillary+fraction_majority_class+ indices
)
# second best model 
meta_reg_6 <- rma.mv(yi, vi,
  data = ies.da ,
  random = ~ 1 | AuthorYear / esid,
  tdist = TRUE,
  method = "REML",
  test="t", 
  dfs="contain", 
  mods = ~ fraction_majority_class
)

# full model 
meta_reg_full <- rma.mv(yi, vi,
  data = ies.da ,
  random = ~ 1 | AuthorYear / esid,
  tdist = TRUE,
  method = "REML",
  test="t", 
  dfs="contain", 
  mods = ~ ancillary+ fraction_majority_class+ indices +
    classification_type  + model_group + Publication.Year +
    SDG_theme + ancillary + RS_spatital_res_grouped + indices
)


# compare models 

model_comp <- data.frame(Model = c("Intercept only", 
                                   "Model 6", 
                                  "Model 22",
                                  "Full model"), 
                         df = AIC(pes.da.lvl3, meta_reg_6, 
                                  meta_reg_22, meta_reg_full)[1], 
                         AIC = AIC(pes.da.lvl3, meta_reg_6, 
                                  meta_reg_22, meta_reg_full)[2], 
                         BIC = BIC(pes.da.lvl3, meta_reg_6, 
                                   meta_reg_22, meta_reg_full)[2])

# Run ANOVA (Likelihood Ratio Test) and extract p-values
model_comp$"Intercept only" <-  c(NA, 
                 anova(pes.da.lvl3, meta_reg_6, refit=TRUE)$pval, 
                 anova(pes.da.lvl3, meta_reg_22, refit=TRUE)$pval, 
                 anova(pes.da.lvl3, meta_reg_full, refit=TRUE)$pval)

model_comp$"Model 6" <-  c(NA, NA,
                 anova(meta_reg_6, meta_reg_22, refit=TRUE)$pval, 
                 anova(meta_reg_6, meta_reg_full, refit=TRUE)$pval)

rownames(model_comp) <- NULL

model_comp |>
  kable(booktabs = TRUE, linesep = "",escape=F, 
        digits = c(0, 0, 2, 2, 3, 3) 
  ) |>
  
  kable_styling(font_size = 10, 
                full_width = FALSE)|>
  column_spec(1, width = "4cm")|>
  column_spec(2:4, width = "1.2cm")|>
  column_spec(5, width = "2.5cm")|>
  column_spec(6, width = "3.6cm")|>
  add_header_above(c("", "", "", "", "Comparisions" = 2))|>
  
   
  footnote(threeparttable=T)



```

Note*:* Comparison of model performance based on degrees of freedom (df), Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and p-values from likelihood ratio tests (ANOVA). Model 6: *Intercept + Majority class proportion*; Model 22: *Intercept + Majority class proportion + Ancillary + Indices*; The Full Model: *Intercept + Majority class proportion + Ancillary + Sample Size + Publication Year + Number of Classes + Classification type + SDG goal + Machine Learning Group + RS Spatial Resolution + Citation Count*. Lower AIC and BIC values indicate better model fit, and p-values less than 0.05 suggest significant improvements over simpler models.

Model Comparison
:::

::: {style="text-align: justify"}
@tbl-coef shows the estimated coefficients of Model 22, both in the FT transformed scale (b) and on the natural scale (b back-transformed). This shows that the proportion of majority class has the largest positive impact on the model's outcome (b = 0.39, p \< .001), while the inclusion of ancillary data has a small but significant negative effect (b = -0.11, p = 0.029). The use of indices has a minimal and non-significant effect (b = 0.06, p = 0.131). The intercept is also significant, indicating a strong baseline effect (b = 0.99, p \< .001).
:::

```{r}
# getting all the results in one table 
backtras.function<- function(t_bar, se){
  v_bar <- (se)^2
  (1/2 * (1 - sign(cos(2*t_bar)) *
           sqrt(1 - (sin(2*t_bar)+(sin(2*t_bar)-1/sin(2*t_bar))/(1/v_bar))^2)))
}
# Cochran's Q paste function 
q.function <- function(Q, df, p.val){
  if(p.val <= .001) p = "p <.001" else p = paste0("p =",round(p.val, 3))
  result <- paste0("Q(", df, ") = ", round(Q, 2),", ",  p) 
  return(result)
} 


# sigmas + confidence intervals for both models
conf_intervals_lvl3 <- as.data.frame(confint(pes.da.lvl3))
conf_intervals_meta_reg <- as.data.frame(confint(meta_reg_22))

# I-squared +CI
pes_reg_I_squared <- dmetar::var.comp(meta_reg_22)
#Explained Variance in by best Mixed-Effects model 
R_squard2 <- round(1 - (meta_reg_22$sigma2[2]/pes.da.lvl3$sigma2[2]), 3)
R_squard3 <- round(1 - (meta_reg_22$sigma2[1]/pes.da.lvl3$sigma2[1]), 3)

results2 <- data.frame(Parameter = c("mu",
                                     "within-study sig", 
                                     "between-study sig"), 
                       int.only = c(pes$pred,
                                    conf_intervals_lvl3[3,1],
                                    conf_intervals_lvl3[1,1]),
                       
                       int.only.lb = c(pes$ci.lb,
                                       conf_intervals_lvl3[3,2],
                                       conf_intervals_lvl3[1,2]),
                       
                       int.only.ub = c(pes$ci.ub, 
                                      conf_intervals_lvl3[3,3],
                                      conf_intervals_lvl3[1,3]),
                       
                       best.mod = c(backtras.function(meta_reg_22$b[1], 
                                                        meta_reg_22$se[1]), 
                                     conf_intervals_meta_reg[3,1], 
                                     conf_intervals_meta_reg[1,1]), 
                       
                       best.mod.lb = c(backtras.function(meta_reg_22$ci.lb[1], 
                                                                    meta_reg_22$se[1]),
                                      conf_intervals_meta_reg[3,2],
                                      conf_intervals_meta_reg[1,2]),
                                                  
                       best.mod.ub = c(backtras.function(meta_reg_22$ci.ub[1], 
                                                                    meta_reg_22$se[1]),
                                       conf_intervals_meta_reg[3,3],
                                       conf_intervals_meta_reg[1,3])
                                              )
results2 <- results2|> mutate_if(is.numeric, round, digits=3)
results2$Intercept.Only <- paste0(results2$int.only,
                                  " [", results2$int.only.lb,
                                  ", ", results2$int.only.ub, "]")
results2$Best.Mod <- paste0(results2$best.mod,
                                  " [", results2$best.mod.lb,
                                  ", ", results2$best.mod.ub, "]")
results2 <- results2|>
  select(Parameter, Intercept.Only, Best.Mod)
results2[4, ] <- c("Cochran's Q", 
                 q.function(Q = pes.da.lvl3$QE,
                            df = pes.da.lvl3$QEdf,
                            p = pes.da.lvl3$QEp),
                 q.function(Q = meta_reg_22$QE,
                            df = meta_reg_22$QEdf,
                            p = meta_reg_22$QEp)
)
results2[5, ] <- c("I2 Level 2", 
                 pes.da.lvl3_I_squared$results$I2[2],
                 pes_reg_I_squared$results$I2[2])
results2[6, ] <- c("I2 Level 3", 
                 pes.da.lvl3_I_squared$results$I2[3],
                 pes_reg_I_squared$results$I2[3])
results2[7, ] <- c("R2", " ", paste0("Level 2 : ",R_squard2, 
                                     " Level 3 : ",R_squard3))
```

::: {#tbl-coef}
```{r}
meta_reg_22_backtransformed <- mapply(backtras.function, meta_reg_22$b, meta_reg_22$se)

meta_reg_22|>
  tidy() |>
  mutate(estimate_back_transfromed = mapply(backtras.function, meta_reg_22$b, meta_reg_22$se))|>
  select(term, estimate, std.error, statistic,	p.value, estimate_back_transfromed) |>
  
  
  kable(booktabs = TRUE, linesep = "", 
        col.names = c("Predictor", "b", "SE", "t", "p", "b back transformed"), 
        digits = c(0, 3, 2, 2, 3, 3), 
        align = c("l", "r", "r", "r", "r", "r")
  ) |>
  kable_styling(font_size = 10, 
                full_width = FALSE)|>
  
  
  footnote(threeparttable=T)
```

Coef
:::

::: {style="text-align: justify"}
@fig-bubble illustrates the relationship between the proportion of the majority class and overall accuracy of the individual studies included in the meta-analysis. The plot is based on Model 22, with the solid black line representing the fitted regression line and the shaded area indicating the 95% confidence interval. Each point (bubble) represents a study, with its size proportional to the weight it received in the analysis (larger points indicate studies with more influence). The plot shows that as the proportion of the majority class increases, overall accuracy tends to improve.
:::

```{r, eval=FALSE}
library(RColorBrewer)
colors <- c(brewer.pal(9, "Pastel1"), brewer.pal(12, "Set3"))
authors <- unique(ies.da$AuthorYear)
author_colors <- setNames(colors[1:length(authors)], authors)
ies.da$color <- author_colors[ies.da$AuthorYear]

# labels 
lowest_yi_per_author <- ies.da |>
  group_by(AuthorYear) |>
  filter(yi == min(yi)) |>
  select(AuthorYear, effectsizeID)
png(file="fig-bubble.png", width=700, height=700)
bubble_plot <- regplot(meta_reg_22,
        mod = "fraction_majority_class", 
        transf = transf.ipft.hm, targ =list(ni=1/(meta_reg_22$se)^2), 
        xlab = "Proportion of majority class", 
        ylab = "Overall Accuracy",
        slab = ies.da$AuthorYear, 
        label = lowest_yi_per_author$effectsizeID,
        labsize = 1, 
        bg=ies.da$color, 
        xlim=c(0.05, 1.05), 
        ylim=c(0.64, 1))

dev.off()
```

![Bubble plot showing the observed effect size, overall accuracy of the individual studies plotted against a the proportion of the majority class. Based on the mixed-effects meta-regression model, the overall accuracy as a function of proportion of the majority with corresponding 95% confidence interval bounds. The size of the points are proportional to the weight that the observation received in the analysis, while the color of the points is unique to each study, with the lowest overall actuary from each study labeled with the first author and publication year.](figures/fig-bubble.png){#fig-bubble}

::: {style="text-align: justify"}
@fig-preds shows the observed overall accuracy against the predicted overall accuracy's made by Model 22. The points are coloured by the addition of ancillary information in the primary study. It appears that the addition of ancillary information leads to a lower overall accuracy, however, this could be due to a number of unmeasured factors, such as study's with more complicated classifications (more similar classes) adding accuracy data. As @fig-preds shows Model 2 over estimates the overall accuracy --- the fit regression line (in grey) is above the line of perfect agreement ($y = x$, in black).
:::

::: {#fig-preds}
```{r}
ies.da$preds <- predict(meta_reg_22, transf = transf.ipft.hm, targ =list(ni=1/(meta_reg_22$se)^2))$pred

ggplot(ies.da, aes(x = OA_reported, y = preds))+
  geom_point(size = 5, aes(colour = ancillary), alpha = 0.7)+
  geom_smooth(method = "lm",formula = 'y ~ x',  se = FALSE, colour = "grey", linetype= "dashed", linewidth = 0.8)+
  # y = x line 
  geom_textabline(label = "y = x", intercept = 0, slope = 1,hjust = 0.2,  linetype= "longdash")+
  scale_colour_brewer(palette = "Dark2",
                      labels = c("Only Remote Sensing Data","Addition of Ancillary Data"))+
  xlim(c(0.64, 1))+
  ylim(c(0.64,1))+
  labs(x = "Reported Overall Accuracy", 
       y = "Predicted Overall Accuracy", 
       title = "Plot of Observed Agaist Predicted Accuracy Based on the Meta-regression Model.")+
    theme(legend.text = element_text(size = 8), 
          legend.position = "top") +
  common_theme
```

Observed and predicted overall accuracy. The colour indicates whether the addition of ancillary data in the primary study's model. The line of perfect agreement $y = x$ is in black and fit regression line in grey.
:::

Lastly, @tbl-results shows the parameter estimates of the meta-analysis comparing the Intercept Only Model and the Mixed Effects Model(Model 22). The Intercept Only Model estimates a population effect size $\hat{\mu}$ of 0.894, with estimated variance components $\sigma^2_{\zeta} =$ `r results$sigma2.1[1]` and $\sigma^2_{\xi} =$ `r results$sigma2.2[1]`. Model 22, reduces the population effect size to 0.696 (95% CI: \[0.579, 0.801\]). The within-study variance (0.009) remains similar, while the between-study variance is reduced to 0.005, indicating less variability between studies when accounting for covariates. This shifts is also reflect in the $I^2$ Level 2 goes from 36.38% to 63.46% and at Level 2 from 63,62% to 36.54%, indicating less variability between studies when accounting for covariates. The total $I^2$ consistently being 100% in both models indicates that almost none of the variation between effect sizes can be attributed to sampling error, this might suggest that the included studies are too different from each to compare (see discussion for apples and oranges problem). Both models show significant heterogeneity (Cochran's Q, p \< 0.001) results. The $R^2$values show that the covariates in the Mixed Effects Model explain 69.9% of the variance at Level 3 and 8.6% at Level 2.

::: {#tbl-results}
```{r}
results2|>
  kable(booktabs = TRUE, linesep = "", 
        col.names = c("Parameter", "Intercept Only Model", "Mixed Effects Model"), 
        align = c("l", "r", "r")
  ) |>
  kable_styling(font_size = 10, 
                full_width = FALSE)|>
  
  
  footnote(threeparttable=T)
```

Heterogeneity Metrics
:::
