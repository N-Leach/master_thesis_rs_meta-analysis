# Abstract {.unnumbered .unlisted}

::: {style="text-align: justify"}
**Objective:** This meta-analysis aims to evaluate machine learning methods in remote sensing applications for monitoring Sustainable Development Goals (SDGs). Specifically the aims are (a) estimate the average performance (population effect size); (b) determine whether performance varies within and across studies (establish the degree of heterogeneity); (c) assess whether study features are related to the performance of machine learning models; and (d) compare the sample-weighted and unweighted estimate summary effect.

**Methods:** The meta-analysis used the PRISMA guidelines. A search was performed across multiple academic databases to identify peer-reviewed studies which applied machine learning models to remote sensing data for SDG monitoring. A random sample of 200 relevant studies was selected for abstract screening, which was reduced to 20 studies with 86 effect sizes for the analysis. To estimate the overall accuracy of machine learning models both a three-level random-effects model and an unweighted model were used.

**Results:** The average overall accuracy of the unweighted model is 0.90 (95% CI \[0.85; 0.94\]), which is not substantially different from the weighted model at 0.89 (CI 95% \[0.85, 0.94\]. The weighted models found substantial heterogeneity between results. The proportion of the majority class was identified as the most important factor affecting the overall accuary, followed by the inclusion of ancillary data. However, type of machine learning model (e.g., neural networks, tree-based models) or SDG goal did not have a significant effect. When the important study features were included (in the mixed effects model) the average overall accuracy dropped 0.70 (CI 95% CI \[0.58, 0.80\]).

**Conclusion:** This study demonstrates the high variability model performance in remote sensing applications. As well as the effect that the proportion of the majority class has on the reported overall accuracy. These findings suggest the need for precise metrics to assess model performance, particularly in imbalanced datasets. Future research should examine a broader range of performance metrics and explore additional study features to explore further what features affect the outcomes. In addition the robustness of the random-effects meta-analysis methods application to this field should be further examined.
:::
