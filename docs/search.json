[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Evaluating the Performance of Machine Learning Models in Remote Sensing for Sustainable Development Goals: A Meta-Analysis",
    "section": "",
    "text": "Foreword\n\nThe aim of this research was to assess whether study features can explain variations in results across studies. To my knowledge, this is the first study to apply weighted meta-analysis techniques to examining the performance of machine learning models in remote sensing. While I intended to adhere to PRISMA guidelines, the exploratory nature of the topic—and my own learning journey—meant pre-registration was ultimately not conducted, and with data extraction carried out solely by myself, there is a degree of subjectivity and potential for error there. But I’m giving away spoilers for the discussion, so I’ll stop myself there!\nI have succeeded in building this manuscript using Quarto, with minimal stylistic adjustments after rendering. The entire code for this project is available on GitHub, and an HTML version with integrated code chunks can be accessed on GitHub Pages website. The data processing and paper selection analysis scripts are all available on the GitHub-hosted site under the appendix (more details about the file organisation are available on the GitHub page). I have also integrated the Leiden University master thesis cover format into the Quarto book, so if any future student would like to reuse it please do! I toyed with the idea of creating a Quarto book template but that is a project for another day.\nOne last thing, in the discussion of this thesis I suggest that journals should begin requesting data submissions alongside the manuscript to enable active, ongoing meta-analyses. In support of this idea, I developed a small pilot website to demonstrate how such a system might work. If anyone feels inclined to add to the dataset, there are instructions on how to do that there.\nTo the reader: thank you for taking the time to read my thesis— there’s still time to stop reading, and I won’t know any different! If you’ve made it this far —hi supervisors, independent reader (and mum)— I would like to apologize in advance for continuing the convention of inconsistent notation across meta-analysis research. I can only hope I have been consistent within my own work as its best not to change notations \\(\\mu\\)-dstream.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "frontmatter/abstract.html",
    "href": "frontmatter/abstract.html",
    "title": "Abstract",
    "section": "",
    "text": "Objective: This meta-analysis aims to evaluate machine learning methods in remote sensing applications for monitoring Sustainable Development Goals (SDGs). Specifically, it aims to (1) estimate the average performance (summary effect size); (2) determine the degree of heterogeneity within and across studies; (3) assess whether specific study features influence model performance; and (4) compare the sample-weighted and unweighted estimate summary effect.\nMethods: The meta-analysis used the PRISMA guidelines. A search was performed across multiple academic databases to identify peer-reviewed studies that applied machine learning models to remote sensing data for SDG monitoring. A random sample of 200 relevant studies was selected for abstract screening, which was reduced to \\(n = 20\\) studies with \\(k = 86\\) effect sizes for the analysis. To estimate the overall accuracy of machine learning models both a three-level random-effects model and an unweighted model were used.\nResults: The average overall accuracy of the unweighted model, \\(\\hat{\\mu}_{_\\text{unweighted}} =\\) 0.90 (95% CI [0.85; 0.94]), which is not substantially different from the weighted model, \\(\\hat{\\mu}_{_\\text{weighted}} =\\) 0.89 (CI 95% [0.85, 0.94]. The weighted models found substantial heterogeneity between results. Unsurprisingly, the proportion of the majority class was identified as the most important factor affecting the overall accuracy, followed by the inclusion of ancillary data. However, machine learning model group (i.e., neural networks, tree-based models) or SDG goal did not have a significant effect on the reported overall accuracy.\nConclusion: This study demonstrates the high variability model performance in remote sensing applications. As well as the impact class imbalance has on the reported overall accuracy. These findings suggest the need for precise metrics to assess model performance, particularly in imbalanced datasets. Future research should examine a broader range of performance metrics and explore additional study features to explore further what features affect the outcomes. In addition, the robustness of the random-effects meta-analysis methods application to this field should be further examined.",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In 2015, all United Nations member states adopted the Sustainable Development Goals (SDGs) to address global challenges such as climate change, environmental degradation, poverty, and inequality (UN DESA, 2023; UN-GGIM:Europe, 2019). This international plan outlines 17 global goals to achieve a better and more sustainable future (UN DESA, 2023; UN-GGIM:Europe, 2019; United Nations, 2024). Having passed the midpoint of the SDGs’ timeline with significant setbacks, the critical role of timely and high-quality data has never been more apparent (UN DESA, 2023; United Nations, 2024). These data are vital to identifying challenges, formulating evidence-based solutions, monitoring the implementation of solutions, and making essential course corrections (UN-GGIM:Europe, 2019). However, despite this necessity for high-quality data, traditional monitoring approaches, such as household- or field-level surveys (ground-acquired data), remain the primary source of data collection for key indicators of SDGs by National Statistical Institutes (NSIs) (Burke et al., 2021; UN-GGIM:Europe, 2019). These methods are expensive and time-consuming to conduct (Burke et al., 2021). As a result, the frequency of ground-acquired data varies significantly around the world; for example, the most recent agricultural census for 24% of the world’s countries was more than 15 years ago (Burke et al., 2021). Recognizing this challenge, both the United Nations SDG Report (2023, p. 49) and the Global Working Group on Big Data for Official Statistics underscore the importance of innovative methodology and data sources, including remote sensing and machine learning, to enhance the monitoring and implementation of the SDGs (UN-GGIM:Europe, 2019; United Nations, 2017).\nRemote sensing — data collected from a distance via satellite, aircraft, or drones — offers a cost-effective approach for monitoring wide-ranging geographic areas (Khatami et al., 2016; Maso et al., 2023; UN-GGIM:Europe, 2019; Zhao et al., 2022). Remote sensing imagery has been limited to agricultural and socioeconomic applications for decades (Burke et al., 2021; Lavallin & Downs, 2021; Zhang et al., 2022). For instance, the Laboratory for Applications of Remote Sensing (LARS) has utilized satellite data and machine learning methods for crop identification since the 1960s (Holloway & Mengersen, 2018). However, in recent years, there has been a considerable increase in the spatial, spectral, and temporal resolution of remote sensing data, alongside a significant increase in free sensor data and computational power for complex data analysis (Burke et al., 2021; Thapa et al., 2023; Zhang et al., 2022). The magnitude of possible applications and increased availability of remote sensing data have rapidly increased the number of published research papers in this field (Burke et al., 2021; Khatami et al., 2016). Earth observation satellites alone can measure 42% of the SDG targets (Zhang et al., 2022).\nDespite the increased research and availability the uptake of remote sensing data by NSIs has been slow. However, many NSIs are now capitalizing on the potential of using new and consistent data sources and methodologies to support and inform official statistics (United Nations, 2017). These can be generated by combining geospatial information, remote sensing, and other big data sources, allowing for the filling of data gaps, providing information where no measurements were previously made, and improving the temporal and spatial resolutions of data (e.g., daily updates on crop area and yield statistics). This paradigm shift from traditional statistical methods—such as counting and measuring by humans—towards estimation from sensors, simulation, and modelling, presents challenges, and requires convincing, statistically sound results, rigorous validation, and a significant shift in resources within institutions to adapt to the higher spatial and temporal resolutions necessary to address emerging policy questions (United Nations, 2017).\nGiven the wide variety of methodologies and contexts in previous studies, a critical question arises: What factors influence the performance of machine learning models using remote sensing data for SDG monitoring? A meta-analysis statistically combines the body of evidence on a specific topic, aiming to produce unbiased summaries of evidence (Iliescu et al., 2022). There are many potential methods to choose from to combine results. One choice that is made when conducting a meta-analysis is whether to use the study’s sample size to weigh the result of each study (sample-weighted estimate) or an unweighted approach, which treats all results equally, disregarding sample size (Hall & Rosenthal, 2018). The current standard in meta-analysis research is to use the sample-weighted estimate (Hall & Rosenthal, 2018). However, the previous meta-analyses investigating the performance of machine learning models on remote sensing data have exclusively relied on unweighted approaches. While these studies have found that certain models, such as Support Vector Machines (SVM) and deep learning methods, often outperform traditional classifiers, the magnitude of these differences can vary across applications. For example, Khatami et al. (2016) selected studies with more than one model, and by making pairwise comparisons they concluded that SVM consistently outperformed other classification models. However, these meta-analyses relied on unweighted approaches, potentially overlooking if these variations in results are due to differences in sample sizes, which could affect the reliability and precision of the findings, as larger studies generally provide more accurate estimates.\nTherefore, this study seeks to address the question of how machine learning models perform when applied to remote sensing data for SDG monitoring. By conducting a meta-analysis on peer-reviewed research articles in this domain, the study aims to; (1) estimate the average performance (summary effect size), (2) determine the degree of heterogeneity within and across studies, (3) assess whether specific study features influence model performance, and (4) compare the sample-weighted and unweighted estimate summary effect.\n\n\n\n\n\nBurke, M., Driscoll, A., Lobell, D. B., & Ermon, S. (2021). Using satellite imagery to understand and promote sustainable development. Science, 371(6535), eabe8628. https://doi.org/10.1126/science.abe8628\n\n\nHall, J. A., & Rosenthal, R. (2018). Choosing between random effects models in meta-analysis: Units of analysis and the generalizability of obtained results. Social and Personality Psychology Compass, 12(10), e12414. https://doi.org/10.1111/spc3.12414\n\n\nHolloway, J., & Mengersen, K. (2018). Statistical Machine Learning Methods and Remote Sensing for Sustainable Development Goals: A Review. Remote Sensing, 10(9), 1365. https://doi.org/10.3390/rs10091365\n\n\nIliescu, D., Rusu, A., Greiff, S., Fokkema, M., & Scherer, R. (2022). Why We Need Systematic Reviews and Meta-Analyses in the Testing and Assessment Literature. European Journal of Psychological Assessment, 38(2), 73–77. https://doi.org/10.1027/1015-5759/a000705\n\n\nKhatami, R., Mountrakis, G., & Stehman, S. V. (2016). A meta-analysis of remote sensing research on supervised pixel-based land-cover image classification processes: General guidelines for practitioners and future research. Remote Sensing of Environment, 177, 89–100. https://doi.org/10.1016/j.rse.2016.02.028\n\n\nLavallin, A., & Downs, J. A. (2021). Machine learning in geography–Past, present, and future. Geography Compass, 15(5), e12563. https://doi.org/10.1111/gec3.12563\n\n\nMaso, J., Zabala, A., & Serral, I. (2023). Earth Observations for Sustainable Development Goals. Remote Sensing, 15(10), 2570. https://doi.org/10.3390/rs15102570\n\n\nThapa, A., Horanont, T., Neupane, B., & Aryal, J. (2023). Deep Learning for Remote Sensing Image Scene Classification: A Review and Meta-Analysis. Remote Sensing, 15(19), 4804. https://doi.org/10.3390/rs15194804\n\n\nUN DESA. (2023). The Sustainable Development Goals Report 2023: Special Edition. United Nations. https://doi.org/10.18356/9789210024914\n\n\nUN-GGIM:Europe. (2019). The territorial dimension in SDG indicators: Geospatial data analysis and its integration with statistical data. Instituto Nacional de Estatística. https://un-ggim-europe.org/wp-content/uploads/2019/05/UN_GGIM_08_05_2019-The-territorial-dimension-in-SDG-indicators-Final.pdf\n\n\nUnited Nations. (2017). Earth observations for official statistics: Satellite imagery and geospatial data task team report. https://unstats.un.org/bigdata/task-teams/earth-observation/UNGWG_Satellite_Task_Team_Report_WhiteCover.pdf\n\n\nUnited Nations. (2024). The sustainable development goals report 2024. https://unstats.un.org/sdgs/report/2024/The-Sustainable-Development-Goals-Report-2024.pdf\n\n\nZhang, Y., Liu, J., & Shen, W. (2022). A Review of Ensemble Learning Algorithms Used in Remote Sensing Applications. Applied Sciences, 12(17), 8654. https://doi.org/10.3390/app12178654\n\n\nZhao, Q., Yu, L., Du, Z., Peng, D., Hao, P., Zhang, Y., & Gong, P. (2022). An Overview of the Applications of Earth Observation Satellite Data: Impacts and Future Trends. Remote Sensing, 14(8), 1863. https://doi.org/10.3390/rs14081863",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/background.html",
    "href": "chapters/background.html",
    "title": "2  Background",
    "section": "",
    "text": "Remote Sensing\nCode\n# data for the number of satellites\nUCS_Satellite_Database &lt;- read.csv(\"figures/UCS-Satellite-Database_5-1-2023.csv\")\n\nUCS_Satellite_Database_EarthObs &lt;- \n  UCS_Satellite_Database |&gt; \n  mutate(Date_Launch = as.Date(Date_Launch, format = \"%Y-%m-%d\"))|&gt;\n  filter(!grepl(\"Military\", Users))|&gt;\n  filter(grepl(\"Earth\",Purpose ))\n\nprop_after_2020&lt;- sum(year(UCS_Satellite_Database_EarthObs$Date_Launch)&gt;=2020,\n                      na.rm = TRUE)/ nrow(UCS_Satellite_Database_EarthObs)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#remote-sensing",
    "href": "chapters/background.html#remote-sensing",
    "title": "2  Background",
    "section": "",
    "text": "In the broadest sense, remote sensing involves acquiring information about an object or phenomenon without direct contact (Campbell & Wynne, 2011). More specifically, remote sensing refers to gathering data about land or water surfaces using sensors mounted on aerial or satellite platforms that record electromagnetic radiation reflected or emitted from the Earth’s surface (Campbell & Wynne, 2011, p. 6). The origins of remote sensing lie with the development of photography in the 19th century, with the earliest aerial or Earth Observation photographs taken with cameras mounted on balloons, kites, pigeons, and aeroplanes. (Burke et al., 2021; Campbell & Wynne, 2011, p. 7). The first mass use of remote sensing was during World War I with aerial photography. The modern era of satellite-based remote sensing started with the launch of Landsat 1 in 1972, the first satellite specifically designed for Earth Observation (Campbell & Wynne, 2011, p. 15). Today, remote sensing technology enables frequent and systematic collection of data about the Earth’s surface with global coverage, revolutionizing our ability to monitor and analyze the Earth’s surface (Burke et al., 2021; NASA, 2019). As of May 2023, roughly 1039 active nonmilitary Earth Observation satellites are in orbit; 51% were launched in 2020 (UCS, 2021).\n\n\nCode\nggplot(data = UCS_Satellite_Database_EarthObs|&gt; \n         group_by(week = lubridate::floor_date(Date_Launch, 'week'))|&gt;\n         summarise(n = n()),\n       aes(x = week, y = n)) +\n  geom_line(aes(y = cumsum(n)), na.rm = TRUE) +\n  \n  scale_x_date(date_breaks = \"5 years\", date_labels = \"%Y\") +\n  \n  labs(y = \"Number of Satellite\", x = \"Year\", \n        title = \"Number of Satellites Launched Over Time\") +\n  common_theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Number of active satellites by date of launch — data acquired from UCS (2021).\n\n\n\n\nSensors on remote sensing devices such as satellites measure electromagnetic radiation reflected by objects on the Earth’s surface. This is done in two different ways: passive and active. Passive sensors rely on natural energy sources, like sunlight, to record incident energy reflected off the Earth’s surface. While active sensors generate their own energy, which is emitted and then measured as it reflects back from with the Earth’s surface (NASA, 2019).\n\n\n\n\n\n\n\nFigure 2.2: Illustration of a passive sensor and an active sensor —source: NASA (2019).\n\n\n\n\nComponents of the Earth’s surface have different spectral signatures — i.e., reflect, absorb, or transmit energy in different amounts and wavelengths (Campbell & Wynne, 2011). Remote sensing devices have several sensors that measure specific ranges of wavelengths in the electromagnetic spectrum; these are referred to as spectral bands; e.g., visible light, infrared, or ultraviolet radiation (NASA, 2019; SEOS, 2014). By capturing information from particular bands, the spectral signatures of surfaces can be used to identify objects on the ground. Figure 2.3 illustrates the differences between the spectral signatures of soil, green vegetation, and water across various wavelengths. The grey bands in the figure represent the specific spectral bands on the Landsat TM satellite (SEOS, 2014). The distinct reflectance properties of each material within these bands enable the differentiation of surface materials, making it possible to identify different land cover types. This information can be used directly for classification, or it can be combined into indices—such as the Normalized Difference Vegetation Index (NDVI)—to enhance the detection of specific features like vegetation health and coverage (Campbell & Wynne, 2011; NASA, 2019). The \\(NDVI\\) uses red light and near-infrared (NIR) to distinguish green vegetation. Higher \\(NDVI\\) values indicate green vegetation as more red light is absorbed, whereas lower values correspond to non-vegetated areas where more red light is reflected.\n\n\n\n\n\n\n\nFigure 2.3: Spectral signatures of soil, green vegetation, and water across different wavelengths, representing the portion of incident radiation that is reflected by each material as a function of wavelength. The grey bands indicate the spectral ranges (channels) of Landsat TM satellite. Bands 1-3 capture visible light (Blue, Green, Red), Band 4 captures near-infrared (NIR), and Bands 5 and 7 cover parts of the intermediate infrared spectrum. These spectral bands allow for the differentiation of various surface materials based on their unique reflectance properties —source: Siegmund and Menz (2005) as cited and modified by SEOS (2014).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#machine-learning",
    "href": "chapters/background.html#machine-learning",
    "title": "2  Background",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nMachine learning techniques such as neural networks, random forests, and support vector machines have long been applied for spatial data analysis and geographic modelling (Haddaway et al., 2022; Lavallin & Downs, 2021). Compared to using indices alone, machine learning techniques enhance the accuracy and efficiency of data analysis and interpretation processes, making it possible to analyze large volumes of data effectively. This is particularly useful for handling the high complexity and dimensionality of remote sensing data. In recent years, the application of machine learning techniques in remote sensing has surged, driven by the increasing availability of large datasets and advancements in computational power (UN-GGIM:Europe, 2019; Y. Zhang et al., 2022).\nThese machine learning models can be grouped into four main types according to the aims of analyses: classification, clustering, regression, and dimension reduction. Table 2.1 describes this grouping and gives examples. It is important to note that recent trends in machine learning and remote sensing analyses use hybrid or ensemble approaches using a combination of these groups, for a thorough review of these methods see UN-GGIM:Europe (2019).\n\n\nCode\nread_excel(\"figures/summary_tables.xlsx\", sheet = \"ML_cat\")|&gt;\n  kable(booktabs = TRUE, linesep = \"\") |&gt;\n  kable_styling(font_size = 10, \n                full_width = FALSE)|&gt;\n  column_spec(1, width = \"2cm\")|&gt;\n  column_spec(2, width = \"14cm\")|&gt;\n  \n  footnote(c(\"Adapted from UN-GGIM:Europe (2019) and Haddaway et al.(2022).\"), \n           threeparttable=TRUE)\n\n\n\n\nTable 2.1: Categories of machine learning methods grouped according to the analytic aim.\n\n\n\n\n\n \n  \n    Analysis aim \n    Explanation \n  \n \n\n  \n    Classification \n    Assigning objects to known classes based on input variables. For example, categorizing pixels in an image into crop types using a model trained on known data. \n  \n  \n    Regression \n    Predict a numeric (discrete or continuous) response variable based on input variables, similar to classification but with numeric outputs. An example is predicting crop yield from Earch Observation image data. \n  \n  \n    Clustering \n    Groups objects based on input variables without pre-defined classes, identifying similarities among the objects. This can help in grouping pixels in an image for further inspection. \n  \n  \n    Dimension reduction \n    Reduces a large set of variables to a smaller set that retains most of the original information. This can simplify analysis or generate new variables like indices (e.g., Vegetation Index) for interpretation. \n  \n\n\nNote: \n\n Adapted from UN-GGIM:Europe (2019) and Haddaway et al.(2022).\n\n\n\n\n\n\n\n\n\nPerformance metrics are used to verify these analyses, which for classification tasks involve creating a confusion matrix — a cross-tabulation of class labels assigned to model predictions and reference data (ground truth). In a confusion matrix, the correctly classified instances are on the diagonal, and the off-diagonal cells indicate which classes are confused (i.e., incorrectly classified). In remote sensing applications, accuracy assessments are undertaken on a pixel, group of pixels (e.g. block), or an object level (Stehman & Foody, 2019).\n\n\nCode\nconfusion &lt;- data.frame(\n  Reference = c(\"Class 1\",\"Class 2\",\"Class 3\",\"Class 4\", \"Total\", \"User's accuracy\"), \n  #Predictions \n  c1 = c(\"$m_{11}$\", \"$m_{21}$\", \"$m_{31}$\", \"$m_{41}$\", \"$m_{.1}$\", \"$m_{11}/m_{.1}$\"),\n  c2 = c(\"$m_{12}$\", \"$m_{22}$\", \"$m_{32}$\", \"$m_{42}$\", \"$m_{.2}$\", \"$m_{22}/m_{.2}$\"),\n  c3 = c(\"$m_{13}$\", \"$m_{23}$\", \"$m_{33}$\", \"$m_{43}$\", \"$m_{.3}$\", \"$m_{33}/m_{.3}$\"),\n  c4 = c(\"$m_{14}$\", \"$m_{24}$\", \"$m_{33}$\", \"$m_{44}$\", \"$m_{.4}$\", \"$m_{44}/m_{.4}$\"),\n  total = c(\"$m_{1.}$\", \"$m_{2.}$\", \"$m_{3.}$\", \"$m_{4.}$\", \"$m$\", \"\"),\n  pa = c(\"$m_{11}/m_{1.}$\", \"$m_{22}/m_{2.}$\",\n         \"$m_{33}/m_{3.}$\", \"$m_{44}/m_{4.}$\", \"\", \"\")\n)\nconfusion|&gt;\n  kable(booktabs = TRUE, linesep = \"\", escape = FALSE, \n        col.names = c(\"Reference\",\"Class 1\",\"Class 2\",\"Class 3\",\"Class 4\", \"Total\", \"Producer's accuracy\")\n        ) |&gt;\n  kable_styling(font_size = 10, \n                full_width = FALSE)|&gt;\n  add_header_above(c(\"\", \"Predictions\" = 6), bold = TRUE) |&gt;\n  row_spec(0, bold = TRUE)|&gt;\n  column_spec(1, bold = TRUE)|&gt;\n  footnote(c(\"The rows ($r$) represent the reference (observed) classification and the columns ($c$) represent the predicted classes. $m_{rc}$ is the number of instances in reference (observered) class $r$ and predicted class $c$, and $m$ is the total number of instances (i.e., the number of pixels/objects classified).\"), \n           threeparttable=TRUE, escape=FALSE)\n\n\n\n\nTable 2.2: Confusion matrix of four classes\n\n\n\n\n\n \n\n\nPredictions\n\n  \n    Reference \n    Class 1 \n    Class 2 \n    Class 3 \n    Class 4 \n    Total \n    Producer's accuracy \n  \n \n\n  \n    Class 1 \n    $m_{11}$ \n    $m_{12}$ \n    $m_{13}$ \n    $m_{14}$ \n    $m_{1.}$ \n    $m_{11}/m_{1.}$ \n  \n  \n    Class 2 \n    $m_{21}$ \n    $m_{22}$ \n    $m_{23}$ \n    $m_{24}$ \n    $m_{2.}$ \n    $m_{22}/m_{2.}$ \n  \n  \n    Class 3 \n    $m_{31}$ \n    $m_{32}$ \n    $m_{33}$ \n    $m_{33}$ \n    $m_{3.}$ \n    $m_{33}/m_{3.}$ \n  \n  \n    Class 4 \n    $m_{41}$ \n    $m_{42}$ \n    $m_{43}$ \n    $m_{44}$ \n    $m_{4.}$ \n    $m_{44}/m_{4.}$ \n  \n  \n    Total \n    $m_{.1}$ \n    $m_{.2}$ \n    $m_{.3}$ \n    $m_{.4}$ \n    $m$ \n     \n  \n  \n    User's accuracy \n    $m_{11}/m_{.1}$ \n    $m_{22}/m_{.2}$ \n    $m_{33}/m_{.3}$ \n    $m_{44}/m_{.4}$ \n     \n     \n  \n\n\nNote: \n\n The rows ($r$) represent the reference (observed) classification and the columns ($c$) represent the predicted classes. $m_{rc}$ is the number of instances in reference (observered) class $r$ and predicted class $c$, and $m$ is the total number of instances (i.e., the number of pixels/objects classified).\n\n\n\n\n\n\n\n\n\nFrom this matrix, performance measures such as overall accuracy are derived (FAO, 2016; Stehman & Foody, 2019; UN-GGIM:Europe, 2019) where the overall accuracy is the total number of successful classifications \\(s\\) over the total number of instances, \\(m\\) (\\(q\\) is the number of classes).\n\\[\n\\text{Overall Accuracy (OA)} = \\frac{\\sum^q_{r=1}m_{rr}}{m}= \\frac{s}{m}\n\\tag{2.1}\\]\nIf the unit of accuracy assessment is a pixel, then overall accuracy is the proportion of pixels classified correctly. Other metrics include reliability (User’s accuracy) and sensitivity (recall or Producer’s accuracy). Reliability is the correct classification for a particular class divided by the column total (\\(m_{.c}\\)), and sensitivity is the correct classification over the row total (\\(m_{r.}\\)). It is important to consider the map’s purpose when evaluating its accuracy, as overall accuracy may not reflect the accuracy of specific classes. Factors such as sample size, class stability, class proportions, and landscape variability influence the overall accuracy (FAO, 2016; see UN-GGIM:Europe, 2019).\n\nAustralia Land Cover Mapping\nAs an example of how remote sensing data and machine learning can be used to support ecologically sustainable development, Owers et al. (2022) developed an approach to monitor and map land cover across Australia using techniques. Their study used Landsat sensor data archive through Digital Earth Australia to generate annual land cover maps from 1988 to 2020 at a 25-meter resolution. The study used random forest and artificial neural networks to classify individual pixels according to the FAO’s Land Cover Classification System (LCCS) framework.\n\n\n\n\n\n\n\n\nFigure 2.4: Land cover mapping created by Owers et al. (2022) using Landstat data to make continent-wide classifications using the LCCS frame work which differentiates six (classes) land cover types: cultivated terrestrial vegetation (CTV), natural terrestrial vegetation (NTV), natural aquatic vegetation (NAV), artificial surfaces (AS), bare surfaces (BS), and water bodies (W).\n\n\n\n\nProducing such detailed maps through traditional topographical field surveys would be impractical, given Australia’s size (\\(7,688,287 \\text{ km}^2\\)). While field surveys are considered the most accurate method for generating training sample data, they are labor-intensive, time-consuming, and costly (C. Zhang & Li, 2022). For example, surveying just 20 hectares (\\(0.2 \\text{ km}^2\\)) would take a team of four people approximately five days to complete, though the resulting topographical map would have a high resolution of 0.5 meters (L.A. Mbila, personal communication, January 26, 2024). In Owers et al. (2022), experts visually inspected the satellite imagery to validate the training and test data. While this method is less labor-intensive, costly, and time-consuming than field surveys, it still demands significant effort and expertise.\nIn contrast to the limitations of field surveys, remote sensing provides an efficient means for continuously monitoring large, often inaccessible areas (Owers et al., 2022; C. Zhang & Li, 2022). The potential applications of this technology are vast, including land use and degradation monitoring, forestry, biodiversity assessment, agriculture, disaster prediction, water resource management, public health, urban planning, poverty tracking, and the management and preservation of world heritage sites (Anshuka et al., 2019; Campbell & Wynne, 2011; Ekmen & Kocaman, 2024; O. Hall et al., 2023; Lavallin & Downs, 2021; Maso et al., 2023).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#australia-land-cover-mapping",
    "href": "chapters/background.html#australia-land-cover-mapping",
    "title": "2  Background",
    "section": "Australia Land Cover Mapping",
    "text": "Australia Land Cover Mapping\nAs an example of how remote sensing data and machine learning can be used to support ecologically sustainable development, Owers et al. (2022) developed an approach to monitor and map land cover across Australia using techniques. Their study used Landsat sensor data archive through Digital Earth Australia to generate annual land cover maps from 1988 to 2020 at a 25-meter resolution. The study used random forest and artificial neural networks to classify individual pixels according to the FAO’s Land Cover Classification System (LCCS) framework.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#previous-reviews",
    "href": "chapters/background.html#previous-reviews",
    "title": "2  Background",
    "section": "Previous Reviews",
    "text": "Previous Reviews\n\nNumerous studies have previously examined the application of remote sensing for SDG monitoring. However, existing reviews are typically either limited to specific contexts, such as the use of satellite data for poverty estimation (O. Hall et al., 2023) or focus on descriptive results (see Yin et al., 2023). The existing reviews either apply methodology that aligns more closely with Synthesis Without Meta-Analysis (Campbell et al., 2020) —for example, Thapa et al. (2023) and Ekmen & Kocaman (2024) — or apply unweighted meta-analysis techniques, such as Khatami et al. (2016) and O. Hall et al. (2023). In an unweighted meta-analysis all studies are treated equally regardless of their sample size, quality, or variance (J. A. Hall & Rosenthal, 2018). However, it is more common in traditional applications of meta-analysis, to use the sample sizes when aggregating individual studies (J. A. Hall & Rosenthal, 2018). However, to my knowledge, no examples of a weighted meta-analysis applied to predictive performance in remote sensing data have been conduced, highlighting a gap that this study aims to address.\n\n\n\n\n\nAnshuka, A., Ogtrop, F. F. van, & Willem Vervoort, R. (2019). Drought forecasting through statistical models using standardised precipitation index: A systematic review and meta-regression analysis. Natural Hazards, 97(2), 955–977. https://doi.org/10.1007/s11069-019-03665-6\n\n\nBurke, M., Driscoll, A., Lobell, D. B., & Ermon, S. (2021). Using satellite imagery to understand and promote sustainable development. Science, 371(6535), eabe8628. https://doi.org/10.1126/science.abe8628\n\n\nCampbell, McKenzie, J. E., Sowden, A., Katikireddi, S. V., Brennan, S. E., Ellis, S., Hartmann-Boyce, J., Ryan, R., Shepperd, S., Thomas, J., Welch, V., & Thomson, H. (2020). Synthesis without meta-analysis (SWiM) in systematic reviews: reporting guideline. BMJ, 368, l6890. https://doi.org/10.1136/bmj.l6890\n\n\nCampbell, & Wynne, R. H. (2011). Introduction to remote sensing (5th ed). Guilford Press.\n\n\nEkmen, O., & Kocaman, S. (2024). Remote sensing for UN SDGs: A global analysis of research and collaborations. The Egyptian Journal of Remote Sensing and Space Sciences, 27(2), 329–341. https://doi.org/10.1016/j.ejrs.2024.04.002\n\n\nFAO, F. and A. O. (2016). Map accuracy assessment and area estimation practical guide., http://www.fao.org/3/a-i5601e.pdf\n\n\nHaddaway, N. R., Bannach-Brown, A., Grainger, M. J., Hamilton, W. K., Hennessy, E. A., Keenan, C., Pritchard, C. C., & Stojanova, J. (2022). The evidence synthesis and meta-analysis in R conference (ESMARConf): Levelling the playing field of conference accessibility and equitability. Systematic Reviews, 11(1), 113. https://doi.org/10.1186/s13643-022-01985-6\n\n\nHall, J. A., & Rosenthal, R. (2018). Choosing between random effects models in meta-analysis: Units of analysis and the generalizability of obtained results. Social and Personality Psychology Compass, 12(10), e12414. https://doi.org/10.1111/spc3.12414\n\n\nHall, O., Dompae, F., Wahab, I., & Dzanku, F. M. (2023). A review of machine learning and satellite imagery for poverty prediction: Implications for development research and applications. Journal of International Development, 35(7), 1753–1768. https://doi.org/10.1002/jid.3751\n\n\nKhatami, R., Mountrakis, G., & Stehman, S. V. (2016). A meta-analysis of remote sensing research on supervised pixel-based land-cover image classification processes: General guidelines for practitioners and future research. Remote Sensing of Environment, 177, 89–100. https://doi.org/10.1016/j.rse.2016.02.028\n\n\nLavallin, A., & Downs, J. A. (2021). Machine learning in geography–Past, present, and future. Geography Compass, 15(5), e12563. https://doi.org/10.1111/gec3.12563\n\n\nMaso, J., Zabala, A., & Serral, I. (2023). Earth Observations for Sustainable Development Goals. Remote Sensing, 15(10), 2570. https://doi.org/10.3390/rs15102570\n\n\nNASA. (2019). What is Remote Sensing? https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing\n\n\nOwers, C. J., Lucas, R. M., Clewley, D., Tissott, B., Chua, S. M. T., Hunt, G., Mueller, N., Planque, C., Punalekar, S. M., Bunting, P., Tan, P., & Metternicht, G. (2022). Operational continental-scale land cover mapping of Australia using the Open Data Cube. International Journal of Digital Earth, 15(1), 1715–1737. https://doi.org/10.1080/17538947.2022.2130461\n\n\nSEOS. (2014). Introduction to remote sensing. https://seos-project.eu/remotesensing/remotesensing-c01-p06.html\n\n\nStehman, S. V., & Foody, G. M. (2019). Key issues in rigorous accuracy assessment of land cover products. Remote Sensing of Environment, 231, 111199. https://doi.org/10.1016/j.rse.2019.05.018\n\n\nThapa, A., Horanont, T., Neupane, B., & Aryal, J. (2023). Deep Learning for Remote Sensing Image Scene Classification: A Review and Meta-Analysis. Remote Sensing, 15(19), 4804. https://doi.org/10.3390/rs15194804\n\n\nUCS. (2021). Union of Concerned Scientists (UCS) Satellite Database. https://www.ucsusa.org/resources/satellite-database\n\n\nUN-GGIM:Europe. (2019). The territorial dimension in SDG indicators: Geospatial data analysis and its integration with statistical data. Instituto Nacional de Estatística. https://un-ggim-europe.org/wp-content/uploads/2019/05/UN_GGIM_08_05_2019-The-territorial-dimension-in-SDG-indicators-Final.pdf\n\n\nYin, C., Peng, N., Li, Y., Shi, Y., Yang, S., & Jia, P. (2023). A review on street view observations in support of the sustainable development goals. International Journal of Applied Earth Observation and Geoinformation, 117, 103205. https://doi.org/10.1016/j.jag.2023.103205\n\n\nZhang, C., & Li, X. (2022). Land Use and Land Cover Mapping in the Era of Big Data. Land, 11(10), 1692. https://doi.org/10.3390/land11101692\n\n\nZhang, Y., Liu, J., & Shen, W. (2022). A Review of Ensemble Learning Algorithms Used in Remote Sensing Applications. Applied Sciences, 12(17), 8654. https://doi.org/10.3390/app12178654",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html",
    "href": "chapters/methods.html",
    "title": "3  Methods",
    "section": "",
    "text": "Specific inclusion and exclusion criteria\nAfter removing review articles and non-research papers, a total of 811 relevant articles remained. Of these potentially relevant papers, 35% were published in 2023, highlighting the growth of research in this field. The trend, as illustrated in Figure 3.1, is consistent with other similar research, for example, Ekmen & Kocaman (2024), which reported a sharp increase in publications related to machine learning and remote sensing for SDG monitoring.\nDue to the large number of papers remaining, a random sample of 200 articles was drawn for title and abstract screening. These potentially relevant articles were screened independently by three reviewers (the author and two internal supervisors) using the R package metagear (Lajeunesse, 2016). The papers were selected according to the following criteria: a) publications utilizing remote sensing and machine learning techniques, (b) indication of a quality assessment for example overall accuracy. Table 3.2 shows the words highlighted in the abstract screening phase to aid the reviewers and Figure 3.2 shows the user interface highlighting these keywords.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html#formulating-the-review-question-and-protocol",
    "href": "chapters/methods.html#formulating-the-review-question-and-protocol",
    "title": "3  Methods",
    "section": "Formulating the review question and protocol",
    "text": "Formulating the review question and protocol\nThe PICOTS (population, intervention, comparison, outcome, timing, and setting) system was used to frame the review aims for this analysis (Debray et al., 2017). As outlined by Table 3.1, the using this framework, the research question was formulated to examine: (1) the overall performance (2) and heterogeneity of machine learning models applied to remote sensing in the context of SDGs, and (3) to assess the influence of specific study features on model performance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html#specific-inclusion-and-exclusion-criteria",
    "href": "chapters/methods.html#specific-inclusion-and-exclusion-criteria",
    "title": "3  Methods",
    "section": "",
    "text": "Code\nggplot(subset(citations, !is.na(Publication.Year)))+\n      geom_bar(mapping = aes(x = as.factor(Publication.Year)), \n                     fill = \"#001158\", alpha = .8) +\n    labs(x = \"Publication Year\", \n       y = \"Number of Reports\", \n       title = \"Number of publications in machine leaning in remote sensing.\") +\n    theme(plot.title = element_text(size=10),\n        axis.title.x = element_text(size=8),\n        axis.title.y = element_text(size=8), \n        axis.text = element_text(size=8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: Publication increase between 2018 and 2022.\n\n\n\n\n\nCode\nread_excel(\"figures/summary_tables.xlsx\", sheet = \"keywords\")|&gt;\n  kable(booktabs = TRUE, linesep = \"\") |&gt;\n  kable_styling(font_size = 10, \n                full_width = FALSE)|&gt;\n  row_spec(c(0),  bold = TRUE)|&gt;\n  column_spec(1, width = \"3cm\")|&gt;\n  column_spec(2, width = \"11cm\")\n\n\n\n\nTable 3.2: Keywords highlighted by the metagear user interface during abstract screening phase as a visual cue to speed up the screening process\n\n\n\n\n\n \n  \n    Category \n    Keywords \n  \n \n\n  \n    General \n    empirical, result, predictive, analysis, sustainable development goal, sustainable development \n  \n  \n    Data related \n    remotely sensed, remote sensing, satellite, earth observation \n  \n  \n    Models \n    deep learning, machine learning, classification, classifier, regression, supervised, test set, training set, cart, svm, rf, ann, random forest, support vector machine, regression tree, decision tree, neural network, boosting, bagging, gradient, bayes \n  \n  \n    Quality metrics \n    overall accuracy, accuracy, coefficient of determination, rmse, mse, f1, precision, auc, roc, recall, sensitivity, specificity, mean absolute error, error, mae \n  \n  \n    To omit \n    systematic review, meta-analysis, review \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Metagear graphical user interface: Example of the metagear abstract screener interface highlighting keywords. On the bottom left the reviewer can select whether the paper is relevant.\n\n\n\n\n\nAs shown in Figure 3.3, of the 200 abstracts screened 57 were deemed potentially relevant by all three reviewers. To have comparable performance metrics it decided to focus on papers related to classification. The titles and abstracts of the 57 articles were screened using metagear dividing them to classification (40) and regression (17) papers. In the 40 papers, overall accuracy was the most commonly reported outcome metric and therefore it was decided to include all papers that report overall accuracy.\n\n\n\n\n\n\n\nFigure 3.3: PRIMSA flow diagram of manuscript selection. The records were identified from databases including Web of Science (WOS), ScienceDirect, PubMed, Journal Storage (JSTOR), American Geophysical Union Publications (Agupubs), EBSCO, IEEE Xplore, Multidisciplinary Digital Publishing Institute (MDPI), ProQuest, and Taylor & Francis Online (Tandfonline), no papers were gathered from official registers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html#feature-collection",
    "href": "chapters/methods.html#feature-collection",
    "title": "3  Methods",
    "section": "Feature collection",
    "text": "Feature collection\nUsing the selected papers and previous systematic reviews a list of potential study features was created and structured in a table for data extraction. Table 3.3 outlines all the extracted features and study identification information. The features in the table are grouped according to their use in the analysis. The most frequently reported performance metric, overall accuracy is used as the effect size of interest. The sample size (\\(m\\)) is important for the weighted meta-analysis and was also used as a feature, as larger sample sizes should influence overall accuracy. The other features describe the methodology and data characteristics, which provide information about the complexity of the classification tasks (e.g., the number of output classes) and the proportion of the majority class, indicating potential class imbalance issues that can affect the performance of classification models. Remote sensing-specific information was also gathered, including the type of devices, spectral bands, and spatial resolution to assess how data collection impacts performance. Of the extracted features, the number of spectral bands and spatial resolution were categorized due to high levels of non-reporting. The type of remote sensing device was excluded because only one study did not use satellite data, and the specifics of the spectral bands used were too different to make meaningful groups. Several potentially useful features were not recorded, including temporal resolution (the frequency of data collection) and pre-processing steps, which also impact the performance of the model. These were excluded as the differences between papers were too large to make groups. The number of citations was gathered using the Local Citation Network web app, which collects article metadata from OpenAlex—a bibliographic catalogue of scientific papers (Priem et al., 2022)1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html#statistical-analysis",
    "href": "chapters/methods.html#statistical-analysis",
    "title": "3  Methods",
    "section": "Statistical analysis",
    "text": "Statistical analysis\nA meta-analysis is a statistical method that aggregates results from several primary studies to assess and interpret the collective evidence on a specific topic or research question. Specifically, the aim is to (a) determine the summary effect, (b) establish the degree of heterogeneity between effect sizes, and (c) access if study characteristics can explain any of the heterogeneity of the effect sizes (Cheung, 2014). In this case the effect size (dependent variable) of interest is the overall accuracy. Let \\(\\hat{\\theta}_{ij}\\) be the \\(i-\\)th observed effect size in study \\(j\\) (where \\(i = 1, ..., k_j\\), \\(j = 1, ..., n\\)). From Equation 2.1, the overall accuracy is the proportion of correctly classified instances, therefore, the effect size is:\n\\[\n\\begin{aligned}\n&\\ \\hat{\\theta}_{ij} = \\frac{s_{ij}}{m_{ij}}\\\\\n&\\ v_{ij} = \\frac{\\hat{\\theta}_{ij}(1-\\hat{\\theta}_{ij})}{m_{ij}}\n\\end{aligned}\n\\tag{3.1}\\]\n\\(s_{ij}\\) is the number of successful predictions and \\(m_{ij}\\) is total number of pixels or objects classified, and \\(v_{ij}\\) is the variance.\n\nWeighted Approach\nBefore conducting the meta-analysis, first the structure of the collected data and assumption of independence of effect sizes need to be addressed. In the context of this research, dependencies are introduced since all reported effect sizes from each study are included. The degree of dependence between effect sizes can be categorized as either known or unknown (Cheung, 2014). Multivariate meta-analytic techniques use known dependencies reported in the primary studies, such as reported correlation coefficients (Cheung, 2014). However, dependency estimates between outcomes are rarely reported (Assink & Wibbelink, 2016). Therefore, to model these unknown dependencies a 3-level random-effects meta-analytic model is used (Cheung, 2014). The three-level meta-analysis approach models three different variance components distributed over three levels:\nAt level 1, the sampling variance of the effect sizes is modelled as: \\[\n\\begin{aligned}\n&\\ \\text{Level 1:  } \\hat{\\theta}_{ij} = \\theta_{ij} + \\epsilon_{ij}, \\\\\n&\\  \\epsilon_{ij} \\sim \\mathcal{N}(0, v_{ij}).\\\\\n\\end{aligned}\n\\tag{3.2}\\]\nThe observed overall accuracy \\(\\hat{\\theta}_{ij}\\) is an estimate of overall accuracy from experiment \\(i\\) in study \\(j\\) and is modelled as the true overall accuracy, \\(\\theta_{ij}\\) and error component \\(\\epsilon_{ij}\\) which is normally distributed with mean \\(0\\) and known variance \\(v_{ij}\\). A model that only takes into account sampling variance is referred to as a fixed-effects model, where it is assumed that all studies included in the meta-analysis share a single true effect size, and therefore, the only source of variation between effect sizes is the sampling variance. The fixed-effects model assumes homogeneity across studies and allows for conditional inference about the specific set of studies included in the analysis, without accounting for variability that might arise from differences between studies. The inclusion of the random effects (at level 2 and 3) means that as well as sampling variance, the heterogeneity due to differing between and within study features are also taken into account (Harrer et al., 2022; Schwarzer et al., 2015, p. 34; Wang, 2023). Therefore, the addition random effect components allow one to make unconditional inferences about the population from which the included studies are a random sample.\nAt level 2, within-study heterogeneity (\\(\\sigma^2_{\\text{level2}}\\)) is modelled as: \\[\n\\begin{aligned}\n&\\ \\text{Level 2:  } \\theta_{ij} = \\kappa_j + \\zeta_{ij}, \\\\\n&\\  \\zeta_{ij} \\sim \\mathcal{N}(0, \\sigma^2_{\\text{level2}}).\\\\\n\\end{aligned}\n\\tag{3.3}\\]\nThe true overall accuracy \\(\\theta_{ij}\\), is modelled as the average overall accuracy \\(\\kappa_{j}\\) of study \\(j\\) and study-specific heterogeneity \\(\\zeta_{ij}\\) which is normally distributed with mean \\(0\\) and variance \\(\\sigma^2_{\\text{level2}}\\).\nLastly, level 3, the variance between heterogeneity (\\(\\sigma^2_{\\text{level3}}\\)) is modelled as: \\[\n\\begin{aligned}\n&\\ \\text{Level 3:  } \\kappa_j = \\mu + \\xi_{j}, \\\\\n&\\  \\xi_{j} \\sim \\mathcal{N}(0, \\sigma^2_{\\text{level3}}).\\\\\n\\end{aligned}\n\\tag{3.4}\\]\nThe average overall accuracy \\(\\kappa_{j}\\) of study \\(j\\) is modelled as the average population effect \\(\\mu\\) and between-study heterogeneity \\(\\xi_{j}\\), which is normally distributed with mean \\(0\\) and variance \\(\\sigma^2_{\\text{level3}}\\). Combined, the three-level meta-analysis models the observed effect size modelled as the sum of the average population effect \\(\\mu\\) and these three error components: \\[\n\\hat{\\theta}_{ij} = \\mu + \\xi_j + \\zeta_{ij} + \\epsilon_{ij}.\n\\tag{3.5}\\]\nFor the expected value of the observed effect size to be the population average, \\(\\mathbb{E}(\\hat{\\theta}_{ij}) = \\mu\\), the random effects at the different levels and the sampling variance are assumed independent: \\(\\text{Cov}(\\xi_j, \\zeta_{ij}) = \\text{Cov}(\\xi_j, \\epsilon_{ij}) = \\text{Cov}(\\zeta_{ij}, \\epsilon_{ij}) = 0\\). Therefore, unconditional sampling variance of the effect size is the sum of level 3 and level 2 heterogeneity, and the known sampling variance: \\(\\text{Var}(\\hat{\\theta}_{ij}) = \\sigma^2_{\\text{level3}} +\\sigma^2_{\\text{level2}} + v_{ij}\\), the effect sizes within the same study share the same covariance \\(\\text{Cov}(\\hat{\\theta}_{ij}, \\hat{\\theta}_{lj}) = \\sigma^2_{\\text{level3}}\\), and the effect sizes in different studies are independent \\(\\text{Cov}(\\hat{\\theta}_{ij}, \\hat{\\theta}_{zu}) = 0\\) (Cheung, 2014)2.\nThe random-effects model can be extended to a mixed-effects model (also referred to as a meta-regression) by including study features as covariates (predictors). Let \\(x\\) denote the value covariate, where \\(b'\\) refers to the number of covariates included in the model. These covariates can be either \\(x_{ij}\\) for a level-2 covariate or \\(x_j\\) for a level-3 covariate. The mixed-effect model defined as: \\[\n\\hat{\\theta}_{ij} = \\mu + \\beta_1 x_{ij1} + .... + \\beta_{b'} x_{jb'} + \\xi_j + \\zeta_{ij} + \\epsilon_{ij}\n\\tag{3.6}\\]\nThe assumptions for Equation 3.6 remain the same as Equation 3.5, but the heterogeneity (\\(\\sigma^2_{\\text{level3}}, \\sigma^2_{\\text{level2}}\\)) is the variability among the true effects which is not explained by the included covariates (Cheung, 2014; Viechtbauer, 2010). The aim of the mixed-effects model is to examine the extent to which the included covariates in the model influence the overall summary effect (population average) \\(\\mu\\) and the heterogeneity \\(\\sigma^2_{\\text{level3}}\\) and \\(\\sigma^2_{\\text{level2}}\\) (Viechtbauer, 2010). Figure 3.4 illustrates this structure of the three-level random-effects meta-analysis model used to account for both within-study and between-study heterogeneity.\n\n\n\n\n\n\n\nFigure 3.4: Three-level random-effects meta-analysis model. At level 1, observed effects \\(\\hat{\\theta}_{ij}\\) are modeled with known sampling variance \\(v_{ij}\\), where larger sample sizes \\(m_{ij}\\) have smaller sampling variances, represented by the narrower distribution around \\(\\hat{\\theta}_{1j}\\) compared to \\(\\hat{\\theta}_{2j}\\). At level 2, the true effects \\(\\theta_{ij}\\), from each study are modeled as normally distributed with mean \\(\\kappa_{j}\\) and within-study variance \\(\\sigma^2_{\\text{level2}}\\). Lastly, at level 3, study average effects are modeled as normally distributed with mean \\(\\mu\\) and between-study variance \\(\\sigma^2_{\\text{level3}}\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/methods.html#footnotes",
    "href": "chapters/methods.html#footnotes",
    "title": "3  Methods",
    "section": "",
    "text": "The idea to add the number of citations was added after the analysis was mostly completed. This suggestion was made during a discussion of the project after the preliminary results were presented to the methodology team at the CBS.↩︎\nLike \\(i\\), \\(l\\) refers to an effect size within the same study \\(j\\). \\(z\\) and \\(u\\) refer to effect sizes in different clusters, where \\(u \\neq j\\) effect sizes are independent.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "chapters/results.html",
    "href": "chapters/results.html",
    "title": "4  Results",
    "section": "",
    "text": "Descriptive Statistics\nCode\nsource(\"../appendix/packages.R\")\nsource(\"../appendix/feature_labels.R\")\n# final dataset, for data wrangling see appendix\nmy_data &lt;- read.csv(\"../data/analysis_df.csv\")\nCode\n# weights and transformation \n# Function to calculate various effect sizes and to estimate the variance \n## for the traditional meta analysis\n\nies.da  &lt;- escalc(xi = event , ni = total , data = my_data ,\n               measure = \"PFT\",  # FT double  arcsine  transformation\n               slab=paste(AuthorYear, \" Estimate \", esid)\n               ) \n#the ies: individual effect size, new variables:\n## yi: FTT effect sizes\n## vi: calculated variances \nies.da$ancillary &lt;- relevel(factor(ies.da$ancillary), ref = \"Remote Sensing Only\")\nies.da$indices &lt;- relevel(factor(ies.da$indices), ref = \"Not Used\")\nCode\nshapiro_test_result &lt;- round(shapiro_test(ies.da$yi)[, 2:3], 2)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/results.html#descriptive-statistics",
    "href": "chapters/results.html#descriptive-statistics",
    "title": "4  Results",
    "section": "",
    "text": "A total of \\(n = 20\\) studies with \\(k = 86\\) effect sizes were included in this analysis, with each primary study reported between one and 27 results (\\(1 \\leq k_j \\leq 27\\)). The research area of these studies span 18 countries, Figure 4.1 shows a map indicating the location of each effect size. These primary studies were grouped into three different SDG goals: SDG 2 Zero Hunger, SDG 11 Sustainable Cities, and SDG 15 Life on Land.\n\n\nCode\nsf_world &lt;- ne_countries(returnclass = \"sf\")\n\n# Get the world map data with country names, latitude, and longitude\nworld_map &lt;- map_data(\"world\") \n \nSDG_colours &lt;- c(\n    \"SDG2: Zero Hunger\" = \"#E8B700\",  \n    \"SDG11: Sustainable Cities\" = \"#FF7518\",  \n    \"SDG15: Life on Land\" = \"#2C9F2C\" )  \n\nstudy_map &lt;- world_map |&gt;\n  group_by(region) |&gt;\n  summarize(lat = mean(lat), long = mean(long), .groups = 'drop')|&gt;\n  right_join(subset(my_data, esid ==1), join_by(\"region\"==\"location\"))\n \nggplot(sf_world) + \n  geom_sf(fill = \"#f4f3f1\")+\n  coord_sf(ylim = c(-55,80)) +\n  geom_jitter(data = study_map, aes(x = long, y = lat, \n                                    fill = SDG_theme), \n              size = 2, \n              width = 5,\n              colour = \"transparent\",\n              alpha = 0.8,\n              height = 0,\n              shape = 25,\n              show.legend = FALSE) +\n  labs(title = \"(a) Map of researched locations\", \n       x = NULL, \n       y = NULL) +\n  scale_fill_manual(values = SDG_colours ) +\n  common_theme +\n   theme(\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    panel.border = element_blank(),\n    plot.margin = margin(0, 0, 0, 0)\n  )\nggplot(ies.da, aes(x = reorder(AuthorYear, OA_reported), y = OA_reported, \n               colour = SDG_theme\n               ))+\n  geom_point(alpha = 0.2, size = 1.5)+\n  stat_summary(geom = \"point\", fun = \"mean\", \n                 size = 2, shape = 17)+\n  labs(x = NULL, \n       y = \"Observed Overall Accuracy\", \n       title = \"(B) Reported overall accuracy by study\") +\n  scale_colour_manual(values = SDG_colours ) +\n  guides(colour=guide_legend(title= NULL, nrow=1, byrow=TRUE, \n                             override.aes = list(shape = 16, size = 2.5)\n                             )\n         )+\n  coord_flip() +\n  common_theme+\n  theme(# grid lines \n        panel.grid.major.y = element_line(linewidth = 0.1, colour = \"grey80\"),\n       # legend specs\n        legend.position = \"bottom\",\n        legend.key.size = unit(0, 'lines'), \n        legend.key.spacing.x = unit(-2.5, \"lines\"),\n        legend.text= element_text(size=8, margin = margin(l = -0.3, r = 1.5, unit = \"cm\")), \n       plot.margin = margin(t= 2, b = 0, r= 0.1, l =0, \"cm\"), \n       legend.box.margin = margin(t= -0.4, b = 0, r= 0, l =0, \"cm\")\n       )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.1: Top, shows primary study location map and bottom the range of reported overall accuracy. With individual overall accuracies shown as points and mean overall accuracy represented by triangles. Both the map and points are colour-coded by SDG goal.\n\n\n\n\n\n@#fig-map_range (b) and Table 4.1 (bellow) show, the reported overall accuracies are not centered around 0.5. Therefore, a transformation is required. Figure 4.2 shows the distribution of observed overall accuracy as well as the logit and FT transformation values. FT visually performs better than the Logit transformation. However the Shapiro-Wilk Normality test shows that the distribution of the FT transformed overall accuracy still departed significantly from normality (\\(W =\\) 0.93, p-value &lt; 0.01). Nevertheless, conducting a meta-analysis remains justified, as these statistical models are generally robust against violations of normality (McCulloch & Neuhaus, 2011).\n\n\nCode\nraw_propotions &lt;- ggplot(ies.da, aes(x = OA_reported))+\n  geom_histogram(bins = 30, fill = \"#001158\", alpha = .8) +\n  labs(x = \"Observed Overall Accuracy\", y = \"Count\") +\n  common_theme\n  \n\nlogit_transformation &lt;- ggplot(ies.da, aes(x = log(OA_reported/(1-OA_reported))))+\n  geom_histogram(bins = 30, fill = \"#001158\", alpha = .8)+\n  labs(x = \"Logit-transformed Overall Accuracy\", y = NULL) +\n  common_theme\n\narcsin_transformation&lt;- ggplot(ies.da, aes(x = yi))+\n  geom_histogram(bins = 30, fill = \"#001158\", alpha = .8)+\n  labs(x = \"FT-transformed Overall Accuracy\", y = NULL)+\n  common_theme\n\nraw_propotions + logit_transformation+ arcsin_transformation +\n  plot_annotation(title = \"Density Plots of Observed and Transformed Overall Accuracy\", \n                  theme = theme(plot.title = element_text(size = 10)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.2: Distribution of the observed overall accuracy and transformed by logit and FT transformation.\n\n\n\n\nTable 4.1 summarises the overall accuracy (effect size of interest), study sample size and the collected study features, including the study features such as sample size, overall accuracy, types of machine learning models used and SDG goal targeted. For the meta-analysis the range of the sample size (259 - 75782016) and overall accuracy (0.6504 - 1) are of importance. Most studies used Neural Networks (48%), followed by Tree-Based Models (45%), and a small portion used other types of models (7%). Regarding SDGs, 44% of the studies aimed at SDG 11 (Sustainable Cities), 43% targeted SDG 15 (Life on Land), and 13% focused on SDG 2 (Zero Hunger).\n\n\n\nCode\nies.da |&gt;\n  select(\n    OA_reported,\n    total,\n    globalCitationsCount,\n    number_classes,\n    fraction_majority_class,\n    Publication.Year,\n    SDG_theme,\n    classification_type, \n    model_group, \n    ancillary,\n    indices,\n    RS_device_type,\n    RS_device_group,\n    no_band_group,\n    RS_spatital_res_grouped, \n    Confusion_matrix\n  ) |&gt; \n  tbl_summary(\n    statistic = list(\n      all_continuous() ~ \"{mean} ({min} - {max})\",\n      all_categorical() ~ \"{n} ({p}%)\"\n    ),\n    label = feature_labels,\n    missing = \"no\"\n  ) |&gt;\n  modify_header(label = \"Feature\") |&gt;\n  modify_header(all_stat_cols() ~ \"Statistic\") |&gt;\n  as_kable(\n    booktabs = TRUE,\n    longtable = TRUE, \n    linesep = \"\",\n    escape = TRUE\n  )|&gt;\n  kable_styling(font_size = 9, \n                full_width = FALSE)|&gt;\n  pack_rows(\"Study Features\", 2, 46, latex_gap_space = \"0.1em\" #, hline_before = T\n            )|&gt;\n  pack_rows(\"Numeric$^b$\", 2, 5, latex_gap_space = \"0em\")|&gt;\n  pack_rows(\"Categorical$^c$\", 6,46, latex_gap_space = \"0em\")|&gt;\n   row_spec(c(0),  bold = TRUE)|&gt;\n  column_spec(1, width = \"7cm\")|&gt;\n  column_spec(2, width = \"6cm\")|&gt;\n  add_indent(c(7:12, 14:16, 18:20, 22:24, 26:27, \n               29:30, 32:35,37:40,42:44,\n               46:48), \n               level_of_indent = 1.5)|&gt;\n  footnote(alphabet = c(\"Effect size of interest. $^a$For numeric predictors: mean (min - max) and $^b$ for categorical variables number of effect sizes (%)\"), \n               threeparttable=TRUE, escape = FALSE)\n\n\n\n\nTable 4.1: Summary table\n\n\n\n\n\n \n  \n    Feature \n    Statistic \n  \n \n\n  \n    Overall Accuracy \n    0.90 (0.65 - 1.00) \n  \n  Study Features\nNumeric$^b$\n\n    Sample Size \n    6,401,352 (259 - 75,782,016) \n  \n  \n    Number of Citations \n    15 (2 - 68) \n  \n  \n    Number of Classes \n    4 (2 - 13) \n  \n  \n    Majority-class Proportion \n    0.72 (0.14 - 1.00) \n  \n  Categorical$^c$\n\n    Publication Year \n     \n  \n  \n    2018 \n    7 (8.1%) \n  \n  \n    2019 \n    4 (4.7%) \n  \n  \n    2020 \n    30 (35%) \n  \n  \n    2021 \n    6 (7.0%) \n  \n  \n    2022 \n    13 (15%) \n  \n  \n    2023 \n    26 (30%) \n  \n  \n    SDG Theme \n     \n  \n  \n    SDG11: Sustainable Cities \n    38 (44%) \n  \n  \n    SDG15: Life on Land \n    37 (43%) \n  \n  \n    SDG2: Zero Hunger \n    11 (13%) \n  \n  \n    Classification Type \n     \n  \n  \n    Object-level \n    46 (53%) \n  \n  \n    Pixel-level \n    36 (42%) \n  \n  \n    Unclear \n    4 (4.7%) \n  \n  \n    Model Group \n     \n  \n  \n    Neural Networks \n    41 (48%) \n  \n  \n    Other \n    6 (7.0%) \n  \n  \n    Tree-Based Models \n    39 (45%) \n  \n  \n    Ancillary Data \n     \n  \n  \n    Remote Sensing Only \n    71 (83%) \n  \n  \n    Ancillary Data Included \n    15 (17%) \n  \n  \n    Indices \n     \n  \n  \n    Not Used \n    23 (27%) \n  \n  \n    Used \n    63 (73%) \n  \n  \n    Remote Sensing Type \n     \n  \n  \n    Active \n    11 (13%) \n  \n  \n    Combined \n    7 (8.1%) \n  \n  \n    Not Reported \n    7 (8.1%) \n  \n  \n    Passive \n    61 (71%) \n  \n  \n    Device Group \n     \n  \n  \n    Landsat \n    15 (17%) \n  \n  \n    Not Reported \n    7 (8.1%) \n  \n  \n    Other \n    44 (51%) \n  \n  \n    Sentinel \n    20 (23%) \n  \n  \n    Number of Spectral Bands \n     \n  \n  \n    Low \n    18 (21%) \n  \n  \n    Mid \n    26 (30%) \n  \n  \n    Not Reported \n    42 (49%) \n  \n  \n    Spatial Resolution \n     \n  \n  \n    &lt;1 metre \n    7 (8.1%) \n  \n  \n    10-30 metres \n    39 (45%) \n  \n  \n    Not Reported \n    40 (47%) \n  \n  \n    Confusion Matrix \n     \n  \n  \n    Not Reported \n    23 (27%) \n  \n  \n    Reported \n    63 (73%) \n  \n\n\na Effect size of interest. $^a$For numeric predictors: mean (min - max) and $^b$ for categorical variables number of effect sizes (%)\n\n\n\n\n\n\n\n\nCode\n# common aesthetics \nbox_theme &lt;- common_theme + theme(axis.title.y = element_blank())\n\npoint_color &lt;- \"#001158\"\nylab &lt;- c(\"FT Transformed Overall Accuracy\")\n\n# Scatter plots for continuous variables\n\nsample_size &lt;- ggplot(ies.da, aes(x = log10(total), y = yi)) +\n  geom_point(size = 1, alpha = 0.8, colour = point_color) +\n  labs(title = feature_labels$total, \n       y = ylab, \n        x = bquote(\"Sample Size:\"~log[10](m[ij])), \n       #caption = stringr::str_wrap(\"Note: Log transformation was applied to handle large variation in sample sizes\", width = 42)\n       ) +\n   #expand_limits(x = 0)+\n\n  common_theme  #+\n  #theme(plot.caption = element_text(size = 7.5, hjust = 0, color = \"gray30\", \n   #                                 margin = margin(t = 5, r = 0, b = 20, l = 0))) \n\nnumber_class &lt;- ggplot(ies.da, aes(x = number_classes, y = yi)) +\n  geom_point(size = 1, alpha = 0.8, colour = point_color) +\n  labs(title = feature_labels$number_classes, y = ylab, x = \"Classes\") + \n  scale_x_continuous(breaks=seq(0, 13, 2))+\n   expand_limits(x = 0)+\n  common_theme \n\nfraction_majority_class &lt;- ggplot(ies.da, aes(x = fraction_majority_class, y = yi)) +\n  geom_point(size = 1, alpha = 0.8, colour = point_color) +\n  labs(title = feature_labels$fraction_majority_class, \n       y = ylab, \n       x = \"Proportion\") +\n   expand_limits(x = 0)+\n  common_theme \n\ncits &lt;- ggplot(ies.da, aes(x = globalCitationsCount , y = yi)) +\n  geom_point(size = 1, alpha = 0.8, colour = point_color) +\n  labs(title = feature_labels$globalCitationsCount, \n       y = ylab, \n       x = \"Citation\") + \n   expand_limits(x = 0)+\n  common_theme \n\n#######################\n# Boxplots for categorical variables\nmodelgroup_box &lt;- ggplot(ies.da, aes(y = yi, \n                                     x = factor(model_group, \n                                                levels = c(\"Other\", \n                                                           \"Neural Networks\", \n                                                           \"Tree-Based Models\")))) +\n  geom_boxplot() +\n  geom_jitter(colour = point_color, alpha = 0.7) +\n  scale_x_discrete_wrap() +\n  labs(title = feature_labels$model_group, \n       y = NULL) +  # patchwork collect x.axis not being agreeable manual selection \n  coord_flip() + box_theme\n\nclasstype_box &lt;- ggplot(ies.da, aes(y = yi, x = fct_rev(classification_type))) +\n  geom_boxplot() +\n  geom_jitter(colour = point_color, alpha = 0.7) +\n  labs(title = feature_labels$classification_type, \n       y = ylab) +\n  coord_flip() + box_theme\n\nindexes_box &lt;- ggplot(ies.da, aes(y = yi, x = indices)) +\n  geom_boxplot() +\n  geom_jitter(colour = point_color, alpha = 0.7) +\n  labs(title = feature_labels$indices, y = ylab) +\n  coord_flip() + box_theme\n\nancillary_box &lt;- ggplot(ies.da, aes(y = yi, x = ancillary)) +\n  geom_boxplot() +\n  geom_jitter(colour = point_color, alpha = 0.7) +\n  scale_x_discrete_wrap() +\n  labs(title = feature_labels$ancillary, y = NULL) +\n  coord_flip() + box_theme\n\n# Data about RS devices and features\nRStype_box &lt;- ggplot(ies.da, aes(y = yi, x = factor(RS_device_type,\n                                                     levels = c(\"Not Reported\", \"Combined\", \"Passive\",\"Active\")))) +\n  geom_boxplot() +\n  geom_jitter(colour = point_color, alpha = 0.7) +\n  scale_x_discrete_wrap() +\n  labs(title = feature_labels$RS_device_type, y = NULL) +\n  coord_flip() + box_theme\n\nRSgroup_box &lt;- ggplot(ies.da, aes(y = yi, \n                                  x = factor(RS_device_group,\n                                          levels = c(\"Not Reported\", \"Other\", \"Sentinel\",\"Landsat\")))) +\n  geom_boxplot() +\n  geom_jitter(colour = point_color, alpha = 0.7) +\n  labs(title = feature_labels$RS_device_group, y = NULL) +\n  coord_flip() + box_theme\n\nRS_resolution_box &lt;- ggplot(ies.da, aes(y = yi, x = fct_rev(RS_spatital_res_grouped))) +\n  geom_boxplot() +\n  geom_jitter(colour = point_color, alpha = 0.7) +\n  labs(title = feature_labels$RS_spatital_res_grouped, y = ylab) +\n  coord_flip() + box_theme\n\nSDG_theme_box &lt;- ggplot(ies.da, \n                        aes(y = yi, x = factor(SDG_theme, \n                                               levels = c(\"SDG2: Zero Hunger\",\n                                                          \"SDG11: Sustainable Cities\", \n                                                          \"SDG15: Life on Land\")))) +\n  geom_boxplot() +\n  geom_jitter(colour = point_color, alpha = 0.7) +\n  scale_x_discrete_wrap() +\n  labs(title = feature_labels$SDG_theme, y = NULL) +\n  coord_flip() + box_theme\n\n\npublication_year_box &lt;- ggplot(ies.da, aes(x = as.factor(Publication.Year), y = yi)) +\n  geom_boxplot() +\n  geom_jitter(colour = point_color, alpha = 0.7) +\n  labs(title = feature_labels$Publication.Year, y = ylab) +\n  coord_flip() + box_theme\n\n\nno_band_group_box &lt;- ggplot(ies.da, aes(y = yi, x = fct_rev(no_band_group))) +\n  geom_boxplot() +\n  geom_jitter(colour = point_color, alpha = 0.7) +\n  labs(title = feature_labels$no_band_group, y = ylab) +\n  coord_flip() + box_theme\n\nconfusion_matrix_box &lt;- ggplot(ies.da, aes(y = yi, x = Confusion_matrix)) +\n  geom_boxplot() +\n  geom_jitter(colour = point_color, alpha = 0.7) +\n  labs(title = feature_labels$Confusion_matrix, y = NULL) +\n  coord_flip() + box_theme\n\n\nCode\n# layout areas \ndesign &lt;- c(\n  area(1, 1, 2, 1),\n  area(1, 2),\n  area(1, 3),\n  area(1, 4),\n  area(2, 2),\n  area(2, 3),\n  area(2, 4),\n  area(3, 1),\n  area(3, 2),\n  area(3, 3),\n  area(3, 4)\n)\n\npublication_year_box + SDG_theme_box + classtype_box +  modelgroup_box + \nancillary_box + indexes_box + RStype_box + no_band_group_box +\nRSgroup_box + RS_resolution_box + confusion_matrix_box +\n  plot_layout(design = design)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: Categorical study features\n\n\n\n\nCode\nfraction_majority_class + cits +\nnumber_class + sample_size +\nplot_layout(axis_titles = 'collect_y', ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Numeric study features",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/results.html#meta-analysis",
    "href": "chapters/results.html#meta-analysis",
    "title": "4  Results",
    "section": "Meta-analysis",
    "text": "Meta-analysis\n\n\nCode\n# METHOD: Weighted: level 2 model\n## pes: pooled effect size\npes.da  &lt;- rma(yi,\n               vi,\n               data = ies.da,\n               method = \"REML\",\n               test = \"t\")\n\n\n\n\nCode\n# METHOD: Weighted: Nested level 3model\npes.da.lvl3 &lt;- rma.mv(yi,\n                      vi,\n                      data = ies.da ,\n                      tdist = TRUE,\n        # adding random effects at the study level and effect size \n                      random = ~ 1 | AuthorYear / esid,\n                      method = \"REML\",\n                      # recommendations from the function documentation:\n                      test=\"t\",  \n                      dfs=\"contain\"\n                    )\n\n#summary(pes.da.lvl3)\n\n\n\n\nCode\npes &lt;- predict(pes.da.lvl3, transf = transf.ipft.hm, targ = list(ni=1/(pes.da.lvl3$se)^2))\n\npes.da.lvl3_I_squared &lt;- dmetar::var.comp(pes.da.lvl3)\n\npes.da.lvl3_CI &lt;- confint(pes.da.lvl3)\n\nresults &lt;- data.frame(\n  theta = pes$pred, \n  ci_l = pes$ci.lb,\n  ci_u = pes$ci.ub,\n  sigma2.1 = pes.da.lvl3$sigma2[1],\n  sigma2.2 = pes.da.lvl3$sigma2[2],\n  df = pes.da.lvl3$QEdf, \n  Q = pes.da.lvl3$QE, \n  p = pes.da.lvl3$QEp,\n  I_L2 = pes.da.lvl3_I_squared$results$I2[2],  \n  I_L3 = pes.da.lvl3_I_squared$results$I2[3] \n)\nrow.names(results) &lt;-\"RE_lvl3\"\n\n# results$theta\n# # same as\n# v_bar &lt;- (pes.da.lvl3$se)^2\n# t_bar &lt;- pes.da.lvl3$b[1]\n# (1/2 * (1 - sign(cos(2*t_bar)) *\n#           sqrt(1 - (sin(2*t_bar)+(sin(2*t_bar)-1/sin(2*t_bar))/(1/v_bar))^2)))\n# \n\n\n\n\nCode\n#profile likelihood plots of the variance components of the model\n#par(mfrow=c(2,1))\n#profile(pes.da.lvl3, sigma2=1)\n#profile(pes.da.lvl3, sigma2=2)\n\n# METHOD 1.2:  \n# multivariate parameterization model \n# this is a multilvl rather than a nested model (if since the samples could be different data or countries this might be the better approch?)\npes.da.lvl3.mv &lt;- rma.mv(yi,\n                      vi,\n                      data = ies.da,\n                      random = ~ esid|AuthorYear,\n                      method = \"REML\",\n                      # recomendations from the function detains +Assink et.al:\n                      tdist = TRUE,\n                      test=\"t\",  \n                      dfs=\"contain\"\n                    )\n\n# should be exactly the same\n#logLik(pes.da.lvl3)\n#logLik(pes.da.lvl3.mv)\n\n\n# future research look into different variance-covariance matrix structures\n\n\n\n\nCode\n# \nW &lt;- weights(pes.da.lvl3, type=\"matrix\")\nX &lt;- model.matrix(pes.da.lvl3)\ny &lt;- cbind(ies.da$yi)\nsolve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y\n\n\n# weights of pixel- vs weights of object- based studies \nies.da$weights &lt;- weights(pes.da.lvl3, type = \"rowsum\")\n\nhist(ies.da$weights)\n\nggplot(ies.da, mapping = aes(classification_type, weights))+\n  geom_boxplot()+\n  geom_jitter(aes(colour = AuthorYear), alpha = 0.7)\n\nt.test(weights~classification_type, \n       # removing unclear\n       data = subset(ies.da, ies.da$classification_type != \"Unclear\"))\n\nggplot( subset(ies.da, ies.da$classification_type != \"Unclear\"),\n        mapping = aes(classification_type, weights))+\n  geom_boxplot()\n\n\n\n\nCode\n# To visualize results aggregate model \n\n### assume that the effect sizes within studies are correlated with \nV &lt;- vcalc(vi, cluster=AuthorYear, obs=esid, data=ies.da, rho=pes.da.lvl3.mv$rho)\n \n### fit multilevel model using this approximate V matrix\npes_agg.da.lvl3 &lt;- rma.mv(yi, V, random = ~ 1 | AuthorYear/esid, data=ies.da)\n\nagg &lt;- aggregate(ies.da, cluster=AuthorYear, V=vcov(pes_agg.da.lvl3, type=\"obs\"), addk=TRUE)\n\nagg.pes.da &lt;- rma(yi, vi, method=\"EE\", data=agg, digits=3)\nagg.pes &lt;- predict(agg.pes.da , transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3$se)^2))\n\n# forest plot:\nmodel_summary &lt;- bquote(paste(\"RE (3-Level) Model \",\n                 \"(Q = \",\n                 .(fmtx(results$Q[1], digits=0)),\n                 \", df = \", \n                 .(fmtx(results$df[1], digits=0)),\n                 \", p &lt;.001; \", \n                 I^2, \" = 100% \",\n                       sep = \"\"))\n\npng(file='forestplot_weighted.png', width = 1000, height = 800)\nforest(agg.pes.da,addpred=TRUE, \n       xlim=c(-2,2.5),\n       alim =c(0, 1),\n       transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3$se)^2), \n       header=TRUE,\n       slab=AuthorYear,\n       order=\"obs\",\n       cex=1.5, \n       refline=NA,\n       digits = 3,\n       ilab=ki, \n       mlab= model_summary,\n       ilab.xpos= -.3,\n       ilab.pos = 2, \n       #showweights = TRUE, \n       shade=\"zebra\"\n       )\n\ntext(-.5, agg.pes.da$k+2, \"OA Count\", cex=1.5, font=2)\ntext(2.1, agg.pes.da$k+3, \"Weighted\", cex=1.5, font=2)\n\n\n\n\nCode\n########################\n# METHOD 2: UnWeighted\n# the mean of the yi should be the estimate of effect \npes.da.lvl3_unweighted  &lt;- rma.mv(yi = yi, \n                                  V = 1,\n                                  random = ~ 1 | AuthorYear / esid, # Nested random effects \n                                  data = ies.da, \n                                  method = \"REML\", \n                                  tdist = TRUE,\n                                  test=\"t\", \n                                  dfs=\"contain\")\n#summary(pes.da.lvl3_unweighted)\n#mean(ies.da$yi)\n\npes_u &lt;- predict(pes.da.lvl3_unweighted, transf = transf.ipft.hm, targ = list(ni=1/(pes.da.lvl3_unweighted$se)^2))\n\ntheta_u &lt;- mean(ies.da$OA_reported)\nt_critical &lt;- qt(0.975, 19)\nse &lt;- 1 / sqrt(86)\n\nresults[\"Unwei_rma.mv\"] &lt;- NA\nresults[\"Unwei_rma.mv\", 1:3 ]&lt;- c(pes_u$pred, pes_u$ci.lb, pes_u$ci.ub)\n\nresults&lt;- results|&gt; mutate_if(is.numeric, round, digits=3)\n\n\n\n\nCode\n# To visualize results aggregate model \n\n### assume that the effect sizes within studies are correlated with \nV &lt;- vcalc(vi, cluster=AuthorYear, obs=esid, data=ies.da, rho=0.6)\n \n### fit multilevel model using this approximate V matrix\npes_agg.unweighted &lt;- rma.mv(yi, V, random = ~ 1 | AuthorYear/esid, data=ies.da)\n\nagg &lt;- aggregate(ies.da, cluster=AuthorYear, V=vcov(pes_agg.unweighted, type=\"obs\"), addk=TRUE)\n\nagg.pes.da &lt;- rma(yi, vi, method=\"EE\", data=agg, digits=3)\nagg.pes &lt;- predict(agg.pes.da , transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3_unweighted$se)^2))\n\n# forest plot:\npng(file='forestplot_unweighted.png', width = 1000, height = 800)\nforest(agg.pes.da,addpred=TRUE, \n       xlim=c(-2,2.5),\n       alim =c(0, 1),\n       transf = transf.ipft.hm, targ =list(ni=1/(pes.da.lvl3_unweighted$se)^2), \n       header=TRUE,\n       slab=AuthorYear,\n       order=\"obs\",\n       refline=NA,\n       col =  \"#001158\",\n       cex=1.5, \n       digits = 3,\n       ilab=ki, \n       mlab= \"Unweighted Model\",\n       ilab.xpos= -.3,\n       ilab.pos = 2, \n       #showweights = TRUE, \n       shade=\"zebra\"\n       )\n\ntext(-.5, agg.pes.da$k+2, \"OA Count\", cex=1.5, font=2)\ntext(2.1, agg.pes.da$k+3, \"Unweighted\", cex=1.5, font=2)\n\n\n\nThe forest plot below (Figure 4.5) compares the overall accuracy effect size across studies using both weighted and unweighted models, with error bars which correspond to the weighted model — at this scale there is no discernible difference between the error bars of the two models. Each study is given with the number of estimates per study \\(k_j\\), and study average effect size (\\(\\kappa_j\\)), with 95% confidence intervals (CI), both for the weighted and unweighted model. Of the 20 primary studies included, six reported only one effect. Based on the unweighted model, the average accuracy of machine learning methods applied to remote sensing data is 0.90 (95% CI[0.85; 0.94]). While the three-level meta-analytic model produced an average accuracy of 0.89 (95% CI[0.85; 0.93]). This implies, that on average, the machine learning methods correctly classify around 90% of the time when applied to remote sensing data.\n\n\n\n\n\n\n\n\nFigure 4.5: Forest plot for both the weighted and unweighted model. \\(k_j\\) is number of reported overall accuracy estimates per study, the corresponding average effect size(\\(\\kappa_j\\)) and confidence interval per study for both models is given on the right. The pooled summary effect size based on the three-level RE meta-analytic and unweighted model are given on the bottom.\n\n\n\n\n\nThe heterogeneity metrics Cochran’s Q indicate significant heterogeneity of the reported overall acccuracies. The percentage of the variance attribution is \\(I^2_{\\text{level3}}\\) = 63.62% which is the fraction of the variation that can be attributed to between-study, and \\(I^2_{\\text{level2}}\\) = 36.38% which is within-study heterogeneity, with negligible fixed effect variance (variance due to sampling error). The \\(I^2\\) value of 100% indicates that all the observed variability in effect sizes across studies is due to heterogeneity rather than sampling error, suggesting substantial differences between the studies and a high degree of variation in their results.\n\n\n\n\nModel Selection\nUsing the multi-model inference function, a total of 31,298 models converged . Figure 4.6, illustrates the predictor importance after evaluating all possible combinations of predictors to identify which combination provides the best fit and which predictors are most influential. Higher importance values indicate more consistent inclusion in high-weight models. The majority class proportion is the most important predictor, followed by the inclusion of ancillary data. Less influential predictors include used of indices, sample size, publication year, and the number of classes in the study. Meanwhile, factors such as classification type, SDG goal, machine learning group, spatial resolution, and citation count have minimal importance in the overall model performance (i.e., were not included in the models top performing models according to AIC).\n\n\n\n\nCode\nstudy_features &lt;- c(\"model_group\", \"indices\", \"SDG_theme\", \"classification_type\",\n  \"Confusion_matrix\", \"RS_device_type\", \"RS_device_group\", \"RS_spatital_res_grouped\",\n  \"ancillary\", \"no_band_group\", \"Publication.Year\", \"total\",\n  \"globalCitationsCount\", \"number_classes\", \"fraction_majority_class\"\n)\n\nies.da$se &lt;- sqrt(ies.da$vi)\nmulti_inf &lt;- multimodel.inference(TE = \"yi\", \n                     seTE = \"se\",\n                     method='REML', \n                     test='t', \n                     data = ies.da,\n                     predictors = study_features,\n                     interaction = FALSE, \n                     seed=357)\n\nmulti_inf$predictor.importance.plot+\n  scale_x_discrete(labels = feature_labels)+\n  labs(x = NULL, \n       title = \"Model-averaged predictor importance plot\" )+\n  \n  theme_apa()+\n  common_theme\nggsave(\"figures/fig-best_mod.png\", height =4, width = 7)\n\n#saveRDS(multi_inf, \"../appendix/multimodel_inference_out.rda\")\n\n\n\n\n\n\n\n\n\nFigure 4.6: Model-averaged predictor importance plot with a reference line at 0.8 a commonly used as a threshold to indicate important predictors.\n\n\n\n\n\nIn the multimodel inference analysis, the five best-performing models were identified based on their AIC scores. The selected top models consistently included key predictors such as the majority-class proportion and the use of ancillary data. Table 4.2 shows the results of the multi-model inference. The significant study features are the majority-class proportion and the inclusion of ancillary data. Interestingly, the use of ancillary data has a negative effect on overall accuracy in the FT transformed scale. Table 4.3 shows these best performing models and the intercept-only model (before adding the predictors), note that the AIC is very similar in among the top five.\n\n\nCode\nmulti_coef&lt;- read_excel(\"figures/summary_tables.xlsx\", \n                        sheet = \"multimodel_inference_results\")\n  kable(\n  multi_coef,\n  booktabs = TRUE,\n  linesep = \"\",\n  col.names = c(\"Importance\", \"Feature (Referance Category)\", \n                \"Comparison Category\", \"b\", \"SE\", \"z\", \"p\"),\n  )|&gt;\n    kable_styling(font_size = 9.5, \n                full_width = FALSE)|&gt;\n    row_spec(c(0),  bold = TRUE)|&gt;\n   \n    footnote(c(\"Importace of each feature, the reference and combarsion categories given with their estimated coefficients (b), standard errors (SE) on the FT transformed scale with correstponding z- and p-values.\"), threeparttable=T)\n\n\n\n\nTable 4.2: Multi-model inference coefficients and feature importance.\n\n\n\n\n\n \n  \n    Importance \n    Feature (Referance Category) \n    Comparison Category \n    b \n    SE \n    z \n    p \n  \n \n\n  \n    NA \n    Intercept \n    na \n    1.29 \n    7.85 \n    0.16 \n    0.869 \n  \n  \n    1.00 \n    Majority-class Proportion \n    na \n    0.47 \n    0.08 \n    6.15 \n    0.000 \n  \n  \n    0.92 \n    Ancillary Data (Remote Sensing Only) \n    Ancillary Data Included \n    -0.12 \n    0.05 \n    2.33 \n    0.020 \n  \n  \n    0.39 \n    Indices (Not Used) \n    Used \n    0.03 \n    0.04 \n    0.67 \n    0.500 \n  \n  \n    0.38 \n    Number of Spectral Bands  (Low) \n    Mid \n    0.05 \n    0.06 \n    0.72 \n    0.471 \n  \n  \n    NA \n    na \n    Not Reported \n    0.02 \n    0.04 \n    0.55 \n    0.581 \n  \n  \n    0.16 \n    Confusion Matrix (Not reported) \n    Reported \n    0.01 \n    0.02 \n    0.29 \n    0.776 \n  \n  \n    0.13 \n    Sample Size \n    na \n    0.00 \n    0.00 \n    0.10 \n    0.922 \n  \n  \n    0.11 \n    Number of Classes \n    na \n    0.00 \n    0.00 \n    0.19 \n    0.846 \n  \n  \n    0.10 \n    Publication Year \n    na \n    0.00 \n    0.00 \n    0.06 \n    0.952 \n  \n  \n    0.03 \n    Remote Sensing Type (Active) \n    Passive \n    0.00 \n    0.02 \n    0.16 \n    0.870 \n  \n  \n    NA \n    na \n    Combined \n    0.01 \n    0.03 \n    0.17 \n    0.869 \n  \n  \n    NA \n    na \n    Not Reported \n    0.00 \n    0.02 \n    0.04 \n    0.971 \n  \n  \n    0.02 \n    Spatial Resolution ( &lt;1 metre) \n    10-30 metres \n    0.00 \n    0.07 \n    0.01 \n    0.990 \n  \n  \n    NA \n    na \n    Not Reported \n    0.00 \n    0.07 \n    0.00 \n    0.996 \n  \n  \n    0.02 \n    SDG Theme (SDG11:Sustainable Cities) \n    SDG2: Zero Hunger \n    0.00 \n    0.01 \n    0.08 \n    0.939 \n  \n  \n    NA \n    na \n    SDG15: Life on Land \n    0.00 \n    0.01 \n    0.10 \n    0.924 \n  \n  \n    0.02 \n    Classification Type (Object-level) \n    Pixel-level \n    0.00 \n    0.01 \n    0.06 \n    0.955 \n  \n  \n    NA \n    na \n    Unclear \n    0.00 \n    0.01 \n    0.05 \n    0.958 \n  \n  \n    0.01 \n    Model Group (Neural Networks) \n    Tree-Based Models \n    0.00 \n    0.01 \n    0.05 \n    0.961 \n  \n  \n    NA \n    na \n    Other \n    0.00 \n    0.01 \n    0.04 \n    0.965 \n  \n  \n    0.00 \n    Device Group (Landsat) \n    Sentinel \n    0.00 \n    0.01 \n    0.05 \n    0.959 \n  \n  \n    NA \n    na \n    Not Reported \n    0.00 \n    0.01 \n    0.05 \n    0.956 \n  \n  \n    NA \n    na \n    Other \n    0.00 \n    0.00 \n    0.05 \n    0.963 \n  \n  \n    0.00 \n    Number of Citations \n    na \n    0.00 \n    0.00 \n    0.01 \n    0.995 \n  \n\n\nNote: \n\n Importace of each feature, the reference and combarsion categories given with their estimated coefficients (b), standard errors (SE) on the FT transformed scale with correstponding z- and p-values.\n\n\n\n\n\n\n\n\n\nCode\nmulti_inf&lt;- readRDS(\"../appendix/multimodel_inference_out.rda\")\nmodels &lt;- data.frame(\n  #Model.no = c(42, 138, 10, 142, 16522, 1),\n  \"Candidate models\" = c(\"Ancillary Data + Majority-class Proportion + Indices\", \n                          \"Ancillary Data + Majority-class Proportion + Number of Spectral Bands\", \n                          \"Ancillary Data + Majority-class Proportion\", \n                          \"Ancillary Data + Confusion Matrix + Majority-class Proportion + Number of Spectral Bands\", \n                          \"Ancillary Data + Majority-class Proportion + Number of Spectral Bands + Sample Size\", \n                         \"Intercept-Only\"), \n  \"df\" =  c(multi_inf$top5.models$df, 2), \n  AICc = c(multi_inf$top5.models$AICc, -41.93257), \n  \"Akaike weights\" = c(multi_inf$top5.models$weight, 2.444007e-17)\n)\n\nkable(\n  models,\n  booktabs = TRUE,\n  escape = FALSE,\n  linesep = c(\"\\\\addlinespace \\\\addlinespace\"),\n  col.names = c(\"Candidate models\", \"df\", \"AIC$_c$\", \"Akaike weights\"),\n  digits = c(0,0, 2, 2,2)\n  )|&gt;\n  kable_styling(font_size = 9.5, \n                full_width = FALSE)|&gt;\n row_spec(c(0),  bold = TRUE)|&gt;\n  column_spec(1, width = \"9cm\")\n\n\n\n\nTable 4.3: Set of 5 best-ranked models and intercept only model ordered by AIC\\(_c\\). \n\n\n\n\n\n \n  \n    Candidate models \n    df \n    AIC$_c$ \n    Akaike weights \n  \n \n\n  \n    Ancillary Data + Majority-class Proportion + Indices \n    5 \n    -115.46 \n    0.39 \n  \n  \n    Ancillary Data + Majority-class Proportion + Number of Spectral Bands \n    6 \n    -114.42 \n    0.23 \n  \n  \n    Ancillary Data + Majority-class Proportion \n    4 \n    -114.13 \n    0.20 \n  \n  \n    Ancillary Data + Confusion Matrix + Majority-class Proportion + Number of Spectral Bands \n    7 \n    -113.08 \n    0.12 \n  \n  \n    Ancillary Data + Majority-class Proportion + Number of Spectral Bands + Sample Size \n    7 \n    -111.65 \n    0.06 \n  \n  \n    Intercept-Only \n    2 \n    -41.93 \n    0.00 \n  \n\n\n\n\n\n\n\n\n\n\nCode\nmeta_reg_42 &lt;- rma.mv(yi, vi,\n  data = ies.da ,\n  random = ~ 1 | AuthorYear / esid,\n  tdist = TRUE,\n  method = \"REML\",\n  test = \"t\", \n  dfs=\"contain\",\n  mods = ~ fraction_majority_class+ancillary+ indices\n)\n\n\n\nTable 4.4 shows the estimated coefficients for the best-fit model — i.e., the model with the lowest AIC value among the candidate models. The coefficients are presented both in the FT-transformed scale (b) and on the natural (back-transformed) scale. The results highlight that the proportion of the majority class has the largest positive effect (\\(b = 0.39\\), \\(b^{BT} = 0.15\\), p &lt; .001). Suggesting that increasing the majority-class proportion significantly improves overall accuracy. While, the inclusion of ancillary data has a small negative effect on the FT-transformed scale (\\(b = -0.11, p = 0.029\\)) but shows a slight positive effect once back-transformed (\\(b^{BF} = 0.01\\)). The use of indices has a minimal and non-significant effect (\\(p = 0.131\\)).\n\n\nCode\nbacktras.function&lt;- function(t_bar, se){\n  v_bar &lt;- (se)^2\n  (1/2 * (1 - sign(cos(2*t_bar)) *\n           sqrt(1 - (sin(2*t_bar)+(sin(2*t_bar)-1/sin(2*t_bar))/(1/v_bar))^2)))\n}\nci.function &lt;- function(ci_l, ci_u, se){\n  back_ci_l = backtras.function(ci_l, se)\n  back_ci_u = backtras.function(ci_u, se)\n  out&lt;- paste0(\"[\", paste0(format(round(back_ci_l, 2), nsmall = 2)), \", \", # must be a better way\n               paste0(format(round(back_ci_u,2), nsmall = 2)), \"]\")\n  return(out)\n}\n\ncoef_bestmod &lt;- \n  meta_reg_42|&gt;\n  tidy() |&gt;\n  mutate(estimate_back_transfromed = mapply(backtras.function, meta_reg_42$b, meta_reg_42$se))|&gt;\n  mutate(CI = mapply(ci.function, meta_reg_42$ci.lb,meta_reg_42$ci.ub, meta_reg_42$se))|&gt;\n  select(term, estimate, std.error, statistic,  p.value, estimate_back_transfromed, CI)\n\ncoef_bestmod$term &lt;- factor(\n  coef_bestmod$term,\n  levels = c(\n    \"intercept\",\n    \"fraction_majority_class\",\n    \"ancillaryAncillary Data Included\",\n    \"indicesUsed\"\n  ),\n  labels = c(\n    \"Intercept\",\n    \"Majority-class Proportion\",\n    \"Ancillary Data:  Included\",\n    \"Indices:  Used\"\n  )\n)\n\ncoef_bestmod|&gt;\n  kable(booktabs = TRUE, \n        linesep = \"\",\n        escape = FALSE, \n        col.names = c(\"Predictor\", \"b\", \"SE\", \"t\", \"p\", \"b$^{B-FT}$\", \"CI\"), \n        digits = c(0, 2, 2, 2, 3, 2), \n        align = c(\"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\")\n  ) |&gt;\n  kable_styling(font_size = 10, \n                full_width = FALSE)|&gt;\n  add_header_above(c(\" \",\"\",\"\",\"\",\"\", \"back-transfromed scale\" = 2)) |&gt;\n  row_spec(c(0),  bold = TRUE)|&gt;\n  column_spec(1, width = \"6cm\")|&gt;\n  footnote(c(\"The estimated coefficients (b), standard errors (SE) on the FT transformed scale, with correstponding t-statistics and p-values. Additionally, the coefficients ($b^{B-FT}$) and corresponding confidence intervals (CI) are shown on the back-transformed scale.\"), escape = FALSE, threeparttable=T)\n\n\n\n\nTable 4.4: Table of estimated coefficients for the best-fit model.\n\n\n\n\n\n \n\n\n\n\n\n\nback-transfromed scale\n\n  \n    Predictor \n    b \n    SE \n    t \n    p \n    b$^{B-FT}$ \n    CI \n  \n \n\n  \n    Intercept \n    0.99 \n    0.06 \n    17.22 \n    0.000 \n    0.70 \n    [0.58, 0.80] \n  \n  \n    Majority-class Proportion \n    0.39 \n    0.08 \n    4.93 \n    0.000 \n    0.15 \n    [0.05, 0.27] \n  \n  \n    Ancillary Data:  Included \n    -0.11 \n    0.05 \n    -2.22 \n    0.029 \n    0.01 \n    [0.04, 0.00] \n  \n  \n    Indices:  Used \n    0.06 \n    0.04 \n    1.53 \n    0.131 \n    0.00 \n    [0.00, 0.02] \n  \n\n\nNote: \n\n The estimated coefficients (b), standard errors (SE) on the FT transformed scale, with correstponding t-statistics and p-values. Additionally, the coefficients ($b^{B-FT}$) and corresponding confidence intervals (CI) are shown on the back-transformed scale.\n\n\n\n\n\n\n\n\n\nTo assess the impact of the study features on the estimated heterogeneity the features included in the best-fit model are fitted as sole covariates. Table 4.5 shows the parameter estimates from the meta-analysis, comparing the intercept-only model with four mixed-effects models, one for each of the features in the best-fit model and best-fit model itself.\n\n\nCode\nmeta_reg_anc &lt;- rma.mv(yi, vi,\n                       data = ies.da,\n                       random = ~ 1 | AuthorYear / esid,\n                       tdist = TRUE,\n                       method = \"REML\",\n                       test = \"t\", \n                       dfs=\"contain\", \n                       mods = ~ ancillary)\n\n\nmeta_reg_frac &lt;- rma.mv(yi, vi,\n                       data = ies.da,\n                       random = ~ 1 | AuthorYear / esid,\n                       tdist = TRUE,\n                       method = \"REML\",\n                       test = \"t\", \n                       dfs=\"contain\", \n                       mods = ~ fraction_majority_class)\nmeta_reg_ind &lt;- rma.mv(yi, vi,\n                       data = ies.da,\n                       random = ~ 1 | AuthorYear / esid,\n                       tdist = TRUE,\n                       method = \"REML\",\n                       test = \"t\", \n                       dfs=\"contain\", \n                       mods = ~ indices)\n\nextract_parameters &lt;- function(model) {\n  out &lt;- data.frame(\n    model = if (!is.null(model$formula.mods)) paste(model$formula.mods)[2] else \"Intercept Only\",  \n    sig_lvl2 = round(model$sigma2[2], 3), \n    sig_lvl3 = round(model$sigma2[1], 3), \n    QE = round(model$QE,0), \n    df_Q = model$QEdf,\n    p_Q = round(model$QEp,3), \n    f_test = if (!is.null(model$formula.mods))\n      round(model$QM, 0)\n    else\n      NA,\n    df_F = if (!is.null(model$formula.mods))\n      model$QMdf[1]\n    else\n      NA,\n    p_F = if (!is.null(model$formula.mods))\n      round(model$QMp, 3)\n    else\n      NA,\n    I2_lvl2 = dmetar::var.comp(model)$results[2, 2],\n    I2_lvl3 = dmetar::var.comp(model)$results[3, 2],\n    R2_lvl2 = if (!is.null(model$formula.mods))\n      (round(1 - (model$sigma2[2] / pes.da.lvl3$sigma2[2]), 3) * 100)\n    else\n      NA,\n    R2_lvl3 = if (!is.null(model$formula.mods))\n      (round(1 - (model$sigma2[1] / pes.da.lvl3$sigma2[1]), 3) * 100)\n    else\n      NA\n  )\n    return(out)\n}\n\nheterogeneity_tbl &lt;- rbind(extract_parameters(pes.da.lvl3), \n                           extract_parameters(meta_reg_frac), \n                           extract_parameters(meta_reg_anc), \n                           extract_parameters(meta_reg_ind),\n                           extract_parameters(meta_reg_42))\n\nheterogeneity_tbl$model &lt;- factor(heterogeneity_tbl$model, \n                                  levels = c(\"Intercept Only\", \n                                             \"fraction_majority_class\",\n                                             \"ancillary\",\n                                             \"indices\",\n                                             \"fraction_majority_class + ancillary + indices\"), \n                                  labels = c(\"Intercept Only\", \n                                             \"Majority-class Proportion\",\n                                             \"Ancillary Data\",\n                                             \"Indices\",\n                                             \"Combinded model$^{\\\\text{a}}$\"))\n\nheterogeneity_tbl$p_Q &lt;- ifelse(heterogeneity_tbl$p_Q &lt;0.0001,\n                                \"&lt;.0001\",\n                                heterogeneity_tbl$p_Q)\n\nkable(\n  heterogeneity_tbl,\n  booktabs = TRUE,\n  escape = FALSE,\n  linesep = \"\", \n  col.names = c(\"Model\",\n    \"$\\\\sigma^2_{\\\\text{level2}}$\", \"$\\\\sigma^2_{\\\\text{level3}}$\", \n    \"$Q_E$\", \"df\", \"$p_Q$\", \"$F$\", \"df\", \"$p_F$\", \n    \"$I^2_{\\\\text{level2}}$\", \"$I^2_{\\\\text{level3}}$\", \n    \"$R^2_{\\\\text{level2}}$\", \"$R^2_{\\\\text{level3}}$\"), \n  align = c(\"l\", \"r\",\"r\",\"r\",\"r\",\"r\",\"r\",  \"r\",\"r\",\"r\",\"r\",\"r\",\"r\")\n  )|&gt;\n  kable_styling(font_size = 9, \n                full_width = FALSE)|&gt;\n  row_spec(c(0),  bold = TRUE)|&gt;\n  #column_spec(1, width = \"4cm\")|&gt;\n  #column_spec(c(2,3,10:13), width = \"0.8cm\")|&gt;\n  #column_spec(c(5,7,8), width = \"0.1cm\")|&gt;\n  footnote(general = \"Test statistic, degrees of freedom and respective p values are provide. This table allows heterogeneity at level 2 and 3 can be compared between the incetept only model, Majority-class Proportion and Adncillary Data only models, as well as the combinded model\", \n           alphabet = \"Combinded model: Ancillary Data + Majority-class Proportion + Indices\", threeparttable=TRUE)\n\n\n\n\nTable 4.5: Results for heterogeneity and covariates tests for intercept only model, individual covariates as well as the best model.\n\n\n\n\n\n \n  \n    Model \n    $\\sigma^2_{\\text{level2}}$ \n    $\\sigma^2_{\\text{level3}}$ \n    $Q_E$ \n    df \n    $p_Q$ \n    $F$ \n    df \n    $p_F$ \n    $I^2_{\\text{level2}}$ \n    $I^2_{\\text{level3}}$ \n    $R^2_{\\text{level2}}$ \n    $R^2_{\\text{level3}}$ \n  \n \n\n  \n    Intercept Only \n    0.010 \n    0.017 \n    12161784 \n    85 \n    &lt;.0001&gt;\n    NA \n    NA \n    NA \n    36.38 \n    63.62 \n    NA \n    NA \n  \n\n\n  \n    Majority-class Proportion \n    0.009 \n    0.007 \n    11458055 \n    84 \n    &lt;.0001&gt;\n    27 \n    1 \n    0.000 \n    57.29 \n    42.71 \n    7.8 \n    60.7 \n  \n\n\n  \n    Ancillary Data \n    0.010 \n    0.015 \n    12035286 \n    84 \n    &lt;.0001&gt;\n    3 \n    1 \n    0.117 \n    40.47 \n    59.53 \n    -1.4 \n    14.7 \n  \n\n\n  \n    Indices \n    0.010 \n    0.018 \n    11986674 \n    84 \n    &lt;.0001&gt;\n    3 \n    1 \n    0.100 \n    34.26 \n    65.74 \n    3.6 \n    -5.8 \n  \n\n\n  \n    Combinded model$^{\\text{a}}$ \n    0.009 \n    0.005 \n    11440331 \n    82 \n    &lt;.0001&gt;\n    13 \n    3 \n    0.000 \n    63.46 \n    36.54 \n    8.6 \n    69.9 \n  \n\n\n\n\nNote: \n\n Test statistic, degrees of freedom and respective p values are provide. This table allows heterogeneity at level 2 and 3 can be compared between the incetept only model, Majority-class Proportion and Adncillary Data only models, as well as the combinded model\n\na Combinded model: Ancillary Data + Majority-class Proportion + Indices\n\n\n\n\n\n\n\n\n\nAs shown in Table 4.5, the Majority-class Proportion explains a greater proportion of the between-study heterogeneity, as indicated by the reduction in \\(\\sigma^2_{\\text{level2}}\\) between the intercept-only model and the model with the Majority-class Proportion. In contrast, the use of Ancillary Data explains relatively little between-study heterogeneity and negligible within-study heterogeneity.\nThe combined model (best-fit model) explains the most heterogeneity overall, as reflected in the shift in \\(I^2\\) values. The total \\(I^2\\), consistently at 100% across all models, suggests that nearly all the variation in effect sizes is due to differences between the studies, rather than sampling error. This observation raises the possibility of an “apples and oranges” problem (see the discussion section), where the included studies may be too heterogeneous to be meaningfully compared.\nAll models show significant heterogeneity, with Cochran’s Q test results being significant (p &lt; 0.001). The \\(R^2\\) values indicate that the covariates in the combined mixed-effects model account for 69.9% of the variance at level 3 (between-study level) and 8.6% of the variance at level 2 (within-study level).\n\n\nFigure 4.7 illustrates the relationship between the proportion of the majority class and overall accuracy of the individual studies included in the meta-analysis. The plot is based on the combined mixed-effects model, where the solid black line represents the fitted regression line, and the shaded area indicates the 95% confidence interval. Each point (or bubble) represents an individual study, with the size of each bubble proportional to the weight it received in the analysis (i.e., larger points represent studies that had more influence on the overall results). The plot demonstrates a clear trend: as the proportion of the majority class increases, overall accuracy tends to improve, indicating a positive correlation between these two variables.\n\n\n\nCode\nlibrary(RColorBrewer)\ncolors &lt;- c(brewer.pal(9, \"Pastel1\"), brewer.pal(12, \"Set3\"))\nauthors &lt;- unique(ies.da$AuthorYear)\nauthor_colors &lt;- setNames(colors[1:length(authors)], authors)\nies.da$color &lt;- author_colors[ies.da$AuthorYear]\n\n\n# labels\nies.da &lt;- tibble::rowid_to_column(ies.da, \"effectsizeID\")\nlowest_yi_per_author &lt;- ies.da |&gt;\n  group_by(AuthorYear) |&gt;\n  filter(yi == min(yi)) |&gt;\n  select(AuthorYear, effectsizeID)\n# 2 level mod\n\npng(file=\"fig-bubble_lvl3.png\", width=700, height=800)\nbubble_plot_3lvl &lt;- regplot(meta_reg_42,\n        mod = \"fraction_majority_class\", \n        transf = transf.ipft.hm, targ =list(ni=1/(meta_reg_42$se)^2), \n        xlab = \"Proportion of majority class\", \n        ylab = \"Overall Accuracy\",\n        slab = ies.da$AuthorYear, \n        label = lowest_yi_per_author$effectsizeID,\n        labsize = 1, \n        bg=ies.da$color, \n        xlim=c(0.05, 1.05), \n        ylim=c(0.64, 1))\ndev.off()\n\n\n\n\n\n\n\n\nFigure 4.7: Bubble plot showing the observed effect size, overall accuracy of the individual studies plotted against a the proportion of the majority class. Based on the mixed-effects model, the overall accuracy as a function of proportion of the majority with corresponding 95% confidence interval bounds. The size of the points are proportional to the weight that the observation received in the analysis, while the color of the points is unique to each study, with the lowest overall actuary from each study labeled with the first author and publication year. \n\n\n\n\nThe size of the points in the bubble plot illustrates the benefit of incorporating the structure of the data into meta-analytic weighting. Namely, that the difference in size of the bubbles are not exessive. Figure 4.8 highlights this by plotting the weights for each study from a fixed effect, random effect with two levels and the structure used here, the random effects with three levels. As shown, the fixed-effects model particularity as one study is heavily weighted, which can distort the overall results. In contrast, the two-level and three-level models distribute the weights more evenly across studies, reflecting the importance of accounting for between-study heterogeneity and within-study variation.\n\n\nCode\n# Fixed-effect model (only within-study variance)\nFE_mod &lt;- rma(yi, vi, method = \"FE\", data = ies.da)\nies.da$w_FE &lt;- weights(FE_mod)\n# 2 level meta analysis weights \nies.da$w_RE2 &lt;- weights(pes.da)\n# 3 level meta analysis weights \nies.da$w_RE3 &lt;- weights(pes.da.lvl3)\n\nggplot(ies.da, aes(x = factor(AuthorYear)))+\n  geom_jitter(aes(y = w_FE, size = w_FE, colour = \"Fixed Effect Model\"), \n             alpha = 0.2, width = 0, height = 0.1)+\n  geom_jitter(aes(y = w_RE2, size = w_RE2, colour = \"Two Level Random Effect Model\"), \n              alpha = 0.2, width = 0, height = 0.1)+\n   geom_jitter(aes(y = w_RE3, size = w_RE3, colour = \"Three Level Random Effect Model\"), \n             alpha = 0.2, width = 0, height = 0.1)+\n   scale_size_continuous(range = c(1, 5), breaks = c(0.5, 0.8, 1.1, 1.4,1.5, 2))+\n  coord_flip() +\n  labs(x = NULL, \n       y = \"Weights\", \n       title = \"Weights of different meta-analysis models\") +\n  guides(size = \"none\")+\n  common_theme+\n  theme(# grid lines \n        panel.grid.major.y = element_line(linewidth = 0.1, colour = \"grey80\"),\n       # legend specs\n        legend.position = \"bottom\", \n        legend.text= element_text(size=8, margin = margin(l = -0.3, r = -0.5, unit = \"cm\"))\n       ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.8: Intercept only models at three different levels plot to compare weighting.\n\n\n\n\nLastly, Figure 4.9 is a plot of observed overall accuracy against the predicted overall accuracy from the combined mixed-effects model. The points are colored based on whether ancillary information was included in the primary study. As Figure 4.9 illustrates, the combined mixed-effects model tends to overestimate overall accuracy — the fitted regression line (in grey) lies above the line of perfect agreement (y = x, in black), indicating that the model’s predictions are generally higher than the observed accuracy values.\n\n\nCode\nies.da$preds &lt;- predict(meta_reg_42, transf = transf.ipft.hm, targ =list(ni=1/(meta_reg_42$se)^2))$pred\n\nggplot(ies.da, aes(x = OA_reported, y = preds))+\n  geom_point(size = 5, aes(colour = ancillary), alpha = 0.7)+\n  geom_smooth(method = \"lm\",formula = 'y ~ x',  se = FALSE, colour = \"grey\", linetype= \"dashed\", linewidth = 0.8)+\n  # y = x line \n  geom_textabline(label = \"y = x\", intercept = 0, slope = 1,hjust = 0.2,  linetype= \"longdash\")+\n  scale_colour_brewer(palette = \"Dark2\",\n                      labels = c(\"Only Remote Sensing Data\",\"Addition of Ancillary Data\"))+\n  xlim(c(0.64, 1))+\n  ylim(c(0.64,1))+\n  labs(x = \"Reported Overall Accuracy\", \n       y = \"Predicted Overall Accuracy\", \n       title = \"Plot of Observed Agaist Predicted Accuracy\")+\n    theme(legend.text = element_text(size = 8), \n          legend.position = \"top\") +\n  common_theme\n\n\nCode\nggsave(\"fig-preds.png\", width = 15,\n  height = 10,\n  units = \"cm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.9: Observed and predicted overall accuracy. The colour indicates whether the addition of ancillary data in the primary study’s model. The line of perfect agreement \\(y = x\\) is in black and fit regression of the points in grey. \n\n\n\n\n\n\n\nMcCulloch, C. E., & Neuhaus, J. M. (2011). Misspecifying the shape of a random effects distribution: Why getting it wrong may not matter. Statistical Science, 26(3), 388–402. https://doi.org/10.1214/11-STS361",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/discussion.html",
    "href": "chapters/discussion.html",
    "title": "5  Discussion",
    "section": "",
    "text": "This meta-analysis aimed to evaluate the performance of machine learning models applied to remote sensing for SDG monitoring. Specifically, the study aimed to estimate the average performance, determine the level of heterogeneity within and across studies, assess whether specific study features influence model performance, and lastly compare the sample-weighted and unweighted estimate summary effect. While previous meta-analyses on machine learning models for remote sensing have predominantly relied on unweighted approaches (Hall et al., 2023; e.g., Khatami et al., 2016), this study found that incorporating a weighted approach did not significantly alter the results. Both the weighted and unweighted estimates showed similar average performance metrics, suggesting that weighting by sample size may not dramatically influence the outcomes in this context.\nThe results from this meta-analysis show that the overall accuracy of machine learning models applied to remote sensing is consistently high. The estimated average overall accuracy of \\(\\hat{\\mu}_{_\\text{unweighted}} =\\) 0.90 and \\(\\hat{\\mu}_{_\\text{weighted}} =\\) 0.89. The results also demonstrate a considerable variability in the predictive performance of machine learning models applied to remote sensing data for SDGs. Some of this variability could be attributed to the proportion of the majority class as well as the inclusion of ancillary data. The type of model, whether neural networks and tree-based models or the SDG studied, showed no differences in overall accuracy. Unsurprisingly, the proportion of the majority class significantly affected the overall accuracy of machine learning models. While the use of ancillary data in primary studies has a small but significant positive effect on overall accuracy performance. No other significant effects were found in the study features examined in this study.\nThe findings of this study regarding the use of ancillary data aling with Khatami et al. (2016) and Hanadé Houmma et al. (2022) who found the use of ancillary data did improve model performance. Some effect of the choice of machine learning model was found by previous research. For example, Khatami et al. (2016) noted that while support vector machines and neural networks performed well, differences between other model types were not significant. Notably, no study was found that explicitly corrected for class imbalance (proportion of the majority class) when assessing the difference in performance between groups. While Khatami et al. (2016) employed pairwise comparisons, which does ensure that models are compared within the same data context, this study goes further in directly highlighting the influence of class proportion on overall accuracy.\n\nLimitations\n\nNumber of reviewers: From the 200 studies randomly sampled, three reviewers assessed whether full-text screening should be conducted. Only 57 papers were agreed upon by all three reviewers, while each reviewer thought between 77 and 81 studies could have been included. This highlights the subjectivity of the selection process and the importance of having multiple reviewers. The full-text screening was only conducted by one person which means that this subjectivity or potential mistakes were missed in the final dataset. This issue is exasperated by the inconsistent reporting on methods in this field. For example, one feature that could not be included in the analysis was whether the results reported were derived from the training or test set because it was very unclear in some of the selected studies.\nSample size: This analysis included a total of 20 studies. While several simulation studies suggest that a three-level meta-analysis can yield accurate results with as few as 20 to 40 studies (Hedges et al., 2010), this analysis is at the lower bound, and the included studies exhibit considerable variability, making the statistical power a concern. Polanin (2014) suggests a minimum of 40 studies is generally recommended to ensure robust results. Furthermore, a relatively high proportion of the studies (6 out of 20) reported only one result (\\(k_j = 1\\)), limiting the ability to assess within-study variability. The small sample size inherently increases the potential for bias and may affect the reliability of the findings (Polanin, 2014).\nChoice of effect size: While overall accuracy is widely used, it does not capture the complexity of model performance, especially in studies with imbalanced classes. To illustrate the problem, if 99% of the data belongs to class A, a model that always predicts class A—without any regard to the predictors—will achieve an overall accuracy of 99%, despite essentially doing nothing and failing to capture meaningful patterns. For more specific details on the issues related to the use of overall accuracy, see Foody (2020) and Stehman & Foody (2019). Alternative metrics include Matthews’ correlation coefficient, F1 score, Somers’ D, and average precision. Unfortunately, these metrics are rarely reported in the studies analyzed here. Moreover, some of these alternatives are also sensitive to class imbalance and must be corrected to ensure comparability across studies (Burger & Meertens, 2020).\nPublication bias: This study only examined published results, which introduces publication bias—a well-documented effect where studies with positive results are more likely to be published, while negative or neutral findings remain unpublished (Borenstein et al., 2009; Bozada et al., 2021; Hansen et al., 2022; Harrer et al., 2022). This bias can lead to an overestimation of effects, as demonstrated in this study, where the average overall accuracy is around 90%.\nStudy features included: The analysis would have benefited from the inclusion of more study features. It is also important to note that most of the study features included in this research were between-study covariates and did not differ within studies, which explains why only the between-study heterogeneity was reduced. Furthermore, due to the small sample size, it was necessary to aggregate the study features into broad categories, which limited the granularity of the analysis.\nApples and oranges problem: The \\(I^2\\) result of effectively 100% may indicate that the included studies are too different to statistically compare. This is often referred to as the “apples and oranges problem” (Harrer et al., 2022, Chapter 1). The extent to which primary studies can differ while still being meaningfully combined in a meta-analysis is debated. However, when Robert Rosenthal, a pioneer in meta-analysis, was asked whether combining studies with significant differences is valid his response was “combining apples and oranges makes sense if your goal is to produce a fruit salad” (Borenstein et al., 2009, Chapter 40, pp. 357). In this case, despite the diverse research aims of the included studies, the objective is to draw general conclusions about machine learning applications in remote sensing for SDG monitoring. This approach can be viewed as a “fruit salad” with potential for broad applicability across different SDG contexts. However, this again raises the issue of sample size, as a large sample is required to ensure sufficient statistical power to draw confident conclusions.\nCochran’s Q and large sample sizes: Another limitation is the reliance on Cochran’s Q for testing heterogeneity. While widely used, the power of the Q-statistic is dependent on the number of included effect sizes (\\(k\\)) and the precision of the studies i.e., the sample size of that study (\\(m_{ij}\\)). In cases with large sample-sizes, the Q-statistic becomes highly sensitive to even minor differences between studies. The Q-statistic is “overpowered”, which results in the detection of statistically significant heterogeneity even when the actual differences between studies are small. Little research has been done on the effect of very large primary-sample-sizes since meta-analyses typically compile studies who’s unit of analysis are human patients. Primary sample sizes in the millions is not a common issue.\nTransformation of the effect size: In general model selection at the transformed level presents limitations, as the relevance of features is assessed on the transformed scale, which may not directly translate to the original effect size after back-transformation. This complicates the interpretation of results, since conclusions drawn on the transformed scale may not have the same meaning when applied to the original scale. This effect is seen with the covariate: use of ancillary data. Additionally, the use of FT transformation is contested in the literature because of several important limitations (Doi & Xu, 2021; Lin & Xu, 2020; Röver & Friede, 2022; Schwarzer et al., 2019). First, the FT is notably unintuitive, specifically the calculation of variance which relies on the structure of an arcsine function’s derivative. Second, back-transforming the pooled effect size using certain methods—such as the harmonic mean of primary sample sizes—can lead to misleading results (Doi & Xu, 2021; Lin & Xu, 2020; Röver & Friede, 2022; see Schwarzer et al., 2019; Wang, 2023). In this analysis, the pooled variance, rather than the harmonic mean, was used for back-transformation, which seems to address the main concern debated in the literature. Nevertheless, the choice of back-transformation method significantly influences the outcome, and justifying a specific method is especially challenging in a multilevel data structure (Röver & Friede, 2022). Lastly, in a random-effects model the true (transformed) proportion is assumed to follow a normal distribution between studies, the FT transformation potentially violates this assumption as the arcsine function has a bounded domain (Röver & Friede, 2022).\n\n\n\nImplications for Future Research\nThe limitations identified in this meta-analysis suggest several directions for future research that can enhance the robustness and generalisability of findings related to machine learning applications in remote sensing for SDG monitoring.\n\nSample size and model complexity: One of the primary limitations of this meta-analysis was the small sample size. Future research should aim to expand the pool of included studies. This would mean that interaction effects between the collected study features could also be included in the analysis. The structure of the random effects can also be explored with the application of more sophisticated variance-covariance structures for random effects. This approach, sometimes referred to as dose-response meta-analysis (Viechtbauer, 2024, p. 269), would provide insights into how specific study characteristics influence effect sizes over time or across varying conditions.\nBroader inclusion of performance metrics: This meta-analysis primarily focused on overall accuracy, a commonly used but potentially misleading performance metric, particularly in imbalanced datasets. Future studies should expand the range of performance metrics, incorporating class-specific precision, recall, F1-score, Matthews’ correlation coefficient, F1 score, or Somers’ D to provide a more comprehensive evaluation of model performance (Burger & Meertens, 2020). More than one effect size can be modeled using network meta- analysis models (Harrer et al., 2022, Chapter 12). The inclusion of more performance metrics would offer a more nuanced understanding of how models perform under different conditions.\nExploring additional study features and moderators: The present study focused on a limited set of study features. Future research should investigate a broader range of potential moderators, such as model complexity, data preprocessing techniques, and environmental or socio-economic factors specific to SDG challenges. By including a more extensive set of features, researchers can better understand the drivers of performance variability and refine model selection for specific applications.\nEffect of large sample size in primary studies: Simulation studies could provide insights into the sensitivity of Cochran’s Q in the context of large sample sizes. Developing less sensitive methods for assessing heterogeneity would improve the reliability of meta-analytic findings, especially when studies involve substantial sample sizes, which can exaggerate minor differences between studies.\nData extraction: In the time frame of this research, the ChatGPT virtual assistant showed significant improvements in data extraction capabilities. Initially, in January 2024, ChatGPT struggled to extract meaningful features. By May 2024, it was capable of accurately filling in all study features directly from the provided papers (in PDF format). Although the improvement was not formally assessed in this study, the difference was striking. Some research has already examined the potential accuracy of large language models (LLMs) in data extraction for meta-analyses, with promising results (Mahuli et al., 2023). However, for this thesis, ChatGPT was not used for formal data extraction. Instead, traditional manual extraction methods were employed to ensure accuracy. Further investigation into the accuracy of LLMs for meta-analysis is required. LLMs can expedite the data extraction process, potentially addressing challenges related to the limited number of included studies. Another unrelated recommendation to improve data extraction would be for journals to require results and specific features to be submitted separately in addition to the manuscript so that the journals themselves can report trends in outcomes.\n\n\n\n6 Conclusion\nThis meta-analysis provides insights into the variability of machine learning models used for remote sensing in SDG monitoring. First, (1) the average performance of machine learning models was found to be high, but strongly influenced by class imbalance. This finding reinforces the limitations of overall accuracy as a metric for assessing model performance. It highlights the need for a shift towards more balanced and nuanced performance metrics in future SDG monitoring studies. Second, (2) the three-level random-effects model showed a substantial degree of heterogeneity across outcomes. Third (3), the role of specific study features was notable: although no significant differences were observed between model types (e.g., neural networks or tree-based models), the proportion of the majority class and the inclusion of ancillary data were important factors. Finally, the comparison of sample-weighted and unweighted models (4) revealed no substantial difference in summary effect size, though the weighted model uncovered significant heterogeneity. Lastly, more research is needed to assess the robustness and applicability of meta-analyses methods to this field. In particular, the use of Cochran’s Q-statistic is questionable in the context of this analysis, as the very large sample sizes might make the Q-statistic overly sensitive. This can result in the detection of statistically significant heterogeneity, even when the heterogeneity may not be practically meaningful.\n\n\n\n\n\n\nBorenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R. (2009). Introduction to meta-analysis. Wiley.\n\n\nBozada, T., Borden, J., Workman, J., Del Cid, M., Malinowski, J., & Luechtefeld, T. (2021). Sysrev: A FAIR platform for data curation and systematic evidence review. Frontiers in Artificial Intelligence, 4, 685298. https://doi.org/10.3389/frai.2021.685298\n\n\nBurger, J., & Meertens, Q. (2020). The algorithm versus the chimps:on the minima of classifier performance metrics. In L. Cao, W. Kosters, & J. Lijffijt (Eds.), BNAIC/BeneLearn 2020 proceedings (pp. 38–55). BNAIC/BeneLearn. https://bnaic.liacs.leidenuniv.nl/bnaic2020proceedings.pdf\n\n\nDoi, S. A., & Xu, C. (2021). The FreemanTukey double arcsine transformation for the meta-analysis of proportions: Recent criticisms were seriously misleading. Journal of Evidence-Based Medicine, 14(4), 259–261. https://doi.org/10.1111/jebm.12445\n\n\nFoody, G. M. (2020). Explaining the unsuitability of the kappa coefficient in the assessment and comparison of the accuracy of thematic maps obtained by image classification. Remote Sensing of Environment, 239, 111630. https://doi.org/10.1016/j.rse.2019.111630\n\n\nHall, O., Dompae, F., Wahab, I., & Dzanku, F. M. (2023). A review of machine learning and satellite imagery for poverty prediction: Implications for development research and applications. Journal of International Development, 35(7), 1753–1768. https://doi.org/10.1002/jid.3751\n\n\nHanadé Houmma, I., El Mansouri, L., Gadal, S., Garba, M., & Hadria, R. (2022). Modelling agricultural drought: A review of latest advances in big data technologies. Geomatics, Natural Hazards and Risk, 13(1), 2737–2776. https://doi.org/10.1080/19475705.2022.2131471\n\n\nHansen, C., Steinmetz, H., & Block, J. (2022). How to conduct a meta-analysis in eight steps: a practical guide. Management Review Quarterly, 72(1), 1–19. https://doi.org/10.1007/s11301-021-00247-4\n\n\nHarrer, M., Cuijpers, P., Furukawa, T. A., & Ebert, D. D. (2022). Doing meta-analysis with r: A hands-on guide. CRC Press/Taylor & Francis Group. https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\n\nHedges, L. V., Tipton, E., & Johnson, M. C. (2010). Robust variance estimation in meta-regression with dependent effect size estimates. Research Synthesis Methods, 1(1), 39–65. https://doi.org/10.1002/jrsm.5\n\n\nKhatami, R., Mountrakis, G., & Stehman, S. V. (2016). A meta-analysis of remote sensing research on supervised pixel-based land-cover image classification processes: General guidelines for practitioners and future research. Remote Sensing of Environment, 177, 89–100. https://doi.org/10.1016/j.rse.2016.02.028\n\n\nLin, L., & Xu, C. (2020). Arcsine-based transformations for meta-analysis of proportions: Pros, cons, and alternatives. Health Science Reports, 3(3), e178. https://doi.org/10.1002/hsr2.178\n\n\nMahuli, S. A., Rai, A., Mahuli, A. V., & Kumar, A. (2023). Application ChatGPT in conducting systematic reviews and meta-analyses. British Dental Journal, 235(2), 90–92. https://doi.org/10.1038/s41415-023-6132-y\n\n\nPolanin, J. R. (2014). An introduction to multilevel meta-analysis,. https://www.youtube.com/watch?v=rJjeRRf23L8&t=1358s; Campbell Colloquium.\n\n\nRöver, C., & Friede, T. (2022). Double arcsine transform not appropriate for meta-analysis. Research Synthesis Methods, 13(5), 645–648. https://doi.org/10.1002/jrsm.1591\n\n\nSchwarzer, G., Chemaitelly, H., Abu-Raddad, L. J., & Rücker, G. (2019). Seriously misleading results using inverse of freeman-tukey double arcsine transformation in meta-analysis of single proportions. Research Synthesis Methods, 10, 476–483. https://doi.org/10.1002/jrsm.1348\n\n\nStehman, S. V., & Foody, G. M. (2019). Key issues in rigorous accuracy assessment of land cover products. Remote Sensing of Environment, 231, 111199. https://doi.org/10.1016/j.rse.2019.05.018\n\n\nViechtbauer, W. (2024). metafor: Meta-Analysis Package for R. https://doi.org/10.32614/CRAN.package.metafor\n\n\nWang, N. (2023). Conducting Meta-analyses of Proportions in R. Journal of Behavioral Data Science, 3(2), 64–126. https://doi.org/10.35566/jbds/v3n2/wang",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "frontmatter/acknowledgement.html",
    "href": "frontmatter/acknowledgement.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "I am very grateful to everyone that has helped me through the process and completion of this thesis. First and foremost, my thesis supervisors at the CBS, Dr. Joep Burger and Dr. Jonas Klingwort. I would like to thank (and berate) them for giving me the freedom of choosing my topic. What a journey it led us through! I never thought I would refer to back and forth articles on the misconceptions of double arcsine back-transformations as drama, but here I am, waiting for the next episode (no spoilers). In all seriousness, I truly appreciate the time and effort they dedicated and the detailed and feedback they provided. I would also like to extend my thanks to my internal supervisor, Prof. Mark de Rooij, for his support and feedback throughout the process, and to my independent reader, Prof. E.M.L. Dusseldorp for stepping on short notice.\nPerhaps unconventionally, I would like to acknowledge Dr. Wolfgang Viechtbauer —the statistician behind the Metafor package. I aspire to write documentation like his when I “grow up”.\nA sincere thank you goes to my friends and family for their unwavering encouragement through all my self-doubt. Special thanks to Paul, Anka and (my favorite uncle) Mike for their proofreading and feedback, and to Bo for the continuous support and for pretending to be interested in the drama of double arcsines. Lastly, a special thank you to Capo for almost never leaving my side and reminding me of the importance of taking breaks, which, of course, are essential because he should have my undivided attention.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anshuka, A., Ogtrop, F. F. van, & Willem Vervoort, R. (2019).\nDrought forecasting through statistical models using standardised\nprecipitation index: A systematic review and meta-regression analysis.\nNatural Hazards, 97(2), 955–977. https://doi.org/10.1007/s11069-019-03665-6\n\n\nAssink, M., & Wibbelink, C. J. M. (2016). Fitting three-level\nmeta-analytic models in R: A step-by-step tutorial. The Quantitative\nMethods for Psychology, 12(3), 154–174. https://doi.org/10.20982/tqmp.12.3.p154\n\n\nBarendregt, J. J., Doi, S. A., Lee, Y. Y., Norman, R. E., & Vos, T.\n(2013). Meta-analysis of prevalence. Journal of Epidemiology and\nCommunity Health (1979-), 67(11), 974–978. https://doi.org/10.1136/jech-2013-203104\n\n\nBorenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R.\n(2009). Introduction to meta-analysis. Wiley.\n\n\nBorges Migliavaca, C., Stein, C., Colpani, V., Barker, T. H., Munn, Z.,\nFalavigna, M., & on behalf of the Prevalence Estimates Reviews\nSystematic Review Methodology Group (PERSyst). (2020). How are\nsystematic reviews of prevalence conducted? A methodological study.\nBMC Medical Research Methodology, 20(1), 96. https://doi.org/10.1186/s12874-020-00975-3\n\n\nBozada, T., Borden, J., Workman, J., Del Cid, M., Malinowski, J., &\nLuechtefeld, T. (2021). Sysrev: A FAIR platform for data curation and\nsystematic evidence review. Frontiers in Artificial\nIntelligence, 4, 685298. https://doi.org/10.3389/frai.2021.685298\n\n\nBurger, J., & Meertens, Q. (2020). The algorithm versus the\nchimps:on the minima of classifier performance metrics. In L. Cao, W.\nKosters, & J. Lijffijt (Eds.), BNAIC/BeneLearn 2020\nproceedings (pp. 38–55). BNAIC/BeneLearn. https://bnaic.liacs.leidenuniv.nl/bnaic2020proceedings.pdf\n\n\nBurke, M., Driscoll, A., Lobell, D. B., & Ermon, S. (2021). Using\nsatellite imagery to understand and promote sustainable development.\nScience, 371(6535), eabe8628. https://doi.org/10.1126/science.abe8628\n\n\nCampbell, McKenzie, J. E., Sowden, A., Katikireddi, S. V., Brennan, S.\nE., Ellis, S., Hartmann-Boyce, J., Ryan, R., Shepperd, S., Thomas, J.,\nWelch, V., & Thomson, H. (2020). Synthesis without meta-analysis\n(SWiM) in systematic reviews: reporting guideline. BMJ,\n368, l6890. https://doi.org/10.1136/bmj.l6890\n\n\nCampbell, & Wynne, R. H. (2011). Introduction to remote\nsensing (5th ed). Guilford Press.\n\n\nCheung, M. W. L. (2014). Modeling dependent effect sizes with\nthree-level meta-analyses: A structural equation modeling approach.\nPsychological Methods, 19(2), 211–229. https://doi.org/10.1037/a0032968\n\n\nDebray, T. P. A., Damen, J. A. A. G., Snell, K. I. E., Ensor, J., Hooft,\nL., Reitsma, J. B., Riley, R. D., & Moons, K. G. M. (2017). A guide\nto systematic review and meta-analysis of prediction model performance.\nBMJ, i6460. https://doi.org/10.1136/bmj.i6460\n\n\nDoi, S. A., & Xu, C. (2021). The FreemanTukey double\narcsine transformation for the meta-analysis of proportions: Recent\ncriticisms were seriously misleading. Journal of Evidence-Based\nMedicine, 14(4), 259–261. https://doi.org/10.1111/jebm.12445\n\n\nEkmen, O., & Kocaman, S. (2024). Remote sensing for UN SDGs: A\nglobal analysis of research and collaborations. The Egyptian Journal\nof Remote Sensing and Space Sciences, 27(2), 329–341. https://doi.org/10.1016/j.ejrs.2024.04.002\n\n\nFAO, F. and A. O. (2016). Map accuracy assessment and area\nestimation practical guide.,\nhttp://www.fao.org/3/a-i5601e.pdf\n\n\nFoody, G. M. (2020). Explaining the unsuitability of the kappa\ncoefficient in the assessment and comparison of the accuracy of thematic\nmaps obtained by image classification. Remote Sensing of\nEnvironment, 239, 111630. https://doi.org/10.1016/j.rse.2019.111630\n\n\nFreeman, M. F., & Tukey, J. W. (1950). Transformations Related to\nthe Angular and the Square Root. The Annals of Mathematical\nStatistics, 21(4), 607–611. https://doi.org/10.1214/aoms/1177729756\n\n\nGusenbauer, M., & Haddaway, N. R. (2020). Which academic search\nsystems are suitable for systematic reviews or meta‐analyses?\nEvaluating retrieval qualities of Google\nScholar, PubMed, and 26 other resources.\nResearch Synthesis Methods, 11(2), 181–217. https://doi.org/10.1002/jrsm.1378\n\n\nHaddaway, N. R., Bannach-Brown, A., Grainger, M. J., Hamilton, W. K.,\nHennessy, E. A., Keenan, C., Pritchard, C. C., & Stojanova, J.\n(2022). The evidence synthesis and meta-analysis in R\nconference (ESMARConf): Levelling the playing field of\nconference accessibility and equitability. Systematic Reviews,\n11(1), 113. https://doi.org/10.1186/s13643-022-01985-6\n\n\nHall, J. A., & Rosenthal, R. (2018). Choosing between random effects\nmodels in meta-analysis: Units of analysis and the generalizability of\nobtained results. Social and Personality Psychology Compass,\n12(10), e12414. https://doi.org/10.1111/spc3.12414\n\n\nHall, O., Dompae, F., Wahab, I., & Dzanku, F. M. (2023). A review of\nmachine learning and satellite imagery for poverty prediction:\nImplications for development research and applications. Journal of\nInternational Development, 35(7), 1753–1768. https://doi.org/10.1002/jid.3751\n\n\nHanadé Houmma, I., El Mansouri, L., Gadal, S., Garba, M., & Hadria,\nR. (2022). Modelling agricultural drought: A review of latest advances\nin big data technologies. Geomatics, Natural Hazards and Risk,\n13(1), 2737–2776. https://doi.org/10.1080/19475705.2022.2131471\n\n\nHansen, C., Steinmetz, H., & Block, J. (2022b). How to conduct a\nmeta-analysis in eight steps: A practical guide. Management Review\nQuarterly, 72(1), 1–19. https://doi.org/10.1007/s11301-021-00247-4\n\n\nHansen, C., Steinmetz, H., & Block, J. (2022a). How to conduct a\nmeta-analysis in eight steps: a practical guide. Management Review\nQuarterly, 72(1), 1–19. https://doi.org/10.1007/s11301-021-00247-4\n\n\nHarrer, M., Cuijpers, P., Furukawa, T. A., & Ebert, D. D. (2022).\nDoing meta-analysis with r: A hands-on guide. CRC Press/Taylor\n& Francis Group. https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/\n\n\nHarrer, M., Cuijpers, P., Furukawa, T., & Ebert, D. D. (2019).\nDmetar: Companion r package for the guide ’doing meta-analysis in\nr’. http://dmetar.protectlab.org/\n\n\nHedges, L. V., Tipton, E., & Johnson, M. C. (2010). Robust variance\nestimation in meta-regression with dependent effect size estimates.\nResearch Synthesis Methods, 1(1), 39–65. https://doi.org/10.1002/jrsm.5\n\n\nHiggins, J. P. T., & Thompson, S. G. (2002). Quantifying\nheterogeneity in a meta-analysis. Statistics in Medicine,\n10, 1539–1558. https://doi:10.1002/sim.1186\n\n\nHolloway, J., & Mengersen, K. (2018). Statistical\nMachine Learning Methods and\nRemote Sensing for Sustainable\nDevelopment Goals: A\nReview. Remote Sensing, 10(9), 1365. https://doi.org/10.3390/rs10091365\n\n\nIliescu, D., Rusu, A., Greiff, S., Fokkema, M., & Scherer, R.\n(2022). Why We Need Systematic Reviews and Meta-Analyses in the Testing\nand Assessment Literature. European Journal of Psychological\nAssessment, 38(2), 73–77. https://doi.org/10.1027/1015-5759/a000705\n\n\nKhatami, R., Mountrakis, G., & Stehman, S. V. (2016). A\nmeta-analysis of remote sensing research on supervised pixel-based\nland-cover image classification processes: General guidelines for\npractitioners and future research. Remote Sensing of\nEnvironment, 177, 89–100. https://doi.org/10.1016/j.rse.2016.02.028\n\n\nLaird, N. M., & Mosteller, F. (1990). Some Statistical Methods for\nCombining Experimental Results. International Journal of Technology\nAssessment in Health Care, 6(1), 5–30. https://doi.org/10.1017/s0266462300008916\n\n\nLajeunesse, M. J. (2016). Facilitating systematic reviews, data\nextraction, and meta-analysis with the metagear package for r.\n7, 323–330.\n\n\nLavallin, A., & Downs, J. A. (2021). Machine learning in\ngeography–Past, present, and future. Geography\nCompass, 15(5), e12563. https://doi.org/10.1111/gec3.12563\n\n\nLin, L., & Xu, C. (2020). Arcsine-based transformations for\nmeta-analysis of proportions: Pros, cons, and alternatives. Health\nScience Reports, 3(3), e178. https://doi.org/10.1002/hsr2.178\n\n\nMahuli, S. A., Rai, A., Mahuli, A. V., & Kumar, A. (2023).\nApplication ChatGPT in conducting systematic reviews and meta-analyses.\nBritish Dental Journal, 235(2), 90–92. https://doi.org/10.1038/s41415-023-6132-y\n\n\nMaso, J., Zabala, A., & Serral, I. (2023). Earth Observations for\nSustainable Development Goals. Remote Sensing, 15(10),\n2570. https://doi.org/10.3390/rs15102570\n\n\nMcCulloch, C. E., & Neuhaus, J. M. (2011). Misspecifying the shape\nof a random effects distribution: Why getting it wrong may not matter.\nStatistical Science, 26(3), 388–402. https://doi.org/10.1214/11-STS361\n\n\nNASA. (2019). What is Remote Sensing? https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing\n\n\nOwers, C. J., Lucas, R. M., Clewley, D., Tissott, B., Chua, S. M. T.,\nHunt, G., Mueller, N., Planque, C., Punalekar, S. M., Bunting, P., Tan,\nP., & Metternicht, G. (2022). Operational continental-scale land\ncover mapping of Australia using the Open Data Cube. International\nJournal of Digital Earth, 15(1), 1715–1737. https://doi.org/10.1080/17538947.2022.2130461\n\n\nPage, M. J., McKenzie, J. E., Bossuyt, P. M., Boutron, I., Hoffmann, T.\nC., Mulrow, C. D., Shamseer, L., Tetzlaff, J. M., Akl, E. A., Brennan,\nS. E., Chou, R., Glanville, J., Grimshaw, J. M., Hróbjartsson, A., Lalu,\nM. M., Li, T., Loder, E. W., Mayo-Wilson, E., McDonald, S., … Moher, D.\n(2021). The PRISMA 2020 statement: an updated guideline for reporting\nsystematic reviews. BMJ, n71. https://doi.org/10.1136/bmj.n71\n\n\nPolanin, J. R. (2014). An introduction to multilevel\nmeta-analysis,. https://www.youtube.com/watch?v=rJjeRRf23L8&t=1358s;\nCampbell Colloquium.\n\n\nPriem, J., Piwowar, H., & Orr, R. (2022). OpenAlex: A fully-open\nindex of scholarly works, authors, venues, institutions, and\nconcepts. https://arxiv.org/abs/2205.01833\n\n\nPustejovsky, J. E. (2020). Weighting in multivariate\nmeta-analysis. https://jepusto.com/posts/weighting-in-multivariate-meta-analysis/.\n\n\nRöver, C., & Friede, T. (2022). Double arcsine transform not\nappropriate for meta-analysis. Research Synthesis Methods,\n13(5), 645–648. https://doi.org/10.1002/jrsm.1591\n\n\nSchwarzer, G., Carpenter, J. R., & Rücker, G. (2015).\nMeta-Analysis with R. Springer\nInternational Publishing. https://doi.org/10.1007/978-3-319-21416-0\n\n\nSchwarzer, G., Chemaitelly, H., Abu-Raddad, L. J., & Rücker, G.\n(2019). Seriously misleading results using inverse of freeman-tukey\ndouble arcsine transformation in meta-analysis of single proportions.\nResearch Synthesis Methods, 10, 476–483. https://doi.org/10.1002/jrsm.1348\n\n\nSEOS. (2014). Introduction to remote sensing. https://seos-project.eu/remotesensing/remotesensing-c01-p06.html\n\n\nStehman, S. V., & Foody, G. M. (2019). Key issues in rigorous\naccuracy assessment of land cover products. Remote Sensing of\nEnvironment, 231, 111199. https://doi.org/10.1016/j.rse.2019.05.018\n\n\nTawfik, G. M., Dila, K. A. S., Mohamed, M. Y. F., Tam, D. N. H., Kien,\nN. D., Ahmed, A. M., & Huy, N. T. (2019). A step by step guide for\nconducting a systematic review and meta-analysis with simulation data.\nTropical Medicine and Health, 47(1), 46. https://doi.org/10.1186/s41182-019-0165-6\n\n\nThapa, A., Horanont, T., Neupane, B., & Aryal, J. (2023). Deep\nLearning for Remote Sensing\nImage Scene Classification:\nA Review and\nMeta-Analysis. Remote Sensing,\n15(19), 4804. https://doi.org/10.3390/rs15194804\n\n\nUCS. (2021). Union of Concerned Scientists (UCS) Satellite\nDatabase. https://www.ucsusa.org/resources/satellite-database\n\n\nUN DESA. (2023). The Sustainable\nDevelopment Goals Report 2023:\nSpecial Edition. United Nations. https://doi.org/10.18356/9789210024914\n\n\nUN-GGIM:Europe. (2019). The territorial dimension in SDG indicators:\nGeospatial data analysis and its integration with statistical data.\nInstituto Nacional de Estatística. https://un-ggim-europe.org/wp-content/uploads/2019/05/UN_GGIM_08_05_2019-The-territorial-dimension-in-SDG-indicators-Final.pdf\n\n\nUnited Nations. (2017). Earth observations for official statistics:\nSatellite imagery and geospatial data task team report. https://unstats.un.org/bigdata/task-teams/earth-observation/UNGWG_Satellite_Task_Team_Report_WhiteCover.pdf\n\n\nUnited Nations. (2024). The sustainable development goals report\n2024. https://unstats.un.org/sdgs/report/2024/The-Sustainable-Development-Goals-Report-2024.pdf\n\n\nVeroniki, A. A., Jackson, D., Viechtbauer, W., Bender, R., Bowden, J.,\nKnapp, G., Kuss, O., Higgins, J. P., Langan, D., & Salanti, G.\n(2015). Methods to estimate the between-study variance and its\nuncertainty in meta-analysis. Research Synthesis Methods,\n7(1), 55–79. https://doi.org/10.1002/jrsm.1164\n\n\nViechtbauer, W. (2010). Conducting Meta-Analyses in R with the metafor\nPackage. Journal of Statistical Software, 36, 1–48. https://doi.org/10.18637/jss.v036.i03\n\n\nViechtbauer, W. (2020). Weights in models fitted with the rma.mv()\nfunction. https://www.metafor-project.org/doku.php/tips:weights_in_rma.mv_models.\n\n\nViechtbauer, W. (2022). Metafor: Model selection using the glmulti\nand MuMIn packages. https://www.metafor-project.org/doku.php/tips:model_selection_with_glmulti_and_mumin#variable_importance.\n\n\nViechtbauer, W. (2024a). Frequently asked questions [the metafor\npackage]: Freeman-tukey transformation of proportions. https://www.metafor-project.org/doku.php/faq#how_is_the_freeman-tukey_trans.\n\n\nViechtbauer, W. (2024b). metafor: Meta-Analysis Package for R.\nhttps://doi.org/10.32614/CRAN.package.metafor\n\n\nWang, N. (2023). Conducting Meta-analyses of Proportions in R.\nJournal of Behavioral Data Science, 3(2), 64–126. https://doi.org/10.35566/jbds/v3n2/wang\n\n\nYin, C., Peng, N., Li, Y., Shi, Y., Yang, S., & Jia, P. (2023). A\nreview on street view observations in support of the sustainable\ndevelopment goals. International Journal of Applied Earth\nObservation and Geoinformation, 117, 103205. https://doi.org/10.1016/j.jag.2023.103205\n\n\nZhang, C., & Li, X. (2022). Land Use and Land Cover Mapping in the\nEra of Big Data. Land, 11(10), 1692. https://doi.org/10.3390/land11101692\n\n\nZhang, Y., Liu, J., & Shen, W. (2022). A Review of\nEnsemble Learning Algorithms\nUsed in Remote Sensing\nApplications. Applied Sciences, 12(17),\n8654. https://doi.org/10.3390/app12178654\n\n\nZhao, Q., Yu, L., Du, Z., Peng, D., Hao, P., Zhang, Y., & Gong, P.\n(2022). An Overview of the Applications of\nEarth Observation Satellite\nData: Impacts and Future\nTrends. Remote Sensing, 14(8), 1863. https://doi.org/10.3390/rs14081863",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix/app1-paper_selection/app1-paper_selection.html",
    "href": "appendix/app1-paper_selection/app1-paper_selection.html",
    "title": "Appendix A — Paper selection",
    "section": "",
    "text": "Retrieving missing abstracts\nref: https://rpubs.com/cschwarz/web_scrape\nCode\n# missing_index &lt;- which(is.na(all_citations$Abstract.Note))\n# not_missing &lt;- all_citations[!is.na(all_citations$Abstract.Note), ]\n\n# retrieve abstract function\ngrab_info &lt;- function(article){\n  if(is.na(article[\"Abstract.Note\"])) {\n     article_link &lt;- paste0(\"https://doi.org/\", article[\"DOI\"])\n     a_html &lt;- try(read_html(article_link), silent = TRUE)\n     if (inherits(a_html, 'try-error')) {\n       article[\"Abstract.Note\"] &lt;- NA\n       }\n     else {\n       abstract &lt;- a_html %&gt;% html_node(\".hlFld-Abstract .last\") %&gt;% html_text()\n       article[\"Abstract.Note\"] &lt;- abstract\n       }\n  }\n  return(article)\n}\n\nlibrary(parallel)\n\ncl &lt;- makeCluster(detectCores())\nclusterExport(cl,varlist = c(\"grab_info\"))\nclusterEvalQ(cl,library(rvest))\nclusterEvalQ(cl,library(stringr))\n\nlibrary(pbapply)\ncitations &lt;- as.data.frame(t(pbapply(all_citations, 1, grab_info, cl=cl)))\nsum(is.na(citations$Abstract.Note))\n\n\n# to make searching for words easier and because metagear is case sensitive \ncitations$Title &lt;- tolower(citations$Title) \ncitations$Abstract.Note &lt;- tolower(citations$Abstract.Note) \n\n#write.csv(citations, file = \"citations_with_reivew.csv\")\nTO DO: - manually add the missing 6 using zotero",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Paper selection</span>"
    ]
  },
  {
    "objectID": "appendix/app1-paper_selection/app1-paper_selection.html#abstract-screening",
    "href": "appendix/app1-paper_selection/app1-paper_selection.html#abstract-screening",
    "title": "Appendix A — Paper selection",
    "section": "Abstract screening",
    "text": "Abstract screening\n\nMetagear package\n\n\nCode\nlibrary(metagear)\n\n\n\nKeywords\n\n\nCode\nkeywords &lt;- c(\n    \n  # general\n  \"empirical\", \"result\", \"predictive\",\n  \"analysis\", \"sustainable development goal\", \n  \"sustainable development\",\n              \n  # data related \n  \"remotely sensed\", \"remote sensing\", \"satellite\", \"earth observation\", \n  \n  # models\n  \"deep learning\", \"machine learning\", \"classification\", \"classifier\",\n  \"regression\", \"supervised\", \"supervized\", \"test set\", \"training set\",\n  \" cart \", \"svm\", \" rf \", \" ann \", \"random forest\", \"support vector machine\", \n  \"regression tree\", \"decision tree\", \"neural network\", \"boosting\", \"bagging\", \n  \"gradient\", \"bayes\",\n  \n  # quality metrics\n  \"overall accuracy\", \"accuracy\", \"coefficient of determination\", \"rmse\", \"mse\",\n  \"f1\", \"precision\", \"auc\", \" roc \", \"recall\",\"sensitivity\", \"specificity\",\n  \"mean absolute error\", \"error\", \"mae\",\n  \n  #to omit \n  \"systematic review\", \"meta-analysis\" , \"review\"\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Paper selection</span>"
    ]
  },
  {
    "objectID": "appendix/app1-paper_selection/app1-paper_selection.html#testing-abstract-screening-process",
    "href": "appendix/app1-paper_selection/app1-paper_selection.html#testing-abstract-screening-process",
    "title": "Appendix A — Paper selection",
    "section": "Testing Abstract Screening process",
    "text": "Testing Abstract Screening process\n\n\nCode\ncitations &lt;- read.csv(\"citations.csv\")\n\n\n\nPhase one\n\n\nCode\ncite_testset &lt;- citations[sample(nrow(citations), size=100), ] # set.seed next time!\n\n\nRandom sample of 100 papers was compaired by three reviewers using metagear and the\nkeywords (from line 144).\n\n\nCode\nmetagear_abstract_screening&lt;- function(data, reviewer){\n  library(metagear)\n  # prime the study‐reference dataset\n  theRefs &lt;- effort_initialize(data)\n  # here one would = distribute screening effort to a team, in this case that is one person \n  # and save to separate files for each team member\n  theRefs_unscreened &lt;- effort_distribute(theRefs, reviewers = reviewer, \n                                        effort = 100, save_split = TRUE)\n}\n\n\n# the following was run for each reviewer \n# eample nina \nmetagear_abstract_screening(citations, \"nina\")\n\n# initialize screener GUI\nabstract_screener(\"effort_nina.csv\", aReviewer = \"nina\", \n                  abstractColumnName = \"Abstract.Note\", \n                                        titleColumnName = \"Title\",\n                                        highlightKeywords = keywords)\n\n\n\nResult\n\n\nCode\nnina_1&lt;- read.csv(\"reviewer_screaning/effort_nina.csv\")\njonas_1&lt;- read.csv(\"reviewer_screaning/effort_jonas.csv\")\njoep_1&lt;- read.csv(\"reviewer_screaning/effort_joep.csv\")\nftable(nina_1$INCLUDE, jonas_1$INCLUDE, joep_1$INCLUDE,  dnn = c(\"nina\", \"jonas\", \"joep\"))\n\n\n            joep maybe NO YES\nnina  jonas                  \nmaybe maybe          0  1   0\n      NO             1  1   0\n      YES            2  0   1\nNO    maybe          0  3   1\n      NO             0 29   0\n      YES            1  7   2\nYES   maybe          0  0   1\n      NO             1  3   1\n      YES            5  4  36\n\n\nAll agreed on YES 36% and NO 29%\nAfter reviewing cases that contradicted, points learnt: - not all the papers included use remote sensing data, these were difficult to categories and might require opening the full paper. Therefore, the next trail will be:\n\nphase 2a: screening for empirical research, rather than papers reviewing or discussing methods.\nphase 2b: from the papers that should be included, we will then assess them more carefully: remote sensing data, machine learning.\n\n\n\n\nPhase 2A\n\n\nCode\n# new random sample: excluding all reviewed in phase 1 \nnot_in_sample1&lt;- subset(citations, !(DOI %in% nina$DOI))\nset.seed(123)\nsample2 &lt;- not_in_sample1[sample(nrow(not_in_sample1), size=100), ]\n\n\n\n\nCode\n# the following was run for each reviewer \n# example nina \nmetagear_abstract_screening(sample2, \"nina_2a\")\n\n# initialize screener GUI\nabstract_screener(\"effort_nina_2a.csv\", aReviewer = \"nina_2a\", \n                  abstractColumnName = \"Abstract.Note\", \n                                        titleColumnName = \"Title\",\n                                        highlightKeywords = keywords)\n\n\n\n\nScreening abstracts of references\nInstructions 1. run keywords line 362\n\n\nCode\nmetagear_abstract_screening&lt;- function(data, reviewer){\n  library(metagear)\n  # prime the study‐reference dataset\n  theRefs &lt;- effort_initialize(data)\n  # here one would = distribute screening effort to a team, in this case that is one person \n  # and save to separate files for each team member\n  theRefs_unscreened &lt;- effort_distribute(theRefs, reviewers = reviewer, \n                                        effort = 100, save_split = TRUE)\n}\n\n\n\nrun the applicable chunk below\n\n\nchoose between relevance categories: Yes, maybe, no\nonce you are done reviewing save (bottom-right: if you don’t save none of the changes are recorded)\n\n\nFor Jonas\n\n\nCode\n# Stage 1 \n\n# initialize screener GUI\nabstract_screener(\"effort_jonas_2a.csv\", aReviewer = \"jonas_2a\", \n                  abstractColumnName = \"Abstract.Note\", \n                                        titleColumnName = \"Title\",\n                                        highlightKeywords = keywords)\n\n\n\n\nCode\nkeywords_new &lt;- c(keywords, \n                  # data related \n                  \"remotely sensed\", \"remote sensing\", \"satellite\", \"earth observation\", \n                  \" rs \", \"images\", \"imagery\", \"sentinel\", \"landsat\", \"openstreetmap\", \n                  \"google earth engine\", \"true color\", \"true colour\", \"false color\",\n                  \"false colour\", \"rgb\", \"resolution\"\n                  ) \n\n\n\n\nCode\n# Stage 2\n# after revieing run:\ns1_review &lt;- read.csv(\"effort_jonas_2a.csv\")\nrev &lt;- subset(s1_review, s1_review$INCLUDE != \"NO\")\n\n\nmetagear_abstract_screening(rev, \"jonas_2b\")\n\n# initialize screener GUI\nabstract_screener(\"effort_jonas_2b.csv\", aReviewer = \"jonas_2b\", \n                  abstractColumnName = \"Abstract.Note\", \n                                        titleColumnName = \"Title\",\n                                        highlightKeywords = keywords_new)\n\n\n\n\n\nCompare\n\n\nCode\nnina_2a&lt;- read.csv(\"reviewer_screaning/effort_nina_2a.csv\")\njonas_2a&lt;- read.csv(\"reviewer_screaning/effort_jonas_2a.csv\")\njoep_2a&lt;- read.csv(\"reviewer_screaning/effort_joep_2a.csv\")\n\nftable(nina_2a$INCLUDE, jonas_2a$INCLUDE, joep_2a$INCLUDE,  dnn = c(\"nina\", \"jonas\", \"joep\"))\n\n\n           joep NO YES\nnina jonas            \nNO   NO         44   1\n     YES         1   1\nYES  NO          2   1\n     YES         5  45\n\n\nStage 1: agreed: 89%\n\n\nStep 2\n\n\nCode\njonas_2b &lt;- read.csv(\"reviewer_screaning/effort_jonas_2b.csv\")\n\nnina_2b &lt;- read.csv(\"reviewer_screaning/effort_nina_2b.csv\")\n\njoep_2b &lt;- read.csv(\"reviewer_screaning/effort_joep_2b.csv\")\n\n\n\n\nCode\nall&lt;- joep_2a|&gt;\n  dplyr::select(INCLUDE, Title, Abstract.Note, Publication.Year , DOI)\n\n\nall2&lt;- left_join(all, subset(joep_2b, select = c(INCLUDE, DOI)), \n                 by = \"DOI\", suffix =c(\"_joep_a\",\"_joep\"))\n\nall2&lt;- left_join(all2, subset(jonas_2b, select = c(INCLUDE, DOI)), \n                 by = \"DOI\")\ncolnames(all2)[colnames(all2) == \"INCLUDE\"] &lt;- \"INCLUDE_jonas\"\n\nall2&lt;- left_join(all2, subset(nina_2b, select = c(INCLUDE, DOI)), \n                 by = \"DOI\")\ncolnames(all2)[colnames(all2) == \"INCLUDE\"] &lt;- \"INCLUDE_nina\"\n\nall2[which(is.na(all2$INCLUDE_joep) & is.na(all2$INCLUDE_jonas) & is.na(all2$INCLUDE_nina)), 6:8] &lt;- \"NO\"\n\n\n\n\nCode\nftable(all2$INCLUDE_nina, all2$INCLUDE_jonas, all2$INCLUDE_joep,  dnn = c(\"nina\", \"jonas\", \"joep\"))\n\n\n67% over all agreement 21% YES that we agree at least one yes:\n\n\nCode\nsum(all2$INCLUDE_joep == \"YES\"| all2$INCLUDE_nina == \"YES\"| all2$INCLUDE_jonas == \"YES\", na.rm = T)\n\nsum(all2$INCLUDE_joep == \"YES\",  na.rm = T)\nsum(all2$INCLUDE_nina == \"YES\",  na.rm = T)\nsum(all2$INCLUDE_jonas == \"YES\",  na.rm = T)\n\n\n\nAdding phase 1 for total agreement\n\n\nCode\nall3&lt;- nina_1|&gt;\n  dplyr::select(INCLUDE, Title, Abstract.Note, Publication.Year ,DOI)\n\n\nall3&lt;- left_join(all3, subset(jonas_1, select = c(INCLUDE, DOI)), \n                 by = \"DOI\")\ncolnames(all3)[colnames(all3) == \"INCLUDE.x\"] &lt;- \"INCLUDE_nina\"\ncolnames(all3)[colnames(all3) == \"INCLUDE.y\"] &lt;- \"INCLUDE_jonas\"\n\nall3&lt;- left_join(all3, subset(joep_1, select = c(INCLUDE, DOI)), \n                 by = \"DOI\")\ncolnames(all3)[colnames(all3) == \"INCLUDE\"] &lt;- \"INCLUDE_joep\"\n\n\n\n\nCode\nall_reviewed &lt;- rbind(all3, all2[, -1])\n\nftable_result&lt;- ftable(all_reviewed$INCLUDE_nina, all_reviewed$INCLUDE_jonas, all_reviewed$INCLUDE_joep,  dnn = c(\"nina\", \"jonas\", \"joep\"))\n\nall_agreed_yes &lt;- subset(all_reviewed, \n                         subset = (INCLUDE_joep == \"YES\"&\n                                  INCLUDE_nina == \"YES\"&\n                                    INCLUDE_jonas == \"YES\"))\n#write.csv(all_agreed_yes, file = \"all_agreed_yes.csv\")\n\nftable_df &lt;- as.data.frame(ftable_result)\n\nsum(ftable_df$Freq[ftable_df$nina == \"YES\"])\nsum(ftable_df$Freq[ftable_df$jonas == \"YES\"])\n\nsum(ftable_df$Freq[ftable_df$joep == \"YES\"])\n\n\n\n\nCode\nall_agreed_yes &lt;- read.csv(\"reviewer_screaning/all_agreed_yes.csv\")\n#metagear_abstract_screening(all_agreed_yes, \"clas_v_reg_nina\")\n\nabstract_screener(\"effort_nina.csv\", aReviewer = \"clas_v_reg_nina\", \n                  abstractColumnName = \"Abstract.Note\", \n                  titleColumnName = \"Title\",\n                  theButtons = c(\"class\", \"reg\", \"unknown\"),\n                  highlightKeywords = keywords)\n\ncurrent &lt;- read.csv(\"effort_clas_v_reg_nina.csv\")\n\nsum(current$INCLUDE == \"class\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Paper selection</span>"
    ]
  },
  {
    "objectID": "appendix/app2-data.html",
    "href": "appendix/app2-data.html",
    "title": "Appendix B — Data wrangling",
    "section": "",
    "text": "The following code shows how I combined the data from different sources and grouped the variables. For the final that see Github repository.\n\nsource(\"../appendix/packages.R\")\n\n# Manualy extracted features from poteniatly relevant papers\nextracted_features &lt;- read_excel(\"data/extracted_features.xlsx\", sheet = \"Include\", na = \"NA\")\nextracted_features &lt;- extracted_features |&gt; mutate(DOI = tolower(DOI))\n\n# Citation data from zeotero\ncitations &lt;- read.csv(\"data/citations.csv\")\ncitations &lt;- citations |&gt; mutate(DOI = tolower(DOI))\n\n# Citation number data from OpenAlex\nopen_alex &lt;- read.csv(\"data/OpenAlex.csv\")\nopen_alex &lt;- open_alex[, c(\"DOI\", \"globalCitationsCount\")]\nopen_alex &lt;- open_alex |&gt; mutate(DOI = tolower(DOI))\n\n# For study label/study ID: merging first author and year \ncitations$AuthorYear &lt;- paste(sapply(strsplit(citations$Author, \", \"), \n                                     `[`, 1), \"et al.\", \n                              citations$Publication.Year)\n\n# Join the publication details with the extracted features\ncitations_needed &lt;- subset(citations, DOI %in% extracted_features$DOI, \n                           select = c(DOI, AuthorYear, \n                                      Publication.Title, \n                                      Publication.Year))\n\ndat &lt;- inner_join(citations_needed, extracted_features, by = \"DOI\")\n\n# Add global citation number from OpenAlex\ndat &lt;- left_join(dat, open_alex, by = \"DOI\")\n\n# Study features: factors\ncols_to_factor &lt;- c(\"SDG_theme\", \"classification_type\", \"model_group\", \"ancillary\",\n                    \"indices\", \"RS_device_type\", \"RS_devices\", \"RS_device_group\",\n                    \"RS_spectral_bands_no\", \"RS_spatital_resolution_m\",\n                    \"Confusion_matrix\")\n\n# Select and clean the final dataset for analysis\nmy_data &lt;- subset(dat, !is.na(total), # Omit studies without totals\n                  select = c(\"DOI\", \"AuthorYear\", \"Publication.Year\", \n                             \"globalCitationsCount\", \"location\", cols_to_factor,\n                             \"OA_reported\", \"number_classes\", \n                             \"fraction_majority_class\", \"total\"))\n\n# Regroup the extracted features: at least 5 for each group\n\nmy_data$model_group &lt;- ifelse((my_data$model_group == \"Neural Networks\" | \n                               my_data$model_group == \"Tree-Based Models\"), \n                              my_data$model_group, \"Other\")\n\nmy_data$model_group &lt;- factor(my_data$model_group, \n                              levels = c(\"Neural Networks\", \"Tree-Based Models\", \"Other\"))\n\n\n## Group the number of bands (low, mid, not reported)\nmy_data$no_band_group &lt;- with(my_data, \n                              ifelse(RS_spectral_bands_no == \"Not Reported\", \n                                     \"Not Reported\", \n                             ifelse(RS_spectral_bands_no %in% c(1, 4, 5), \n                                    \"Low\", \n                             ifelse(RS_spectral_bands_no %in% \n                                      c(7, 8, 9, 10, 11, 13, 14), \n                                    \"Mid\", NA)))\n                             )\n\n# Group remote sensing spatial resolution\nmy_data$RS_spatital_res_grouped &lt;- ifelse(my_data$RS_spatital_resolution_m &lt; 1, \n                                          \"&lt;1 metre\",\n                                  ifelse(my_data$RS_spatital_resolution_m &gt;= 10 & \n                                         my_data$RS_spatital_resolution_m &lt;= 30, \n                                         \"10-30 metres\", \n                                         my_data$RS_spatital_resolution_m))\n\n#  ## maybe this is better: \n# my_data$RS_spatital_res_grouped &lt;- ifelse(my_data$RS_spatital_resolution_m !=\n#                                             \"Not Reported\", \"Reported\",  \n#                                           my_data$RS_spatital_resolution_m)\n\n# Reorder RS_device_group\nmy_data$RS_device_group &lt;- factor(my_data$RS_device_group, \n                                  levels = c(\"Sentinel\", \"Landsat\", \n                                             \"Other\", \"Not Reported\"))\n\n# SDG\nmy_data$SDG_theme &lt;- factor(my_data$SDG_theme, levels = c(\"SDG2: Zero Hunger\", \n                                                          \"SDG11: Sustainable Cities\", \n                                                          \"SDG15: Life on Land\"))\n\n# Label for ancillary\nmy_data$ancillary &lt;- factor(my_data$ancillary, \n                            levels = c(0, 1), \n                            labels = c(\"Remote Sensing Only\", \"Ancillary Data Included\"))\n\n# Label for indices\nmy_data$indices &lt;- factor(my_data$indices, \n                          levels = c(0, 1), \n                          labels = c(\"Not Used\", \"Used\"))\nmy_data$Confusion_matrix &lt;- factor(my_data$Confusion_matrix, \n                            levels = c(0, 1), \n                            labels = c(\"Not Reported\", \"Reported\"))\n\n# Estimate ID (esid) based on each study (AuthorYear)\nmy_data &lt;- my_data |&gt; group_by(AuthorYear) |&gt; mutate(esid = row_number())\n\n# Event (s_ij) variable for analysis of proportions\nmy_data$event &lt;- my_data$total * my_data$OA_reported\n\n# Save the final dataset for analysis\nwrite.csv(my_data, \"../data/analysis_df.csv\")\n\nThe following assesses whether any categorical variables in the dataset had values that are unique to a single study. First, the relevant categorical variables, including features like remote sensing device type and spatial resolution are selected. A function is defined to group each variable by its values and count the number of distinct papers associated with each value. The tables show the number of papers, effect sizes, and highlights the specific study name if that category is only represented by a single source. The number of effect sizes is always greater than 5, however there are a few instances that only one study contributed to a category.\n\ncategorical_cols &lt;- c(\"SDG_theme\", \"classification_type\", \"model_group\", \"ancillary\", \n                      \"indices\", \"RS_device_group\", \"RS_devices\", \"RS_device_type\",\n                      \"RS_device_group\", \"no_band_group\", \n                      \"RS_spatital_res_grouped\", \"Confusion_matrix\")\n\ncheck_single_study &lt;- function(df, var_name) {\n  df %&gt;%\n    group_by_at(var_name) %&gt;% \n    summarise(unique_studies = n_distinct(AuthorYear)) %&gt;% \n    filter(unique_studies == 1) %&gt;% \n    summarise(total_entries = n()) %&gt;% \n    pull(total_entries) &gt; 0\n}\n\nsingle_study_vars &lt;- categorical_cols[sapply(categorical_cols, \n                                             function(v) check_single_study(my_data, v))]\n\ncount_studies_effect_sizes_and_study &lt;- function(df, var_name) {\n  df %&gt;%\n    group_by_at(var_name) %&gt;%\n    summarise(count_papers = n_distinct(AuthorYear),  # Count distinct papers\n              count_effect_sizes = n(),  # Count total number of effect sizes\n              study = ifelse(count_papers == 1, first(AuthorYear), NA)) %&gt;%  #name if count is 1\n    arrange(desc(count_papers))\n}\n\n# Loop through the categorical variables and count the number of papers,\n    ## effect sizes, study name given if unique\ncount_values_list &lt;- list()\nfor (var in single_study_vars) {\n  count_values &lt;- count_studies_effect_sizes_and_study(my_data, var)\n  count_values_list[[var]] &lt;- count_values\n}\n\n\nfor (var in names(count_values_list)) {\n  print(count_values_list[[var]])\n}\n\n# A tibble: 4 × 4\n  RS_device_group count_papers count_effect_sizes study             \n  &lt;fct&gt;                  &lt;int&gt;              &lt;int&gt; &lt;chr&gt;             \n1 Sentinel                   9                 20 &lt;NA&gt;              \n2 Landsat                    8                 15 &lt;NA&gt;              \n3 Other                      4                 44 &lt;NA&gt;              \n4 Not Reported               1                  7 Jochem et al. 2018\n# A tibble: 2 × 4\n  RS_devices                 count_papers count_effect_sizes study           \n  &lt;chr&gt;                             &lt;int&gt;              &lt;int&gt; &lt;chr&gt;           \n1 satellite                            19                 79 &lt;NA&gt;            \n2 aerial photographic images            1                  7 Shen et al. 2023\n# A tibble: 4 × 4\n  RS_device_type count_papers count_effect_sizes study             \n  &lt;chr&gt;                 &lt;int&gt;              &lt;int&gt; &lt;chr&gt;             \n1 Passive                  15                 61 &lt;NA&gt;              \n2 Combined                  4                  7 &lt;NA&gt;              \n3 Active                    3                 11 &lt;NA&gt;              \n4 Not Reported              1                  7 Jochem et al. 2018\n# A tibble: 3 × 4\n  RS_spatital_res_grouped count_papers count_effect_sizes study           \n  &lt;chr&gt;                          &lt;int&gt;              &lt;int&gt; &lt;chr&gt;           \n1 10-30 metres                      16                 39 &lt;NA&gt;            \n2 Not Reported                       4                 40 &lt;NA&gt;            \n3 &lt;1 metre                           1                  7 Shen et al. 2023",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  }
]